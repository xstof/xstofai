[
  {
    "objectID": "pytorch-broadcasting.html",
    "href": "pytorch-broadcasting.html",
    "title": "Pytorch Broadcasting",
    "section": "",
    "text": "Let’s start with some simple tensors we can use to learn with:\n\nx = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])\ny = torch.tensor([2,4,5])\n\n\nx.shape, y.shape\n\n(torch.Size([2, 3]), torch.Size([3]))\n\n\nx has 2 rows (first dimension) and 3 cols (second dimension)"
  },
  {
    "objectID": "pytorch-broadcasting.html#broadcasting---2-dimensions",
    "href": "pytorch-broadcasting.html#broadcasting---2-dimensions",
    "title": "Pytorch Broadcasting",
    "section": "Broadcasting - 2 dimensions",
    "text": "Broadcasting - 2 dimensions\nPytorch broadcasting takes 2 tensors and compares their dimensions, starting from the right to the left. So:\n\nx has dim: 2, 3\ny has dim: 3\n\nnow pytorch will align the most right dimensions and use y two times (because of the missing corresponding dimension 2 x has but y has not. This always starts from the most inner dimension.\n\nx * y # element wise addition\n\ntensor([[ 2,  8, 15],\n        [ 8, 20, 30]])\n\n\nWhat if y has two dimensions but the first dimension is 1?\n\nx has dim: 2, 3\ny has dim: 1, 3\n\n\ny = torch.tensor([[2,4,5]])\n\n\nx * y\n\ntensor([[ 2,  8, 15],\n        [ 8, 20, 30]])\n\n\nThis is exactly the same result: y will be broadcasted. Will this also work when y has 2 dimensions, just like x? In that case, no broadcasting is needed:\n\nx = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])\ny = torch.tensor([[2, 3, 4],\n                  [5, 6, 7]])\nx * y\n\ntensor([[ 2,  6, 12],\n        [20, 30, 42]])\n\n\nWhat if x has 4 dimensions? Will it still broadcast?\n\nx has dim: 4, 3\ny has dim: 2, 3\n\n\nx = torch.tensor([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 6, 5],\n                  [4, 3, 2]])\ny = torch.tensor([[2, 3, 4],\n                  [5, 6, 7]])\n# THIS DOES NOT WORK: x * y\n# ERROR: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0\n\nThis means: this will not work, we need to make sure that either the dimension is missing or the dimension is 1 for broadcasting to work."
  },
  {
    "objectID": "pytorch-broadcasting.html#broadcasting---more-dimensions",
    "href": "pytorch-broadcasting.html#broadcasting---more-dimensions",
    "title": "Pytorch Broadcasting",
    "section": "Broadcasting - more dimensions",
    "text": "Broadcasting - more dimensions\nWhat if we have two tensors with more than 2 dimentions?\n\nx = torch.tensor([[[1, 2, 3],\n                  [4, 5, 6]],\n                 [[7, 6, 5],\n                  [4, 3, 2]],\n                 [[2, 3, 4],\n                  [5, 6, 7]],\n                 [[6, 5, 4],\n                  [3, 2, 1]]])\ny = torch.tensor([2, 3, 4])\n\n\nx.shape, y.shape\n\n(torch.Size([4, 2, 3]), torch.Size([3]))\n\n\nWhat happens if we multiply this (\\(4 \\times 2 \\times 3\\)) by (\\(3\\)):\n\nx has dim: 4, 2, 3\ny has dim: 3\n\n\nz = x * y\nz\n\ntensor([[[ 2,  6, 12],\n         [ 8, 15, 24]],\n\n        [[14, 18, 20],\n         [ 8,  9,  8]],\n\n        [[ 4,  9, 16],\n         [10, 18, 28]],\n\n        [[12, 15, 16],\n         [ 6,  6,  4]]])\n\n\n\nz.shape\n\ntorch.Size([4, 2, 3])\n\n\n\ny = torch.tensor([[2,3,4]])\nz2 = x * y\ntorch.allclose(z, z2)\n\nTrue\n\n\nThis last result means that when:\n\nx has dim: 4, 2, 3\ny has dim: 1, 3\n\nthe result is exactly the same.\nLet’s try have a broadcast dimension of 1 in both tensors:\n\nx = torch.tensor([\n                 [[1, 2, 3]],\n                 [[7, 6, 5]],\n                 [[2, 3, 4]],\n                 [[6, 5, 4]]\n                ])\ny = torch.tensor([[2, 1, 4],\n                  [1, 3, 5],\n                  [1, 2, 3]])\n\nNow:\n\nx has dim: 4, 1, 3\ny has dim: 3, 3\n\nSo the result must be \\(4 \\times 3 \\times 3\\)\n\nx * y\n\ntensor([[[ 2,  2, 12],\n         [ 1,  6, 15],\n         [ 1,  4,  9]],\n\n        [[14,  6, 20],\n         [ 7, 18, 25],\n         [ 7, 12, 15]],\n\n        [[ 4,  3, 16],\n         [ 2,  9, 20],\n         [ 2,  6, 12]],\n\n        [[12,  5, 16],\n         [ 6, 15, 20],\n         [ 6, 10, 12]]])\n\n\nHow did we get here?\nThe last dimension for x and y is equal, so we multiplied element wise the most inner elements. However, along dimension with index 1, y has 3 times 3 numbers and x only has 1, so we need to broadcast x here and use [1,2,3] 3 times to match up with y. So to match the shape with y, the dimension fo x with index 1 will first be duplicated 3 times:\n\nx2 = torch.tensor([\n                 [[1, 2, 3],\n                  [1, 2, 3],\n                  [1, 2, 3]],\n    \n                 [[7, 6, 5],\n                  [7, 6, 5],\n                  [7, 6, 5]],\n    \n                 [[2, 3, 4],\n                  [2, 3, 4],\n                  [2, 3, 4]],\n    \n                 [[6, 5, 4],\n                  [6, 5, 4],\n                  [6, 5, 4]]\n                ])\ny2 = torch.tensor([[2, 1, 4],\n                  [1, 3, 5],\n                  [1, 2, 3]])\n\nNow, they have dimensions:\n\nx has dim: 4, 3, 3\ny has dim: 3, 3\n\nTo completely match up, y will need to be repeated 4 times:\n\ny2 = torch.tensor([\n                   [[2, 1, 4],\n                    [1, 3, 5],\n                    [1, 2, 3]],\n    \n                    [[2, 1, 4],\n                     [1, 3, 5],\n                     [1, 2, 3]],\n    \n                    [[2, 1, 4],\n                     [1, 3, 5],\n                     [1, 2, 3]],\n    \n                    [[2, 1, 4],\n                     [1, 3, 5],\n                     [1, 2, 3]],\n                 ])\n\n\nx2 * y2\n\ntensor([[[ 2,  2, 12],\n         [ 1,  6, 15],\n         [ 1,  4,  9]],\n\n        [[14,  6, 20],\n         [ 7, 18, 25],\n         [ 7, 12, 15]],\n\n        [[ 4,  3, 16],\n         [ 2,  9, 20],\n         [ 2,  6, 12]],\n\n        [[12,  5, 16],\n         [ 6, 15, 20],\n         [ 6, 10, 12]]])"
  },
  {
    "objectID": "pytorch-broadcasting.html#matrix-multiplication",
    "href": "pytorch-broadcasting.html#matrix-multiplication",
    "title": "Pytorch Broadcasting",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nInstead of doing an element-wise operation, let’s try do matrix multiplication with pytorch. The shorthand syntax for this is the @ symbol.\nLet’s first do multiplication of two 1 dimensional tensors. This results in Pytorch doing a dot product:\n\na = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\nc = a @ b\nc.shape, c\n\n(torch.Size([]), tensor(32))\n\n\n\n1*4 + 2*5 + 3*6\n\n32\n\n\nIf both arguments are two-dimensional then the matrix multiplication is being performed:\n\nx = torch.tensor([[1, 2],\n                  [3, 4]])\ny = torch.tensor([[2, 3],\n                  [4, 5]])\nx @ y\n\ntensor([[10, 13],\n        [22, 29]])\n\n\n\n1*2+2*4, 1*3+2*5, 3*2+4*4, 3*3+4*5\n\n(10, 13, 22, 29)\n\n\nIf the first argument only has 1 dimension while the second has 2 dimensions, Pytorch adds a first dimension of 1 (so the dimensions become: \\(1 \\times 2 @ 2 \\times 2 = 1 \\times 2\\)), does the multiplication and removes the dimension again:\n\nx = torch.tensor([1, 2])\ny = torch.tensor([[2, 3],\n                  [4, 5]])\nx @ y\n\ntensor([10, 13])\n\n\n\nx2 = torch.tensor([[1, 2]])\nx2 @ y  # but when pytorch does this, the first dimension of the result is removed again\n\ntensor([[10, 13]])\n\n\nIf on the contrary the first tensor is two dimensional and the second is one dimensional, Pytorch does a matrix-vector product:\n\nx = torch.tensor([[2, 3],\n                  [4, 5]])\ny = torch.tensor([1, 2])\nx @ y\n\ntensor([ 8, 14])\n\n\n\n2*1+3*2, 4*1+5*2\n\n(8, 14)\n\n\nThis is essentially the same as if you would transpose the second tensor and do a matrix multiplication:\n\nx @ y.T\n\ntensor([ 8, 14])"
  },
  {
    "objectID": "pytorch-broadcasting.html#resources",
    "href": "pytorch-broadcasting.html#resources",
    "title": "Pytorch Broadcasting",
    "section": "Resources",
    "text": "Resources\n\nPytorch documentation on broadcasting\nPytorch broadcasting semantics\nPytorch matrix multiplication"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "titanic - linear and nn from scratch.html",
    "href": "titanic - linear and nn from scratch.html",
    "title": "Titanic",
    "section": "",
    "text": "import os\nfrom pathlib import Path\n\n\n!pip install kaggle  # make sure kaggle api is here so we can use it to dowload the dataset\n\nRequirement already satisfied: kaggle in /home/xstof/mambaforge/lib/python3.9/site-packages (1.5.12)\nRequirement already satisfied: python-dateutil in /home/xstof/mambaforge/lib/python3.9/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: tqdm in /home/xstof/mambaforge/lib/python3.9/site-packages (from kaggle) (4.64.0)\nRequirement already satisfied: urllib3 in /home/xstof/mambaforge/lib/python3.9/site-packages (from kaggle) (1.26.9)\nRequirement already satisfied: python-slugify in /home/xstof/mambaforge/lib/python3.9/site-packages (from kaggle) (6.1.2)\nRequirement already satisfied: certifi in /home/xstof/mambaforge/lib/python3.9/site-packages (from kaggle) (2022.6.15)\nRequirement already satisfied: requests in /home/xstof/mambaforge/lib/python3.9/site-packages (from kaggle) (2.27.1)\nRequirement already satisfied: six>=1.10 in /home/xstof/mambaforge/lib/python3.9/site-packages (from kaggle) (1.16.0)\nRequirement already satisfied: text-unidecode>=1.3 in /home/xstof/mambaforge/lib/python3.9/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /home/xstof/mambaforge/lib/python3.9/site-packages (from requests->kaggle) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /home/xstof/mambaforge/lib/python3.9/site-packages (from requests->kaggle) (3.3)\n\n\n\n! pip install sympy\n\nRequirement already satisfied: sympy in /home/xstof/mambaforge/lib/python3.9/site-packages (1.10.1)\nRequirement already satisfied: mpmath>=0.19 in /home/xstof/mambaforge/lib/python3.9/site-packages (from sympy) (1.2.1)\n\n\n\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\nisRunningOnKaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif isRunningOnKaggle: path = Path('../input/titanic')\nelse:\n    import zipfile,kaggle\n    path = Path('titanic')\n    kaggle.api.competition_download_cli(str(path))\n\ntitanic.zip: Skipping, found more recently modified local copy (use --force to force download)\n\n\n\nimport torch, numpy as np, pandas as pd\nnp.set_printoptions(linewidth=140)\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)"
  },
  {
    "objectID": "titanic - linear and nn from scratch.html#analyzing-and-cleaning-the-data",
    "href": "titanic - linear and nn from scratch.html#analyzing-and-cleaning-the-data",
    "title": "Titanic",
    "section": "Analyzing and cleaning the data",
    "text": "Analyzing and cleaning the data\n\n# for local run: unzipt the data first into 'titanic' directory\ndf = pd.read_csv('./titanic/train.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n\n\nThe idea here is that we multiply each column by some coefficient and add those all up to get to a result that will predict if a person survived or not.\n\ndf.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\nThis learns us that there’s some columns like “Age” and “Cabin” and “Embarked” that have no values - which is problematic as we can’t multiply “nothing” by a coefficient. We’ll replace these with “the mode” of the respective column, which is the most common value.\n\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\n\ndf.fillna(modes, inplace=True)\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\nHere we go - no more missing values.\n\ndf.describe(include=(np.number))\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      count\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n      891.000000\n    \n    \n      mean\n      446.000000\n      0.383838\n      2.308642\n      28.566970\n      0.523008\n      0.381594\n      32.204208\n    \n    \n      std\n      257.353842\n      0.486592\n      0.836071\n      13.199572\n      1.102743\n      0.806057\n      49.693429\n    \n    \n      min\n      1.000000\n      0.000000\n      1.000000\n      0.420000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      223.500000\n      0.000000\n      2.000000\n      22.000000\n      0.000000\n      0.000000\n      7.910400\n    \n    \n      50%\n      446.000000\n      0.000000\n      3.000000\n      24.000000\n      0.000000\n      0.000000\n      14.454200\n    \n    \n      75%\n      668.500000\n      1.000000\n      3.000000\n      35.000000\n      1.000000\n      0.000000\n      31.000000\n    \n    \n      max\n      891.000000\n      1.000000\n      3.000000\n      80.000000\n      8.000000\n      6.000000\n      512.329200\n    \n  \n\n\n\n\n\ndf['Fare'].hist()\n\n<AxesSubplot:>\n\n\n\n\n\nAs we can see, Fare is not equally distributed compared to the other variables, so lets take the log of that:\n\ndf['LogFare'] = np.log(df['Fare']+1)\n\n\ndf['LogFare'].hist()\n\n<AxesSubplot:>\n\n\n\n\n\n\ndf.Pclass.unique()\n\narray([3, 1, 2])\n\n\n\ndf.describe(include=[object])\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Ticket\n      Cabin\n      Embarked\n    \n  \n  \n    \n      count\n      891\n      891\n      891\n      891\n      891\n    \n    \n      unique\n      891\n      2\n      681\n      147\n      3\n    \n    \n      top\n      Braund, Mr. Owen Harris\n      male\n      347082\n      B96 B98\n      S\n    \n    \n      freq\n      1\n      577\n      7\n      691\n      646\n    \n  \n\n\n\n\nNow, we can’t mulitply non-numbers by coefficients, so let’s replace categories like Male and Female with numbers instead.\n\ndf = pd.get_dummies(df, columns=['Sex','Pclass','Embarked'])\ndf\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Name\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      LogFare\n      Sex_female\n      Sex_male\n      Pclass_1\n      Pclass_2\n      Pclass_3\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n    \n  \n  \n    \n      0\n      1\n      0\n      Braund, Mr. Owen Harris\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      B96 B98\n      2.110213\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      2\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      4.280593\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      3\n      1\n      Heikkinen, Miss. Laina\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      B96 B98\n      2.188856\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      3\n      4\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      3.990834\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n    \n    \n      4\n      5\n      0\n      Allen, Mr. William Henry\n      35.0\n      0\n      0\n      373450\n      8.0500\n      B96 B98\n      2.202765\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      Montvila, Rev. Juozas\n      27.0\n      0\n      0\n      211536\n      13.0000\n      B96 B98\n      2.639057\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      887\n      888\n      1\n      Graham, Miss. Margaret Edith\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      3.433987\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n    \n    \n      888\n      889\n      0\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      24.0\n      1\n      2\n      W./C. 6607\n      23.4500\n      B96 B98\n      3.196630\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      889\n      890\n      1\n      Behr, Mr. Karl Howell\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      3.433987\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      0\n    \n    \n      890\n      891\n      0\n      Dooley, Mr. Patrick\n      32.0\n      0\n      0\n      370376\n      7.7500\n      B96 B98\n      2.169054\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n    \n  \n\n891 rows × 18 columns\n\n\n\nNote how this get_dummies function removed the original columns and included the new ones.\n\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ndf[added_cols].head()\n\n\n\n\n\n  \n    \n      \n      Sex_male\n      Sex_female\n      Pclass_1\n      Pclass_2\n      Pclass_3\n      Embarked_C\n      Embarked_Q\n      Embarked_S\n    \n  \n  \n    \n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      0\n    \n    \n      2\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      3\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n    \n    \n      4\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1"
  },
  {
    "objectID": "titanic - linear and nn from scratch.html#prepare-dependent-and-independent-variables",
    "href": "titanic - linear and nn from scratch.html#prepare-dependent-and-independent-variables",
    "title": "Titanic",
    "section": "Prepare dependent and independent variables",
    "text": "Prepare dependent and independent variables\nOur dependent variable is easy: we’re trying to predict if someone survived Titanic:\n\nfrom torch import tensor\nt_dep = tensor(df.Survived)\nt_dep\n\ntensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0])\n\n\nOur independent variables are all the columns we want to use to predict the outcome with:\n\nindep_cols = ['Age','SibSp','Parch', 'LogFare'] + added_cols\nindep_cols\n\n['Age',\n 'SibSp',\n 'Parch',\n 'LogFare',\n 'Sex_male',\n 'Sex_female',\n 'Pclass_1',\n 'Pclass_2',\n 'Pclass_3',\n 'Embarked_C',\n 'Embarked_Q',\n 'Embarked_S']\n\n\n\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\nt_indep\n\ntensor([[22.0000,  1.0000,  0.0000,  2.1102,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [38.0000,  1.0000,  0.0000,  4.2806,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [26.0000,  0.0000,  0.0000,  2.1889,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  1.0000,  0.0000,  3.9908,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [35.0000,  0.0000,  0.0000,  2.2028,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  0.0000,  0.0000,  2.2469,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [54.0000,  0.0000,  0.0000,  3.9677,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        ...,\n        [25.0000,  0.0000,  0.0000,  2.0857,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [39.0000,  0.0000,  5.0000,  3.4054,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000],\n        [27.0000,  0.0000,  0.0000,  2.6391,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [19.0000,  0.0000,  0.0000,  3.4340,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n        [24.0000,  1.0000,  2.0000,  3.1966,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000],\n        [26.0000,  0.0000,  0.0000,  3.4340,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n        [32.0000,  0.0000,  0.0000,  2.1691,  1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000]])\n\n\n\nt_indep.shape\n\ntorch.Size([891, 12])\n\n\nThis tells us that we have data for 891 people (rows) with each having 12 indep variables associated (cols). These number represent row by row our x-values while we’ll initialize our weights at random."
  },
  {
    "objectID": "titanic - linear and nn from scratch.html#setting-up-a-linear-model",
    "href": "titanic - linear and nn from scratch.html#setting-up-a-linear-model",
    "title": "Titanic",
    "section": "Setting up a Linear Model",
    "text": "Setting up a Linear Model\n\nGet to predictions\nAt first we’ll do a manual single step of - calculating predictions - calculating the loss for these\n\n# make sure our random numbers are \"predictable\" when we re-run the notebook:\ntorch.manual_seed(42)\n\n<torch._C.Generator>\n\n\n\nn_coeff = t_indep.shape[1]\nn_coeff\n\n12\n\n\nWe’ll initialize 12 coefficients, one for each column upon which we’ll base our predictions.\n\ncoeffs = torch.rand(n_coeff)\ncoeffs\n\ntensor([0.8823, 0.9150, 0.3829, 0.9593, 0.3904, 0.6009, 0.2566, 0.7936, 0.9408, 0.1332, 0.9346, 0.5936])\n\n\nThis gets us coeffs which are between 0 and 1. Let’s make sure these are between -0.5 and 0.5 instead:\n\ncoeffs = torch.rand(n_coeff) - 0.5\ncoeffs\n\ntensor([ 0.3694,  0.0677,  0.2411, -0.0706,  0.3854,  0.0739, -0.2334,  0.1274, -0.2304, -0.0586, -0.2031,  0.3317])\n\n\nWe will multiply every row with these coefficients to get to our prediction, so we’ll do this for every person:\n\nt_indep * coeffs\n\ntensor([[ 8.1269,  0.0677,  0.0000, -0.1490,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [14.0374,  0.0677,  0.0000, -0.3022,  0.0000,  0.0739, -0.2334,  0.0000, -0.0000, -0.0586, -0.0000,  0.0000],\n        [ 9.6045,  0.0000,  0.0000, -0.1545,  0.0000,  0.0739, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [12.9292,  0.0677,  0.0000, -0.2817,  0.0000,  0.0739, -0.2334,  0.0000, -0.0000, -0.0000, -0.0000,  0.3317],\n        [12.9292,  0.0000,  0.0000, -0.1555,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [ 8.8657,  0.0000,  0.0000, -0.1586,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.2031,  0.0000],\n        [19.9478,  0.0000,  0.0000, -0.2801,  0.3854,  0.0000, -0.2334,  0.0000, -0.0000, -0.0000, -0.0000,  0.3317],\n        ...,\n        [ 9.2351,  0.0000,  0.0000, -0.1472,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [14.4068,  0.0000,  1.2055, -0.2404,  0.0000,  0.0739, -0.0000,  0.0000, -0.2304, -0.0000, -0.2031,  0.0000],\n        [ 9.9739,  0.0000,  0.0000, -0.1863,  0.3854,  0.0000, -0.0000,  0.1274, -0.0000, -0.0000, -0.0000,  0.3317],\n        [ 7.0187,  0.0000,  0.0000, -0.2424,  0.0000,  0.0739, -0.2334,  0.0000, -0.0000, -0.0000, -0.0000,  0.3317],\n        [ 8.8657,  0.0677,  0.4822, -0.2257,  0.0000,  0.0739, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [ 9.6045,  0.0000,  0.0000, -0.2424,  0.3854,  0.0000, -0.2334,  0.0000, -0.0000, -0.0586, -0.0000,  0.0000],\n        [11.8209,  0.0000,  0.0000, -0.1531,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.2031,  0.0000]])\n\n\nThe first col jumps out: it has way higher values than all the rest. We’ll want to normalize this and make sure all cols are between 0 and 1 by dividing by their max value:\n\nvals, indices = t_indep.max(dim=0)\n(vals, indices)\n\n(tensor([80.0000,  8.0000,  6.0000,  6.2409,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]),\n tensor([630, 159, 678, 258,   0,   1,   1,   9,   0,   1,   5,   0]))\n\n\n\nt_indep = t_indep / vals # Note: this uses \"broadcasting\"\nt_indep\n\ntensor([[0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.4750, 0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [0.3250, 0.0000, 0.0000, 0.3507, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.4375, 0.1250, 0.0000, 0.6395, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n        [0.4375, 0.0000, 0.0000, 0.3530, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.3000, 0.0000, 0.0000, 0.3600, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n        [0.6750, 0.0000, 0.0000, 0.6358, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n        ...,\n        [0.3125, 0.0000, 0.0000, 0.3342, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.4875, 0.0000, 0.8333, 0.5456, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000],\n        [0.3375, 0.0000, 0.0000, 0.4229, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n        [0.2375, 0.0000, 0.0000, 0.5502, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n        [0.3000, 0.1250, 0.3333, 0.5122, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000],\n        [0.3250, 0.0000, 0.0000, 0.5502, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n        [0.4000, 0.0000, 0.0000, 0.3476, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000]])\n\n\n\nt_indep * coeffs  # this now provides the values which are to be added row by row\n\ntensor([[ 0.1016,  0.0085,  0.0000, -0.0239,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [ 0.1755,  0.0085,  0.0000, -0.0484,  0.0000,  0.0739, -0.2334,  0.0000, -0.0000, -0.0586, -0.0000,  0.0000],\n        [ 0.1201,  0.0000,  0.0000, -0.0248,  0.0000,  0.0739, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [ 0.1616,  0.0085,  0.0000, -0.0451,  0.0000,  0.0739, -0.2334,  0.0000, -0.0000, -0.0000, -0.0000,  0.3317],\n        [ 0.1616,  0.0000,  0.0000, -0.0249,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [ 0.1108,  0.0000,  0.0000, -0.0254,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.2031,  0.0000],\n        [ 0.2493,  0.0000,  0.0000, -0.0449,  0.3854,  0.0000, -0.2334,  0.0000, -0.0000, -0.0000, -0.0000,  0.3317],\n        ...,\n        [ 0.1154,  0.0000,  0.0000, -0.0236,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [ 0.1801,  0.0000,  0.2009, -0.0385,  0.0000,  0.0739, -0.0000,  0.0000, -0.2304, -0.0000, -0.2031,  0.0000],\n        [ 0.1247,  0.0000,  0.0000, -0.0299,  0.3854,  0.0000, -0.0000,  0.1274, -0.0000, -0.0000, -0.0000,  0.3317],\n        [ 0.0877,  0.0000,  0.0000, -0.0388,  0.0000,  0.0739, -0.2334,  0.0000, -0.0000, -0.0000, -0.0000,  0.3317],\n        [ 0.1108,  0.0085,  0.0804, -0.0362,  0.0000,  0.0739, -0.0000,  0.0000, -0.2304, -0.0000, -0.0000,  0.3317],\n        [ 0.1201,  0.0000,  0.0000, -0.0388,  0.3854,  0.0000, -0.2334,  0.0000, -0.0000, -0.0586, -0.0000,  0.0000],\n        [ 0.1478,  0.0000,  0.0000, -0.0245,  0.3854,  0.0000, -0.0000,  0.0000, -0.2304, -0.0000, -0.2031,  0.0000]])\n\n\n\npreds = (t_indep * coeffs).sum(dim=1)\n(preds.shape, preds[:5])\n\n(torch.Size([891]), tensor([ 0.5729, -0.0826,  0.2705,  0.2971,  0.6235]))\n\n\nThis gives us 891 predictions (out of which only 5 shown above), one for each person (row in our original t_indep matrix).\n\n\nCalculate loss\n\nprediction_errors = preds-t_dep  # this gives the rror for each person, compared to the truth value from t_dep\nprediction_errors[:5]            # show 5 examples\n\ntensor([ 0.5729, -1.0826, -0.7295, -0.7029,  0.6235])\n\n\n\nabs_pred_errors = torch.abs(prediction_errors)\nabs_pred_errors[:5]\n\ntensor([0.5729, 1.0826, 0.7295, 0.7029, 0.6235])\n\n\n\nmean_error = abs_pred_errors.mean()\nmean_error\n\ntensor(0.5908)\n\n\nThis is our loss. Note that so far we’ve not done any optimization steps. To do this, we’ll create some helper functions first.\n\ndef calc_preds(coeffs, indeps): return (indeps * coeffs).sum(dim=1)\ndef calc_loss(coeffs, indeps, deps): return (torch.abs(calc_preds(coeffs, indeps)-deps)).mean()\n\n\ncalc_loss(coeffs, t_indep, t_dep) # see if our function gets to the same result as we've seen manually\n\ntensor(0.5908)\n\n\n\n\nGradient Descent\n\ncoeffs.requires_grad_()\n\ntensor([ 0.3694,  0.0677,  0.2411, -0.0706,  0.3854,  0.0739, -0.2334,  0.1274, -0.2304, -0.0586, -0.2031,  0.3317], requires_grad=True)\n\n\nNow are coefficients are marked so the gradients are being tracked throughout the calculations we do with those. Let’s calculate our loss with the coefficients:\n\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss\n\ntensor(0.5908, grad_fn=<MeanBackward0>)\n\n\nWe can now ask to calculate our gradients with respect to each weight / coefficient:\n\nloss.backward()\ncoeffs.grad\n\ntensor([ 0.0826,  0.0174, -0.0007,  0.0453,  0.4052, -0.2088, -0.0651,  0.0157,  0.2458, -0.0382, -0.0034,  0.2379])\n\n\n\ncoeffs\n\ntensor([ 0.3694,  0.0677,  0.2411, -0.0706,  0.3854,  0.0739, -0.2334,  0.1274, -0.2304, -0.0586, -0.2031,  0.3317], requires_grad=True)\n\n\nLet’s take our first weight: if we increase our first coefficient with 1, then our resulting loss will increase with that weight. This only goes for very small tiny amounts those, so let’s not try this with 1 but rather with for example 0.01\n\ncoeffs_increased_first = coeffs + tensor([0.01,0,0,0,0,0,0,0,0,0,0,0])\ncoeffs_increased_first # increased the first of our weights with 1\n\ntensor([ 0.3794,  0.0677,  0.2411, -0.0706,  0.3854,  0.0739, -0.2334,  0.1274, -0.2304, -0.0586, -0.2031,  0.3317],\n       grad_fn=<AddBackward0>)\n\n\n\ncoeffs.grad.zero_()  # don't forget to reset gradients as normally those get added every time they are calculated\nloss_new = calc_loss(coeffs_increased_first, t_indep, t_dep)\nloss_new\n\ntensor(0.5916, grad_fn=<MeanBackward0>)\n\n\nThe difference of this new loss with our previous loss is:\n\n(loss_new-loss)*100\n\ntensor(0.0826, grad_fn=<MulBackward0>)\n\n\nSee how our loss is now slightly higher, with approximately or exactly the same amount as our gradient told us.\nLet’s take a single Gradient Descent step:\n\ncoeffs.grad.zero_()\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\nwith torch.no_grad():\n    coeffs.sub_(coeffs.grad * 0.1)\n    coeffs.grad.zero_()\n    lower_loss = calc_loss(coeffs, t_indep, t_dep)\n    print(f'first our loss was {loss} and after our gradient descent step it was: {lower_loss}')\n\nfirst our loss was 0.590818464756012 and after our gradient descent step it was: 0.5592127442359924\n\n\n\n\nTraining the linear model\nFor training our model we also need a validation set. To create such set we can use RandomSplitter from FastAI:\n\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(df)\nprint(f'Our training set has {len(trn_split)} items while our validation set has {len(val_split)} items')\n\nOur training set has 713 items while our validation set has 178 items\n\n\nThe contents of those lists coming from RandomSplitter are merely indexes. We can apply them to our data:\n\ntrn_indep, trn_dep = t_indep[trn_split], t_dep[trn_split]\nval_indep, val_dep = t_indep[val_split], t_dep[val_split]\nlen(trn_indep),len(val_indep)\n\n(713, 178)\n\n\nLet’s create some functions:\n\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad(): update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n    \ndef init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_()\n\n\ndef train_model(epochs=50, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs): one_epoch(coeffs, lr=lr)\n    return coeffs\n\n\ncoeffs = train_model(50, 0.1)\n\n0.536; 0.518; 0.503; 0.489; 0.477; 0.465; 0.454; 0.443; 0.431; 0.420; 0.410; 0.399; 0.388; 0.378; 0.368; 0.358; 0.350; 0.343; 0.337; 0.330; 0.325; 0.319; 0.315; 0.310; 0.309; 0.302; 0.301; 0.296; 0.295; 0.290; 0.291; 0.285; 0.284; 0.280; 0.280; 0.275; 0.276; 0.270; 0.271; 0.266; 0.269; 0.264; 0.267; 0.263; 0.264; 0.261; 0.262; 0.259; 0.258; 0.257; \n\n\nSee how the loss is going down! Let’s look at the coefficients for each feature column:\n\ndef show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False)))\nshow_coeffs()\n\n{'Age': tensor(-0.2391),\n 'SibSp': tensor(0.0568),\n 'Parch': tensor(0.2175),\n 'LogFare': tensor(0.0012),\n 'Sex_male': tensor(-0.4694),\n 'Sex_female': tensor(0.3018),\n 'Pclass_1': tensor(0.6391),\n 'Pclass_2': tensor(0.4451),\n 'Pclass_3': tensor(0.4072),\n 'Embarked_C': tensor(0.1547),\n 'Embarked_Q': tensor(0.2134),\n 'Embarked_S': tensor(0.1760)}\n\n\nLet’s see how we do on our validation set, with data that has never been seen by our model:\n\npreds = calc_preds(coeffs, val_indep)\nprint(f'we have now {len(preds)} predictions for our validation set.  First 5 are:')\npreds[:5]\n\nwe have now 178 predictions for our validation set.  First 5 are:\n\n\ntensor([0.8891, 0.0799, 0.0212, 0.0987, 0.0775])\n\n\n\nresults = val_dep.bool() == (preds>0.5)\nresults\n\ntensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False,  True,  True, False,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True, False,  True,  True,\n         True, False, False,  True,  True,  True,  True, False, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True, False,  True, False,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n         True, False,  True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n         True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False,  True, False,  True,  True, False, False,  True,\n         True,  True, False,  True,  True,  True,  True, False,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n         True,  True, False,  True,  True,  True,  True,  True,  True,  True,  True, False,  True,  True,  True,  True, False, False, False,\n         True,  True, False,  True,  True, False, False,  True,  True,  True,  True, False,  True,  True,  True,  True,  True,  True, False,\n         True,  True,  True, False,  True, False, False])\n\n\n\nprint(f'our average accuracy is: {results.float().mean()}')\npreds\n\nour average accuracy is: 0.7921348214149475\n\n\ntensor([     0.8891,      0.0799,      0.0212,      0.0987,      0.0775,      0.0578,      0.8885,      0.8398,      0.2196,      0.8360,\n             0.0186,      0.0646,      0.1457,      0.8511,      0.0126,      0.2118,      0.1520,      0.8851,      0.0358,      0.7928,\n             0.0425,      0.1789,      1.0233,      0.8599,      0.0544,      0.0872,      0.9259,      0.1610,      0.0634,      0.8511,\n             0.9421,      0.9460,      0.0574,      0.0231,      0.8712,      0.9806,      0.2059,      1.0247,      0.0425,      0.8179,\n             0.0954,      0.0425,      0.1157,      1.0008,      0.0544,      0.9445,      0.0369,      0.0808,      0.0799,      0.8197,\n             0.2666,      0.1957,      0.1885,      0.0395,      0.0745,      0.0565,      0.0799,      0.0139,      0.0515,      0.0292,\n             0.2353,      0.0705,      0.1072,      0.0515,      0.7992,      0.3200,      0.0805,     -0.0021,      0.0755,      0.7890,\n             0.0365,      0.8650,      0.0595,      0.0425,      0.2745,      0.0664,      0.0425,      0.2401,      0.0954,      0.0118,\n             0.1999,      1.1206,      0.0455,      0.0425,      0.7740,      0.1360,      0.2785,      1.1556,      0.8257,      0.0835,\n             0.0186,      0.0425,      1.0068,      0.0032,      0.9125,      0.0233,      0.0149,      0.8655,      0.8571,      0.0948,\n             0.1038,      0.8511,      0.0186,      0.0246,      0.2609,      0.0267,      0.0246,      0.8475,      0.2576,      0.0346,\n             0.8974,      0.8001,      0.2746,      1.0636,      0.0425,      0.9242,      0.8581,      0.0799,     -0.0113,      0.9835,\n             0.1314,      0.1892,      0.0799,      0.9717,      0.0212,      0.2596,      0.8108,      0.8078,      0.0664,      0.0279,\n             1.0699,      0.0497,      0.8231,      0.8263,      1.0319,      0.0574,     -0.0241,      0.0276,      0.0410,      0.8189,\n             0.0625,      0.9481,     -0.0203,      0.0560,      0.8715,      0.9366,      0.0007,      0.8651,      1.1060,      0.7660,\n             0.8460,      0.9236,      0.8606,      0.1926,      0.0943,      0.1722,      0.0425,      0.8328,      0.0506,      0.0426,\n             1.0246,      0.0515,      0.0498,      0.3186,      0.9900,      1.0257,      0.8922,      0.7979,      0.3075,      0.1819,\n             0.9735,      0.0575,      1.0591,      0.0425,     -0.0173,      1.0826,      0.2746,      0.1308])\n\n\n\ndef acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)>0.5)).float().mean()\nprint(f'our accuracy is: {acc(coeffs)}')\n\nour accuracy is: 0.7921348214149475\n\n\n\n\nSigmoid normalization\nSome of our predictions are below zero and some are above 1 - let’s normalize those using a sigmoid function:\n\nimport sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));\n\n\n\n\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1)) # this redefines the prev function\n\n\ncoeffs = train_model(lr=100)\n\n0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; \n\n\n\nprint(f'our accuracy is: {acc(coeffs)}')  # is improved normally\n\nour accuracy is: 0.8258426785469055"
  },
  {
    "objectID": "titanic - linear and nn from scratch.html#submit-to-kaggle",
    "href": "titanic - linear and nn from scratch.html#submit-to-kaggle",
    "title": "Titanic",
    "section": "Submit to Kaggle",
    "text": "Submit to Kaggle\n\ntest_df = pd.read_csv('./titanic/test.csv')\ntest_df['Fare'] = test_df.Fare.fillna(0)\n\n\ntest_df.fillna(modes, inplace=True)\ntest_df['LogFare'] = np.log(test_df['Fare']+1)\ntest_df = pd.get_dummies(test_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n\ntest_indep = tensor(test_df[indep_cols].values, dtype=torch.float)\ntest_indep = test_indep / vals\nvals\n\ntensor([80.0000,  8.0000,  6.0000,  6.2409,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000])\n\n\n\ntest_df['Survived'] = (calc_preds(test_indep, coeffs)>0.5).int()\n\n\nsub_df = test_df[['PassengerId','Survived']]\n\n\nsub_df.to_csv('./titanic/sub.csv', index=False)\n\n\n# command line to submit to Kaggle:\n# kaggle competitions submit -c titanic -f nbs/titanic/sub.csv -m \"first submission\""
  },
  {
    "objectID": "titanic - linear and nn from scratch.html#using-a-matrix-product",
    "href": "titanic - linear and nn from scratch.html#using-a-matrix-product",
    "title": "Titanic",
    "section": "Using a Matrix product",
    "text": "Using a Matrix product\nWhen we multiplied our weights/coefficients with our inputs (independent vars), that looked like this:\n\nprint(f'shape for our indep variables: {val_indep.shape} and for our coeffs: {coeffs.shape}')\noutput = (val_indep*coeffs).sum(axis=1)  # [x1, x2, x3, ..., x12] * [c1, c2, c3, ..., c12]\nprint(f'shape of the output: {output.shape}')\noutput\n\nshape for our indep variables: torch.Size([178, 12]) and for our coeffs: torch.Size([12])\nshape of the output: torch.Size([178])\n\n\ntensor([ 13.6089, -14.7533, -15.4929, -13.1274, -13.3325, -13.6610,   4.0167,   5.8253, -23.0645,   3.4834, -22.6130, -15.7467, -22.3585,\n          4.4615, -22.6582, -12.4425, -12.8174,   5.8064, -13.5660,  -3.3766, -22.4632, -12.6487,  13.6359,   5.1308, -22.3926, -14.8863,\n         -3.0881, -12.7612, -22.3283,   4.4609,   5.8044,  -4.1316, -22.3695, -22.5843,  13.4964,  -3.7069, -12.4694,  14.1804, -22.4624,\n         -3.4474, -13.2288, -22.4624, -13.7275,  14.0347, -22.3875,  -4.0215, -22.4182, -22.6391, -14.7575,  -3.2149,  -5.0196, -23.2145,\n        -23.0521, -22.4864, -13.3600, -13.4725, -14.7532, -22.8404, -22.4062, -22.5187,  -5.1856, -22.5161, -13.0934, -22.4068,   5.3747,\n        -12.0506, -13.3138, -22.6985, -15.3090,   5.5082, -22.5007,   3.5429, -13.4537, -22.4559, -12.0498, -22.3138, -22.4624, -12.4381,\n        -13.2201, -13.7391, -13.2998,  13.7273, -22.4445, -22.4665,   5.4042,  -6.6977,  -5.0811,   7.0301,  12.6450, -13.2951, -22.6109,\n        -22.4624,  14.0867, -15.6054,   3.7333,  -6.5654, -13.6972,   4.1465,   4.4984, -14.6649, -22.6348,   4.4609, -22.6132, -22.5757,\n        -12.3268, -13.6512, -22.5684,   5.3629,  -5.2202, -22.7411,   5.5522,  -3.5265, -12.0436,  14.5804, -22.4624,  13.3295,  -4.1303,\n        -14.7532, -22.8019,  13.6447, -23.0690,  -5.9866, -14.7532,  -3.7630, -15.4929, -12.1373,  -3.2684,  -3.2868, -22.3059, -13.8394,\n          7.3663, -22.6140,   5.5247,   5.7410,  14.0335, -22.3687, -13.9684, -22.5497,  -6.6439,   5.7141, -13.4263,  13.4812, -22.8617,\n        -14.9032,  -3.8442,   6.3010, -22.7220,   3.5589,   7.2280,   3.2512,  -3.4625,   4.2525,   5.9479, -15.2217, -15.0676, -24.0177,\n        -22.4624,  -3.3451, -13.5013, -22.4387,  14.1658,  -6.5817, -22.6050, -12.5964,  13.7395,   6.7952,   5.6328,   5.5629, -13.2952,\n        -12.6240,  -3.5321, -22.3449,   7.0628, -22.4559, -22.8380,   6.7744, -12.0429, -22.4375], grad_fn=<SumBackward1>)\n\n\nHow the operator * works on tensors:\n\nt1 = torch.tensor([1,2,3])\nt2 = torch.tensor([2])\nt3 = t1 * t1 # elementwise multiplication: tensor([1, 4, 9])\nt4 = t1 * t2  # broadcasting: tensor([2, 4, 6])\nt5 = torch.tensor([2,3])\n# this gives an error: t6 = t1 * t5 => The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\nt7 = torch.tensor([1,2,3]) * torch.tensor([2,3,4]) # tensor([ 2,  6, 12]) => element wise\n\nWe can however be more efficient by using a matrix dot product to calculate our indep * coeffs:\n\n# For the behavior of matrix multiplication operator @, see:\n# https://pytorch.org/docs/stable/generated/torch.matmul.html\n# the behavior depends on the dimensionality of the params:\ntorch.tensor([1,2,3])@torch.tensor([2,4,6]) # is: 2 + 8 + 18 = 28\na = torch.tensor( [2, 3, 4])\nb = torch.tensor([[1, 2, 3],\n                  [2, 3, 1],\n                  [3, 2, 1]])\nprint(f'a is: {a}')\nprint(f'b is {b}')\nprint(f'a is a 1x3 matrix and b a 3x3 matrix, so result should be a 1x3 matrix')\na@b # [2+6+12=20, 4+9+8=21, 6+3+4=13]\n\na is: tensor([2, 3, 4])\nb is tensor([[1, 2, 3],\n        [2, 3, 1],\n        [3, 2, 1]])\na is a 1x3 matrix and b a 3x3 matrix, so result should be a 1x3 matrix\n\n\ntensor([20, 21, 13])\n\n\n\noutput = val_indep @ coeffs  # this simple operation replaces: (val_indep*coeffs).sum(axis=1)\nprint(f'shape of the output: {output.shape}')\noutput\n\nshape of the output: torch.Size([178])\n\n\ntensor([ 13.6089, -14.7533, -15.4929, -13.1274, -13.3325, -13.6610,   4.0167,   5.8253, -23.0645,   3.4834, -22.6130, -15.7467, -22.3585,\n          4.4615, -22.6582, -12.4425, -12.8174,   5.8064, -13.5660,  -3.3766, -22.4632, -12.6487,  13.6359,   5.1308, -22.3926, -14.8863,\n         -3.0881, -12.7612, -22.3283,   4.4609,   5.8044,  -4.1316, -22.3695, -22.5843,  13.4964,  -3.7069, -12.4694,  14.1804, -22.4624,\n         -3.4474, -13.2288, -22.4624, -13.7275,  14.0347, -22.3875,  -4.0215, -22.4182, -22.6391, -14.7575,  -3.2149,  -5.0196, -23.2145,\n        -23.0521, -22.4864, -13.3600, -13.4725, -14.7532, -22.8404, -22.4062, -22.5187,  -5.1856, -22.5161, -13.0934, -22.4068,   5.3747,\n        -12.0506, -13.3138, -22.6985, -15.3090,   5.5082, -22.5007,   3.5429, -13.4537, -22.4559, -12.0498, -22.3138, -22.4624, -12.4381,\n        -13.2201, -13.7391, -13.2998,  13.7273, -22.4445, -22.4665,   5.4042,  -6.6977,  -5.0811,   7.0301,  12.6450, -13.2951, -22.6109,\n        -22.4624,  14.0867, -15.6054,   3.7333,  -6.5654, -13.6972,   4.1465,   4.4984, -14.6649, -22.6348,   4.4609, -22.6132, -22.5757,\n        -12.3268, -13.6512, -22.5684,   5.3629,  -5.2202, -22.7411,   5.5522,  -3.5265, -12.0436,  14.5804, -22.4624,  13.3295,  -4.1303,\n        -14.7532, -22.8019,  13.6447, -23.0690,  -5.9866, -14.7532,  -3.7630, -15.4929, -12.1373,  -3.2684,  -3.2868, -22.3059, -13.8394,\n          7.3663, -22.6140,   5.5247,   5.7410,  14.0335, -22.3687, -13.9684, -22.5497,  -6.6439,   5.7141, -13.4263,  13.4812, -22.8617,\n        -14.9032,  -3.8442,   6.3010, -22.7220,   3.5589,   7.2280,   3.2512,  -3.4625,   4.2525,   5.9479, -15.2217, -15.0676, -24.0177,\n        -22.4624,  -3.3451, -13.5013, -22.4387,  14.1658,  -6.5817, -22.6050, -12.5964,  13.7395,   6.7952,   5.6328,   5.5629, -13.2952,\n        -12.6240,  -3.5321, -22.3449,   7.0628, -22.4559, -22.8380,   6.7744, -12.0429, -22.4375], grad_fn=<MvBackward0>)\n\n\n\ndef calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs) # this redefines the prev function\n\n\ncoeffs = train_model(lr=100)\ncoeffs\n\n0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; \n\n\ntensor([ -1.4997,  -1.4376,  -0.6251,   0.2758, -10.4215,   8.7926,   3.8766,   2.6415,  -6.4879,   1.7732,   2.5102,  -5.2006],\n       requires_grad=True)\n\n\n\nacc(coeffs)\n\ntensor(0.8258)\n\n\nOur coefficients are now in the form of a simple row-vector. The behavior of the @ operator is slightly differnet and we’ll later need to do matrix-matrix multiplications for which we’ll need the coefficients to be a column vector instead of a row vector. We can do this by providing an additional param to torch.rand:\n\ndef init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_()\ncol_coeffs = init_coeffs()\ncol_coeffs\n\ntensor([[0.0672],\n        [0.0732],\n        [0.0142],\n        [0.0499],\n        [0.0317],\n        [0.0259],\n        [0.0549],\n        [0.0587],\n        [0.0447],\n        [0.0813],\n        [0.0351],\n        [0.0245]], requires_grad=True)\n\n\n\ntrn_dep # see how this is a simple row vector - this needs to become a col vector\n\ntensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n        1, 1, 0, 0, 0, 0, 1, 0, 0])\n\n\n\na = torch.randn(4) # TODO try transpose this to col vector\nprint(f'a is: \\n {a}')\n# b = torch.transpose(a,0,1) => this does not work: Dimension out of range (expected to be in range of [-1, 0], but got 1)\na2 = a.resize(1,4)\nprint(f'a2 after resize of a has 1 row and 4 cols: \\n {a2}')\nb = torch.transpose(a2,0,-1)\nprint(f'a2 transposed is now a column vector: \\n {b}')\n\na is: \n tensor([-1.8477,  0.8635, -0.9901,  1.6304])\na2 after resize of a has 1 row and 4 cols: \n tensor([[-1.8477,  0.8635, -0.9901,  1.6304]])\na2 transposed is now a column vector: \n tensor([[-1.8477],\n        [ 0.8635],\n        [-0.9901],\n        [ 1.6304]])\n\n\n\nprint(f'shape of trn_dep is: {trn_dep.shape}')\nprint(f'shape of val_dep is: {val_dep.shape}')\ntrn_dep.resize_(1,713)\ntrn_dep.transpose_(0,1)\nval_dep.resize_(1,178)\nval_dep.transpose_(0,1)\nprint('')\n\nshape of trn_dep is: torch.Size([713, 1])\nshape of val_dep is: torch.Size([178, 1])\n\n\n\n\ncoeffs = train_model(lr=100)\n\n0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; \n\n\n\nacc(coeffs)\n\ntensor(0.8258)"
  },
  {
    "objectID": "titanic - linear and nn from scratch.html#towards-a-neural-network",
    "href": "titanic - linear and nn from scratch.html#towards-a-neural-network",
    "title": "Titanic",
    "section": "Towards a Neural Network",
    "text": "Towards a Neural Network\nLet’s make a neural network with 2 layers. Each layer has a set of weights, one for each feature column (12 in total: n_coeff). The output for layer 1 is hidden and connected to layer 2, so we can choose how many hidden outputs we make this; let’s call this n_hidden.\nLayer 1: - input is: 1 x n_coeff (rowvector of 12 inputs) - weights are: n_coeff x n_hidden (so we have n_hidden outputs) we will divide those coefficients by the number of hidden outputs, so size remains proportional when those are added up in the next layer - result is: 1 x n_hidden\nLayer 2: - input is: 1 x n_hidden - weights are: n_hidden x 1 (we just need a single output) - result is: 1 output\n\ndef init_coeffs(n_hidden = 20):\n    coeffs_layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden\n    coeffs_layer2 = torch.rand(n_hidden, 1)-0.5\n    const = torch.rand(1)[0]\n    return coeffs_layer1.requires_grad_(), coeffs_layer2.requires_grad_(), const.requires_grad_()\n\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs,indeps):\n    l1,l2,const = coeffs\n    res = F.relu(indeps@l1)\n    res = res@l2 + const\n    return torch.sigmoid(res)\n\nTo update our coefficients in both layers we’ll loop through them:\n\ndef update_coeffs(coeffs, lr):\n    for layer in coeffs:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\ncoeffs = train_model(lr=20)\n\n0.542; 0.334; 0.238; 0.215; 0.210; 0.218; 0.209; 0.200; 0.201; 0.202; 0.205; 0.197; 0.194; 0.194; 0.194; 0.194; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; \n\n\nOur loss is now lower than before!\n\nacc(coeffs)\n\ntensor(0.8258)"
  },
  {
    "objectID": "titanic - linear and nn from scratch.html#deep-learning",
    "href": "titanic - linear and nn from scratch.html#deep-learning",
    "title": "Titanic",
    "section": "Deep Learning",
    "text": "Deep Learning\nwe’ve now build a 2 layer model for a neural network. See Linear Model and Neural Net from scratch for this last mile using more layers."
  },
  {
    "objectID": "nanogpt-bigram-model.html",
    "href": "nanogpt-bigram-model.html",
    "title": "NanoGPT - Bigram Model",
    "section": "",
    "text": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntorch.set_printoptions(precision=1, sci_mode=False, profile='short')\ngenerator = torch.manual_seed(42)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice\n\n'cuda'\n\n\nRead our input data:\n\ninputfile = './tiny_shakespeare.txt'\nwith open(inputfile, 'r', encoding='utf-8') as f:\n    text = f.read()\n\nprint(f'the length of the shakespeare text is: {len(text)} characters')\nprint(f'the first 200 are: \\n')\nprint(text[:200])\n\nthe length of the shakespeare text is: 1115394 characters\nthe first 200 are: \n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you\n\n\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(f'the size of our vocabulary is: {vocab_size}')\nprint(f'and looks like: {\"\".join(chars)}')\n\nthe size of our vocabulary is: 65\nand looks like: \n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n\n\nWe now need to be able to move from characters to tokens and back: from tokens to characters. In a real language model, a token is more than a single character. Real tokenizers like tiktoken are used. In our case, we’ll just simple map from character to token index and back:\n\nstoi = {ch:i for i, ch in enumerate(chars)}\nitos = {i:ch for i, ch in enumerate(chars)}\nencode = lambda str: [stoi[c] for c in str]\ndecode = lambda idx_list: ''.join([itos[idx] for idx in idx_list])\n\n\n# let's test this:\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\nWe will now encode the entire dataset:\n\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(f'shape of data: {data.shape}')\nprint(f'a piece of this data: {data[:100]}')\n\nshape of data: torch.Size([1115394])\na piece of this data: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n\n\n\n\n\n\ntrain_fraction = .9\nn = int(train_fraction*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\nWe will look at our data using a given ‘context_size’ (also called ‘block_size’). This is how many tokens we’ll have maximum as input. It can be less, in which case we’ll pad our input with preceding padding characters.\n\nblock_size = 8\n\nWe will look at each block of 8 characters (or less) and try to determine the next character. This means that if we have 8 chars and we need to predict the nineth, we will need to shift the ground_truth vector by one:\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nx,y\n\n(tensor([18, 47, 56, 57, 58,  1, 15, 47]),\n tensor([47, 56, 57, 58,  1, 15, 47, 58]))\n\n\nHidden in this series of 8 characters are 8 ‘exercises’ as input when training our model:\n\nfor i in range(block_size):\n    print(f'{i+1} - when input is tensor {x[:i+1]} then output is: {y[i]}')\n\n1 - when input is tensor tensor([18]) then output is: 47\n2 - when input is tensor tensor([18, 47]) then output is: 56\n3 - when input is tensor tensor([18, 47, 56]) then output is: 57\n4 - when input is tensor tensor([18, 47, 56, 57]) then output is: 58\n5 - when input is tensor tensor([18, 47, 56, 57, 58]) then output is: 1\n6 - when input is tensor tensor([18, 47, 56, 57, 58,  1]) then output is: 15\n7 - when input is tensor tensor([18, 47, 56, 57, 58,  1, 15]) then output is: 47\n8 - when input is tensor tensor([18, 47, 56, 57, 58,  1, 15, 47]) then output is: 58\n\n\n\n\n\nTo train our model, we’ll build batches of examples for the model to learn from. We need to settle on a batch size for this:\n\nblock_size = 8\nbatch_size = 4\n\n\ndef get_batch(split, batch_size=batch_size):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(train_data) - block_size, (batch_size,)) # this gets us a tensor like [476250,  18899, 645194, 831150]\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\n\nxb, yb = get_batch('train')\nxb, yb\n\n(tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n         [ 1, 58, 46, 43, 56, 43,  1, 41],\n         [17, 26, 15, 17, 10,  0, 32, 53],\n         [57, 58,  6,  1, 61, 47, 58, 46]]),\n tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n         [58, 46, 43, 56, 43,  1, 41, 39],\n         [26, 15, 17, 10,  0, 32, 53,  1],\n         [58,  6,  1, 61, 47, 58, 46,  0]]))\n\n\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\nwhen input is [57] the target: 1\nwhen input is [57, 1] the target: 46\nwhen input is [57, 1, 46] the target: 47\nwhen input is [57, 1, 46, 47] the target: 57\nwhen input is [57, 1, 46, 47, 57] the target: 1\nwhen input is [57, 1, 46, 47, 57, 1] the target: 50\nwhen input is [57, 1, 46, 47, 57, 1, 50] the target: 53\nwhen input is [57, 1, 46, 47, 57, 1, 50, 53] the target: 60\nwhen input is [1] the target: 58\nwhen input is [1, 58] the target: 46\nwhen input is [1, 58, 46] the target: 43\nwhen input is [1, 58, 46, 43] the target: 56\nwhen input is [1, 58, 46, 43, 56] the target: 43\nwhen input is [1, 58, 46, 43, 56, 43] the target: 1\nwhen input is [1, 58, 46, 43, 56, 43, 1] the target: 41\nwhen input is [1, 58, 46, 43, 56, 43, 1, 41] the target: 39\nwhen input is [17] the target: 26\nwhen input is [17, 26] the target: 15\nwhen input is [17, 26, 15] the target: 17\nwhen input is [17, 26, 15, 17] the target: 10\nwhen input is [17, 26, 15, 17, 10] the target: 0\nwhen input is [17, 26, 15, 17, 10, 0] the target: 32\nwhen input is [17, 26, 15, 17, 10, 0, 32] the target: 53\nwhen input is [17, 26, 15, 17, 10, 0, 32, 53] the target: 1\nwhen input is [57] the target: 58\nwhen input is [57, 58] the target: 6\nwhen input is [57, 58, 6] the target: 1\nwhen input is [57, 58, 6, 1] the target: 61\nwhen input is [57, 58, 6, 1, 61] the target: 47\nwhen input is [57, 58, 6, 1, 61, 47] the target: 58\nwhen input is [57, 58, 6, 1, 61, 47, 58] the target: 46\nwhen input is [57, 58, 6, 1, 61, 47, 58, 46] the target: 0\n\n\nxb is what will be our input into our transformer:\n\nevery row represents a set of training examples, each drawn from the same set of consecutive characters, up to the context_length\nevery column represents a token position and a sample, consisting of the token at the position in the row and all tokens that come before it\n\n\nxb\n\ntensor([[57,  1, 46, 47, 57,  1, 50, 53],\n        [ 1, 58, 46, 43, 56, 43,  1, 41],\n        [17, 26, 15, 17, 10,  0, 32, 53],\n        [57, 58,  6,  1, 61, 47, 58, 46]])"
  },
  {
    "objectID": "nanogpt-bigram-model.html#creating-a-bigram-model",
    "href": "nanogpt-bigram-model.html#creating-a-bigram-model",
    "title": "NanoGPT - Bigram Model",
    "section": "Creating a Bigram model",
    "text": "Creating a Bigram model\n\nCreate a Bigram module\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        \n        # add an embedding layer for lookup of tokens (their index) and translating them in a vector:\n        # for now, as the number of embedding dimensions, use our vocab_size\n        # this then can be used to, given a char, lookup the probability of the next char\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n        \n    def forward(self, idx ,targets=None):\n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the logits from idx, using our embedding table\n        #                                        #    idx dimension: B x T\n        logits = self.token_embedding_table(idx) # result dimension: B x T x C\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\ndef generate_some_text(model=m, max_new_tokens=100):\n    generated_tokens = model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=max_new_tokens)[0].tolist()\n    print(decode(generated_tokens))\n    \ngenerate_some_text()\n\ntorch.Size([32, 65])\ntensor(4.7, grad_fn=<NllLossBackward0>)\n\nF$z\nE?kFwu\n'buM-,Y3fYNsA3xp.mpsAqaZ-RUldc;F\nM$GfYCUCkFO-bJbz-R;O!slp.FNsJDV'jRzIMQ'EdRbqAoWTjrniaIIa\n\n\n\n\nTrain the Bigram module\nTo train our model parameters, we’ll fetch an optimizer: Adam in this case.\n\noptimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n\n\nbatch_size = 32\nsteps=[]\nlosses=[]\n\nfor step in range(10000):\n    # get a batch\n    xb, yb = get_batch('train')\n    \n    # calculate the loss\n    logits,loss = m(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \n    if step%100 == 0 :\n        steps.append(step)\n        losses.append(loss.item())\n        # print(f'loss for iteration {step} is: {loss}')\n\nplt.plot(steps, losses)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.title('loss over time')\n\nText(0.5, 1.0, 'loss over time')\n\n\n\n\n\n\ngenerate_some_text()\n\n\nSx\nAYO:\nTutthanot ucll tathoman?\nSABs.O:\nCEETunodwowfo t g tathfovy!\nA'seathovizARU'st ine feirrdWca\n\n\nSee how this looks still like garbage, but already way better than it used to be! Let’s do this once more but up our batch size to see the impact on our loss curve.\n\nm2 = BigramLanguageModel(vocab_size)\noptimizer2 = torch.optim.Adam(m2.parameters(), lr=1e-3)\nsteps=[]\nlosses=[]\n\nfor step in range(10000):\n    # get a batch\n    xb, yb = get_batch('train', batch_size=64)\n    \n    # calculate the loss\n    logits,loss = m2(xb,yb)\n    optimizer2.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer2.step()\n    \n    if step%100 == 0 :\n        steps.append(step)\n        losses.append(loss.item())\n        # print(f'loss for iteration {step} is: {loss}')\n\nplt.plot(steps, losses)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.ylim(2,5)\nplt.title('loss over time')\nplt.show()\n\n\n\n\nWe can say that increasing the batch_size smooths out our loss curve."
  },
  {
    "objectID": "nanogpt-bigram-model.html#code-cleanup",
    "href": "nanogpt-bigram-model.html#code-cleanup",
    "title": "NanoGPT - Bigram Model",
    "section": "Code Cleanup",
    "text": "Code Cleanup\nSee the Let’s build GPT: from scratch, in code, spelled out. for the section where some code is cleaned up first before moving on.\n\n# hyper parameters:\nbatch_size = 64\nblock_size = 8\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32             # number of embedding dimensions\n\nclass BigramLanguageModel2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # add an embedding layer for lookup of tokens (their index) and translating them in a vector:\n        # for now, as the number of embedding dimensions, use our vocab_size\n        # this then can be used to, given a char, lookup the probability of the next char\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        \n        # add a language modeling head that will translate from token embeddings to logits:\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx ,targets=None):\n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the token embeddings from idx, using our embedding table\n        #                                        #    idx dimension: B x T\n        tok_embd = self.token_embedding_table(idx) # token embeddings with result dimension: B x T x C\n        logits = self.lm_head(tok_embd)            # (B x T x C) @ (C, vocab_size) = (B x T x vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\n\nmodel = BigramLanguageModel2()\noptim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nsteps=[]\nlosses=[]\n\nfor step in range(max_iters):\n    # get a batch\n    xb, yb = get_batch('train', batch_size=batch_size)\n    \n    # calculate the loss\n    logits,loss = model(xb,yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n    \n    if step%eval_interval == 0 :\n        steps.append(step)\n        losses.append(loss.item())\n        # print(f'loss for iteration {step} is: {loss}')\n\nplt.plot(steps, losses)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.ylim(2,5)\nplt.title('loss over time')\nplt.show()\n\n\n\n\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\n\nlast recorded loss was 2.5102274417877197\n\n\nBefore we move on: it would also be useful for the network, not just to know the tokens itself but also their positions in the sequence. For this we create a position embedding table:\n\n# hyper parameters:\nbatch_size = 64\nblock_size = 8\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32             # number of embedding dimensions\n\nclass BigramLanguageModel3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # add an embedding layer for lookup of tokens (their index) and translating them in a vector:\n        # for now, as the number of embedding dimensions, use our vocab_size\n        # this then can be used to, given a char, lookup the probability of the next char\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     # give it index in vocab and it looks up the embedding vector in n_embd dims\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # each position will also get its own embedding vector\n        \n        # add a language modeling head that will translate from token embeddings to logits:\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx ,targets=None):\n        B,T = idx.shape()\n        \n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the token embeddings from idx, using our embedding table\n        #                                                    #                             idx dimension: B x T\n        tok_embd = self.token_embedding_table(idx)           # token embeddings with result dimension:    B x T x C\n        pos_embd = self.pos(torch.arange(T, device=device))  # position embeddings with result dimension:     T x C\n        x = tok_embd + pos_embd # B x T x C\n        logits = self.lm_head(x)            # (B x T x C) @ (C, vocab_size) = (B x T x vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\nBy means of a small sample with random numbers, let’s have a look to how the token embeddings are added to the positional embeddings. For every item in the batch, for each of the 4 tokens, we add the embedding for the position of the token to the token embedding for for the token:\n\n# code example: concatenating the token embedding with the positional embedding:\nb2,t2,c2 = (2,4,6)  # batch of 2 sample, each having 4 tokens, which each is represented by n_embd of 6 dimensions\n\ntoken_embedding = torch.randn((b2,t2,c2))\npos_embedding = torch.randn((t2,c2))\nsum_embeddings = token_embedding + pos_embedding\ntoken_embedding, pos_embedding, sum_embeddings\n\n(tensor([[[-0.4,  0.8,  0.3,  0.4,  0.3, -0.4],\n          [-0.2,  2.3, -0.3,  0.9,  0.7, -0.4],\n          [-0.1, -0.5,  1.3, -0.2, -0.3,  1.0],\n          [-1.3,  0.5, -0.5,  0.9,  1.3,  0.3]],\n \n         [[ 0.1,  0.1, -0.2, -0.0, -0.7, -0.9],\n          [-1.6,  0.3, -0.7, -1.6,  1.4,  1.0],\n          [ 0.2,  0.1, -0.8, -0.9, -0.7, -2.6],\n          [ 1.0, -0.6, -0.3, -0.7, -0.4, -0.1]]]),\n tensor([[    -0.1,      0.3,      0.2,     -0.4,      0.4,     -0.4],\n         [     0.0,     -0.5,     -0.4,      1.3,      0.9,     -0.8],\n         [     1.3,     -0.3,      1.1,     -0.1,     -1.2,     -0.2],\n         [    -0.8,     -0.3,      0.3,      0.0,     -1.3,      1.1]]),\n tensor([[[-0.4,  1.2,  0.5,  0.0,  0.7, -0.8],\n          [-0.2,  1.7, -0.7,  2.2,  1.6, -1.2],\n          [ 1.2, -0.8,  2.4, -0.3, -1.5,  0.8],\n          [-2.1,  0.2, -0.1,  0.9, -0.0,  1.4]],\n \n         [[ 0.0,  0.5, -0.0, -0.4, -0.3, -1.3],\n          [-1.6, -0.2, -1.2, -0.3,  2.3,  0.2],\n          [ 1.5, -0.3,  0.3, -1.0, -1.8, -2.8],\n          [ 0.1, -0.9,  0.0, -0.7, -1.7,  1.0]]]))"
  },
  {
    "objectID": "nanogpt-bigram-model.html#self-attention",
    "href": "nanogpt-bigram-model.html#self-attention",
    "title": "NanoGPT - Bigram Model",
    "section": "Self Attention",
    "text": "Self Attention\nLet’s now see how self-attention works. (See the video Let’s build GPT: from scratch, in code, spelled out. for more detail.\n\nb2,t2,c2 = (2,4,6) \nx = torch.randn(b2,t2,c2)\ntril = torch.tril(torch.ones(t2,t2))\nweights = torch.zeros((t2,t2))  # this line makes it so that currently there's an average being made of token and prev tokens, all with same weight\nweights = weights.masked_fill(tril == 0, float('-inf'))\nweights = F.softmax(weights, dim=1)\nout = weights @ x \nprint(f'x: \\n{x}')\nprint(f'------')\nprint(f'tril: \\n{tril}')\nprint(f'------')\nprint(f'weights: \\n{weights}')\nprint(f'------')\nprint(f'out: \\n{out}')\n\nx: \ntensor([[[    -1.3,      0.4,      0.0,     -1.7,     -0.7,      0.5],\n         [     0.3,      0.4,      0.5,     -2.4,     -2.2,      0.4],\n         [    -0.4,      0.9,     -0.1,      0.3,      1.1,      1.3],\n         [    -0.5,      0.6,      0.1,     -0.6,      0.2,      1.8]],\n\n        [[    -1.6,      0.9,      0.4,     -0.7,     -0.1,     -1.6],\n         [    -1.2,     -0.5,     -0.4,     -0.9,     -1.0,      0.5],\n         [    -0.6,      0.0,     -0.5,     -0.3,      0.5,     -0.1],\n         [    -0.2,     -0.9,     -0.6,     -0.1,     -0.4,      0.6]]])\n------\ntril: \ntensor([[1., 0., 0., 0.],\n        [1., 1., 0., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 1.]])\n------\nweights: \ntensor([[1.0, 0.0, 0.0, 0.0],\n        [0.5, 0.5, 0.0, 0.0],\n        [0.3, 0.3, 0.3, 0.0],\n        [0.2, 0.2, 0.2, 0.2]])\n------\nout: \ntensor([[[-1.3,  0.4,  0.0, -1.7, -0.7,  0.5],\n         [-0.5,  0.4,  0.3, -2.1, -1.5,  0.4],\n         [-0.5,  0.6,  0.2, -1.3, -0.6,  0.7],\n         [-0.5,  0.6,  0.1, -1.1, -0.4,  1.0]],\n\n        [[-1.6,  0.9,  0.4, -0.7, -0.1, -1.6],\n         [-1.4,  0.2, -0.0, -0.8, -0.5, -0.5],\n         [-1.1,  0.1, -0.2, -0.6, -0.2, -0.4],\n         [-0.9, -0.1, -0.3, -0.5, -0.2, -0.1]]])\n\n\nLet’s change this:\n\nevery token, at every position, will emit two vectors:\n\nquery: “what am I looking for?”\nkey: “what do I contain?”\n\nwe then do a dot product between the keys and the queries to get this “affinity”\n\nFor a given token, its query will do a dot product with the keys of all the previous tokens. That dot product will become our weights. Weights was a matrix of dimensions T x T and multiplying x by those weights gave us the same dimensions as x itself: B x T x C.\nWe calculate the:\n\nkeys separately by multiplying a key-matrix by the input tensor; giving us a B x T x head_size (a vector of length head_size for every position)\nqueries separately by multiplying a query-matrix by the input tensor; giving us a B x T x head_size (a vector of length head_size for every position)\n\nThen we calculate the affinities between the queries and the keys, by doing a dot product, which can be done in batch using a multrix multiplication:  weights = q @ k.T(-2,-1) (taking into account dimensions; we do not want to touch the batch dimension and just switch the last two dimensions). This gives us a result of shape B x T x T. This means: a different TxT matrix for every item in our batch. Note that the key and query matrix is the same for all of our items in our batch.\n\nb2,t2,c2 = (2,4,6) \nx = torch.randn(b2,t2,c2)\n\n# a single head of self-attention:\nhead_size = 8\n\nkey = nn.Linear(c2, head_size, bias=False)   # this will do matrix multiply with fixed weights\nquery = nn.Linear(c2, head_size, bias=False) # this will do matrix multiply with fixed weights\n\n# by multiplying x by the linear layers for key and query, all of the samples, all of the tokens will, in parallel, calculate their key and query independently\n# at this point in time, there has been no communication between tokens yet\nk = key(x)   # (B x T x C) @ (C, head_size) => (B x T x head_size)\nq = query(x)\n\n# now all the queries will need to dot-product with all the keys:\nweights = q @ k.transpose(-2,-1)  # (B x T x head_size) @ (B x head_size x T) => B x T x T\n\n# this means that for every row in our batch B, we will have a weight matrix T x T now with the affinities\n# we will use this instead of the weights matrix that we filled with zeroes before.  \n# what we did before was:\n#   weights = torch.zeros((t2,t2))  # this line makes it so that currently there's an average being made of token and prev tokens, all with same weight\n\ntril = torch.tril(torch.ones(t2,t2))\nweights = weights.masked_fill(tril == 0, float('-inf'))\nweights = F.softmax(weights, dim=1)\nout = weights @ x \nprint(f'x: \\n{x}')\nprint(f'------')\nprint(f'tril: \\n{tril}')\nprint(f'------')\nprint(f'weights: \\n{weights}')\nprint(f'------')\nprint(f'out: \\n{out}')\n\nx: \ntensor([[[ 2.4,  0.6,  1.4, -0.7,  0.9,  0.6],\n         [-0.5,  0.5, -1.0, -1.0,  0.1, -2.0],\n         [ 0.1,  1.3,  0.2,  0.1,  0.6, -0.2],\n         [-0.0, -0.7, -2.2, -0.3,  2.4,  1.3]],\n\n        [[-0.2,  0.0,  1.2, -0.8,  0.3, -0.7],\n         [ 1.8,  0.8,  0.2,  1.0, -0.2,  1.4],\n         [ 0.1,  2.0, -1.0, -0.4,  0.1,  1.1],\n         [ 0.2, -0.9,  0.3,  0.2,  2.2, -0.4]]])\n------\ntril: \ntensor([[1., 0., 0., 0.],\n        [1., 1., 0., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 1.]])\n------\nweights: \ntensor([[[0.7, 0.0, 0.0, 0.0],\n         [0.0, 0.5, 0.0, 0.0],\n         [0.1, 0.3, 0.5, 0.0],\n         [0.2, 0.3, 0.5, 1.0]],\n\n        [[0.3, 0.0, 0.0, 0.0],\n         [0.2, 0.3, 0.0, 0.0],\n         [0.1, 0.5, 0.5, 0.0],\n         [0.4, 0.3, 0.5, 1.0]]], grad_fn=<SoftmaxBackward0>)\n------\nout: \ntensor([[[ 1.6,  0.4,  0.9, -0.4,  0.6,  0.4],\n         [-0.2,  0.2, -0.4, -0.5,  0.1, -0.9],\n         [ 0.1,  0.8, -0.0, -0.3,  0.4, -0.5],\n         [ 0.4,  0.3, -2.1, -0.7,  2.9,  0.8]],\n\n        [[-0.0,  0.0,  0.4, -0.2,  0.1, -0.2],\n         [ 0.4,  0.2,  0.3,  0.1,  0.0,  0.2],\n         [ 0.9,  1.3, -0.3,  0.2, -0.0,  1.1],\n         [ 0.8,  0.4,  0.3, -0.0,  2.3,  0.4]]], grad_fn=<UnsafeViewBackward0>)\n\n\nSee how weights:\n\nhas a different weights matrix for every batch element\n(after all: every batch element has different tokens on different positions)\nis no longer evenly distributed but some element positions have more weight than others\n\nTo get to the final stage of self-attention, we don’t multiply our input vector x directly with our weights, instead we compute a value by:\n\ncreating a value matrix\ncalculating the value by doing a matrix multiply of our input with the value matrix\nuse as our output, not just the weights @ x but instead now do weights @ v\n\nSo instead of the token embeddings directly, for the purposes of this attention head, it is actually v, the value tensor, that gets aggregated for every token and its past tokens.\n\nb2,t2,c2 = (2,4,6) \nx = torch.randn(b2,t2,c2)\n\n# a single head of self-attention:\nhead_size = 8\n\nkey = nn.Linear(c2, head_size, bias=False)   # this will do matrix multiply with fixed weights\nquery = nn.Linear(c2, head_size, bias=False) # this will do matrix multiply with fixed weights\nvalue = nn.Linear(c2, head_size, bias=False) # this will do matrix multiply with fixed weights\n\nk = key(x)   # (B x T x C) @ (C, head_size) => (B x T x head_size)\nq = query(x)\n\nweights = q @ k.transpose(-2,-1) * (head_size**-0.5)  # (B x T x head_size) @ (B x head_size x T) => B x T x T\n# multiplication by square root of head_size is needed to keep distribution equal\n\ntril = torch.tril(torch.ones(t2,t2))\nweights = weights.masked_fill(tril == 0, float('-inf'))  # this is what makes this an decoder block instead of an encoder block (where this is left off)\nweights = F.softmax(weights, dim=1)\n\nv = value(x) # (B x T x C) @ (C, head_size) => (B x T x head_size)\n\nout = weights @ v # (B x T x T) @ (B x T x head_size) => (B x T x head_size), here: 2 x 4 x 8\n\nprint(f'x: \\n{x}')\nprint(f'------')\nprint(f'tril: \\n{tril}')\nprint(f'------')\nprint(f'weights: \\n{weights}')\nprint(f'------')\nprint(f'v: \\n{v}')\nprint(f'------')\nprint(f'out: \\n{out}')\n\nx: \ntensor([[[-0.1, -1.4,  1.6, -2.1,  0.2,  0.0],\n         [ 0.0,  1.2,  0.3,  0.2,  1.4, -0.4],\n         [-0.1,  1.1, -0.1,  0.7, -1.2,  1.2],\n         [ 0.6,  1.3,  1.7, -0.5, -1.2, -2.1]],\n\n        [[-0.2, -0.5,  0.2,  0.6,  0.4, -0.6],\n         [ 0.4, -0.7,  0.7, -0.6, -0.2,  0.9],\n         [ 0.4,  0.9, -1.0, -0.3,  2.0,  1.1],\n         [-0.3, -1.7, -0.2,  1.7, -0.6,  1.0]]])\n------\ntril: \ntensor([[1., 0., 0., 0.],\n        [1., 1., 0., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 1.]])\n------\nweights: \ntensor([[[0.2, 0.0, 0.0, 0.0],\n         [0.4, 0.4, 0.0, 0.0],\n         [0.2, 0.3, 0.4, 0.0],\n         [0.2, 0.3, 0.6, 1.0]],\n\n        [[0.2, 0.0, 0.0, 0.0],\n         [0.2, 0.3, 0.0, 0.0],\n         [0.2, 0.5, 0.6, 0.0],\n         [0.3, 0.3, 0.4, 1.0]]], grad_fn=<SoftmaxBackward0>)\n------\nv: \ntensor([[[ 0.6,  0.3, -0.1,  1.2, -0.2, -0.5,  0.1,  0.3],\n         [ 0.5,  0.4, -0.4, -0.2, -0.3, -0.0,  0.4,  0.6],\n         [-0.3, -0.5,  0.6, -0.9,  0.3,  0.4, -0.2, -0.7],\n         [ 1.7,  0.7,  0.2, -0.1,  1.5,  0.6, -0.1,  0.5]],\n\n        [[-0.0,  0.0, -0.6,  0.1,  0.3, -0.3, -0.0,  0.5],\n         [ 0.1, -0.1, -0.0,  0.3, -0.2, -0.3,  0.1, -0.0],\n         [-0.1,  0.2, -0.0, -0.1, -1.7, -0.0,  0.7, -0.0],\n         [-1.2, -0.8, -0.6, -0.2,  0.3, -0.7, -0.4, -0.0]]],\n       grad_fn=<UnsafeViewBackward0>)\n------\nout: \ntensor([[[     0.1,      0.1,     -0.0,      0.3,     -0.0,     -0.1,      0.0,\n               0.1],\n         [     0.4,      0.2,     -0.2,      0.3,     -0.2,     -0.2,      0.2,\n               0.4],\n         [     0.2,     -0.0,      0.1,     -0.1,     -0.0,     -0.0,      0.1,\n               0.0],\n         [     1.7,      0.6,      0.5,     -0.5,      1.6,      0.7,     -0.1,\n               0.4]],\n\n        [[    -0.0,      0.0,     -0.1,      0.0,      0.1,     -0.1,     -0.0,\n               0.1],\n         [     0.0,     -0.0,     -0.1,      0.1,     -0.0,     -0.2,      0.0,\n               0.1],\n         [    -0.0,      0.1,     -0.2,      0.1,     -1.0,     -0.2,      0.5,\n               0.1],\n         [    -1.2,     -0.7,     -0.8,     -0.1,     -0.4,     -0.9,     -0.1,\n               0.1]]], grad_fn=<UnsafeViewBackward0>)\n\n\nKeys, queries and values in our case come from the same x vector; which is why we call this self-attention . This does not have to be the case always; there’s something called cross-attention as well."
  },
  {
    "objectID": "nanogpt-bigram-model.html#scaled-dot-product-attention-module",
    "href": "nanogpt-bigram-model.html#scaled-dot-product-attention-module",
    "title": "NanoGPT - Bigram Model",
    "section": "Scaled Dot Product Attention Module",
    "text": "Scaled Dot Product Attention Module\nWe will not create a self-attention module as described in Let’s build GPT: from scratch, in code, spelled out.\n\n# hyper parameters:\nbatch_size = 64\nblock_size = 8\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32             # number of embedding dimensions\n\n\ndef get_batch(split, batch_size=batch_size):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(train_data) - block_size, (batch_size,)) # this gets us a tensor like [476250,  18899, 645194, 831150]\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)\n\ndef generate_some_text(model=m, max_new_tokens=100):\n    generated_tokens = model.generate(idx = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=max_new_tokens)[0].tolist()\n    print(decode(generated_tokens))\n\n\nclass Head(nn.Module):\n    \"\"\"One head of self-attention\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)    # n_embd x head_size\n        self.query = nn.Linear(n_embd, head_size, bias=False) \n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        \n    def forward(self, x):\n        B,T,C = x.shape\n        \n        k = self.key(x)   # (B x T x C) @ () => B x T x C\n        q = self.query(x) # B x T x C\n        \n        # compute attention scores:\n        weights = q @ k.transpose(-2,-1) * (C**-0.5) # BxTxC @ BxCxT       => BxTxT\n        weights = weights.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # BxTxT\n        weights = F.softmax(weights, dim=-1) # BxTxT\n        \n        # do weighted aggregation:\n        v = self.value(x) # BxTxC\n        out = weights @ v # BxTxT @ BxTxC => BxTxC\n        return out\n\n\nclass BigramLanguageModel4(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     # give it index in vocab and it looks up the embedding vector in n_embd dims\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # each position will also get its own embedding vector\n        \n        # add self-attention head:\n        self.self_att_head = Head(n_embd)\n        \n        # add a language modeling head that will translate from token embeddings to logits:\n        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n        \n    def forward(self, idx ,targets=None):\n        B,T = idx.shape\n        \n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the token embeddings from idx, using our embedding table\n        #                                                                             #                             idx dimension: B x T\n        tok_embd = self.token_embedding_table(idx)                                    # token embeddings with result dimension:    B x T x C\n        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # position embeddings with result dimension:     T x C\n        x = tok_embd + pos_embd   # B x T x C\n        x = self.self_att_head(x) # appy a single head of self-attention: B x T x C\n        logits = self.lm_head(x)            # (B x T x C) @ (C, vocab_size) = (B x T x vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens:\n            idx_cond = idx[:,-block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\n\nmodel = BigramLanguageModel4()\nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nsteps=[]\nlosses=[]\n\nfor step in range(max_iters):\n    # get a batch\n    xb, yb = get_batch('train', batch_size=batch_size)\n    \n    # calculate the loss\n    logits,loss = model(xb,yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n    \n    if step%eval_interval == 0 :\n        steps.append(step)\n        losses.append(loss.item())\n        # print(f'loss for iteration {step} is: {loss}')\n\nplt.plot(steps, losses)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.ylim(2,5)\nplt.title('loss over time')\nplt.show()\n\n\n\n\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\ngenerate_some_text(model=model)\n\nlast recorded loss was 2.3017351627349854\n\nO: asafnte th'd tes thut mod thand he, preckn,\nOhe thme, achile lapilisers we ss, tean,\nTheelu.\nGLAB"
  },
  {
    "objectID": "nanogpt-bigram-model.html#multi-head-attention-module",
    "href": "nanogpt-bigram-model.html#multi-head-attention-module",
    "title": "NanoGPT - Bigram Model",
    "section": "Multi-Head Attention Module",
    "text": "Multi-Head Attention Module\nWe will now apply multiple heads of attention, in parallel, and then concatenate the results.\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of attention in parallel\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        \n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim = -1)\n\n\nclass BigramLanguageModel5(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     # give it index in vocab and it looks up the embedding vector in n_embd dims\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # each position will also get its own embedding vector\n        \n        # add multiple self-attention heads:\n        # each head will typically be smaller correspondingly (that's why we divide n_embd by number of channels)\n        # if we then concat the output for each, we'll get back to our original n_embd = 32\n        self.self_att_heads = MultiHeadAttention(4, n_embd//4) # 4 heads of 32/4=8-dimensional self-attention\n        \n        # add a language modeling head that will translate from token embeddings to logits:\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx ,targets=None):\n        B,T = idx.shape\n        \n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the token embeddings from idx, using our embedding table\n        #                                                                             #                             idx dimension: B x T\n        tok_embd = self.token_embedding_table(idx)                                    # token embeddings with result dimension:    B x T x C\n        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # position embeddings with result dimension:     T x C\n        x = tok_embd + pos_embd    # B x T x C\n        x = self.self_att_heads(x) # appy self-attentions in parallel: B x T x C\n        logits = self.lm_head(x)   # (B x T x C) @ (C, vocab_size) = (B x T x vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens:\n            idx_cond = idx[:,-block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\n\nmodel = BigramLanguageModel5()\nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nsteps=[]\nlosses=[]\n\nfor step in range(max_iters):\n    # get a batch\n    xb, yb = get_batch('train', batch_size=batch_size)\n    \n    # calculate the loss\n    logits,loss = model(xb,yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n    \n    if step%eval_interval == 0 :\n        steps.append(step)\n        losses.append(loss.item())\n        # print(f'loss for iteration {step} is: {loss}')\n\nplt.plot(steps, losses)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.ylim(2,5)\nplt.title('loss over time')\nplt.show()\n\n\n\n\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\n\nlast recorded loss was 2.065643548965454\n\n\n\ngenerate_some_text(model=model, max_new_tokens=100)\n\n\nERTZAK:\nMiced that mang youghthy\nbrothaves.\nSer url hat gre me machand He.\n\nNo dieen with upore cans"
  },
  {
    "objectID": "nanogpt-bigram-model.html#optimizing-multi-head-attention",
    "href": "nanogpt-bigram-model.html#optimizing-multi-head-attention",
    "title": "NanoGPT - Bigram Model",
    "section": "Optimizing Multi-Head Attention",
    "text": "Optimizing Multi-Head Attention\n\nIntroducing a Feed-Forward layer\nWe have a better result, but still a long way to go. So far, we went too fast in calculating the logits. There’s not much compute in between. To fix this, we’ll introduce a small feedforward layer (basically a normal Linear Layer with a non-linearity, being a ReLu.)\nThis feed-forward layer happens on a per-token basis. This means that all tokens do this independently. (Also see video here)\n\nclass FeedForward(nn.Module):\n    \"\"\"Single Layer followed by a non-linearity\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\n\nclass BigramLanguageModel6(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     # give it index in vocab and it looks up the embedding vector in n_embd dims\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # each position will also get its own embedding vector\n        self.self_att_heads = MultiHeadAttention(4, n_embd//4) # 4 heads of 32/4=8-dimensional self-attention\n        self.feed_fwd = FeedForward(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx,targets=None):\n        B,T = idx.shape\n        \n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the token embeddings from idx, using our embedding table\n        #                                                                             #                             idx dimension: B x T\n        tok_embd = self.token_embedding_table(idx)                                    # token embeddings with result dimension:    B x T x C\n        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # position embeddings with result dimension:     T x C\n        x = tok_embd + pos_embd    # B x T x C\n        x = self.self_att_heads(x) # appy self-attentions in parallel: B x T x C\n        x = self.feed_fwd(x)\n        logits = self.lm_head(x)   # (B x T x C) @ (C, vocab_size) = (B x T x vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens:\n            idx_cond = idx[:,-block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\n\nmodel = BigramLanguageModel6()\nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nsteps=[]\nlosses=[]\n\nfor step in range(max_iters):\n    # get a batch\n    xb, yb = get_batch('train', batch_size=batch_size)\n    \n    # calculate the loss\n    logits,loss = model(xb,yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n    \n    if step%eval_interval == 0 :\n        steps.append(step)\n        losses.append(loss.item())\n        # print(f'loss for iteration {step} is: {loss}')\n\nplt.plot(steps, losses)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.ylim(2,5)\nplt.title('loss over time')\nplt.show()\n\n\n\n\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\n\nlast recorded loss was 2.0591189861297607\n\n\nOur loss keeps going down from:\n\n2.553\n2.298\n2.150\n2.081\n\n\ngenerate_some_text(model=model, max_new_tokens=100)\n\n\nGRUENTANUM:\nTo shou and wordomest;\nAlcome ming I he wours old!-\nI in Wh hut is ther'd ban no wersay\n\n\n\n\n\nCreating multiple Blocks\nAs explained in Let’s build GPT: from scratch, in code, spelled out. we will now have multiple blocks with each\n\n# hyper parameters:\nbatch_size = 64\nblock_size = 8\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32             # number of embedding dimensions\n\n\nclass Head(nn.Module):\n    \"\"\"One head of self-attention\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)    # n_embd x head_size\n        self.query = nn.Linear(n_embd, head_size, bias=False) \n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        \n    def forward(self, x):\n        B,T,C = x.shape\n        \n        k = self.key(x)   # (B x T x C) @ () => B x T x C\n        q = self.query(x) # B x T x C\n        \n        # compute attention scores:\n        weights = q @ k.transpose(-2,-1) * (C**-0.5) # BxTxC @ BxCxT       => BxTxT\n        weights = weights.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # BxTxT\n        weights = F.softmax(weights, dim=-1) # BxTxT\n        \n        # do weighted aggregation:\n        v = self.value(x) # BxTxC\n        out = weights @ v # BxTxT @ BxTxC => BxTxC\n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of attention in parallel\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        \n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim = -1)\n\n\nclass FeedForward(nn.Module):\n    \"\"\"Single Layer followed by a non-linearity\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\n\nclass Block(nn.Module):\n    \"\"\"Transformer Block: communications followed by computation\"\"\"\n    \n    def __init__(self,n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head,head_size)\n        self.ffwd = FeedForward(n_embd)\n        \n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n\n\nclass BigramLanguageModel7(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     # give it index in vocab and it looks up the embedding vector in n_embd dims\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # each position will also get its own embedding vector\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4)\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        B,T = idx.shape\n        \n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the token embeddings from idx, using our embedding table\n        #                                                                             #                             idx dimension: B x T\n        tok_embd = self.token_embedding_table(idx)                                    # token embeddings with result dimension:    B x T x C\n        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # position embeddings with result dimension:     T x C\n        x = tok_embd + pos_embd    # B x T x C\n        x = self.blocks(x)\n        logits = self.lm_head(x)   # (B x T x C) @ (C, vocab_size) = (B x T x vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens:\n            idx_cond = idx[:,-block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\n\nmodel = BigramLanguageModel7()\nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nsteps=[]\nlosses=[]\n\nfor step in range(max_iters):\n    # get a batch\n    xb, yb = get_batch('train', batch_size=batch_size)\n    \n    # calculate the loss\n    logits,loss = model(xb,yb)\n    optim.zero_grad(set_to_none=True)\n    loss.backward()\n    optim.step()\n    \n    if step%eval_interval == 0 :\n        steps.append(step)\n        losses.append(loss.item())\n        # print(f'loss for iteration {step} is: {loss}')\n\nplt.plot(steps, losses)\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.ylim(2,5)\nplt.title('loss over time')\nplt.show()\n\n\n\n\nWhen you try to run this, the results won’t be stellar. The reason is that this is quite a deep neural net, which is not yet optimized.\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\n\nlast recorded loss was 2.0725607872009277\n\n\n\ngenerate_some_text(model=model, max_new_tokens=100)\n\n\nGo be'ts.\n\nEULYIZDA:\nO, Rpefoul the gosonn, hear now of him your dis the barete sheir bouls thou Iw \n\n\nOur loss keeps going down from:\n\n2.553\n2.298 (scaled dot product attention)\n2.150 (multi-head attention)\n2.081 (feed fwd layer)\n2.16 => blocks without optimization - this is worse; but we’re dealing with a deep network here\n\n\n\nOptimizations\nThere’s two optimizations that will enable us to really use a deep network like this. Discussed in the video here: Let’s build GPT: from scratch, in code, spelled out.\n\nResidual Connections (also called skip-connections) - see: Deep Residual Learning for Image Recognition\nLayer Normalization\n\n\nResidual Connections\nA residual connection, also known as a skip connection, is a way to add the input of a layer to its output, bypassing the layer’s activation function. This is done by adding the input tensor to the output tensor, and passing the sum through the next layer’s activation function. The output of the layer with the skip connection is therefore the sum of the original output and the original input.\nThe purpose of the residual connection is to help alleviate the vanishing gradient problem, which can occur when training deep neural networks. In traditional neural networks, gradients can become very small as they propagate backwards through many layers, making it difficult to update the weights of the earlier layers. By adding a residual connection, the gradient can flow directly from the output to the input of the layer, bypassing the activation function and allowing the gradients to be directly added together.\nThis can be thought of as a “shortcut” for the gradient, which can help to propagate it more efficiently through the network. By enabling the gradients to flow directly through the skip connection, the residual connection helps to ensure that the weights of the earlier layers are updated more effectively during training.\nIn summary, a residual connection or skip connection is a way to add the input of a layer to its output, bypassing the layer’s activation function. This helps to alleviate the vanishing gradient problem and enable more efficient gradient flow through the network during training. The addition of the input and output tensors during the residual connection is important for allowing gradients to be computed with addition, enabling more efficient weight updates during training.\n\n# hyper parameters:\nbatch_size = 64\nblock_size = 8\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32             # number of embedding dimensions\n\nclass Head(nn.Module):\n    \"\"\"One head of self-attention\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)    # n_embd x head_size\n        self.query = nn.Linear(n_embd, head_size, bias=False) \n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        \n    def forward(self, x):\n        B,T,C = x.shape\n        \n        k = self.key(x)   # (B x T x C) @ () => B x T x C\n        q = self.query(x) # B x T x C\n        \n        # compute attention scores:\n        weights = q @ k.transpose(-2,-1) * (C**-0.5) # BxTxC @ BxCxT       => BxTxT\n        weights = weights.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # BxTxT\n        weights = F.softmax(weights, dim=-1) # BxTxT\n        \n        # do weighted aggregation:\n        v = self.value(x) # BxTxC\n        out = weights @ v # BxTxT @ BxTxC => BxTxC\n        return out\n\nWe’ll add a projection back into the residual pathway to our MultiHeadAttentionand FeedForward - which is just a linear transformation of the output of this layer:\n\nclass FeedForward(nn.Module):\n    \"\"\"Single Layer followed by a non-linearity\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n            nn.Linear(n_embd, n_embd)  # projection back into residual pathway\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of attention in parallel\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd) # projection\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim = -1)\n        out = self.proj(out)  # projection\n        return out\n\nWe’ll change the block to use a residual connection:\n\nclass Block(nn.Module):\n    \"\"\"Transformer Block: communications followed by computation\"\"\"\n    \n    def __init__(self,n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head,head_size)\n        self.ffwd = FeedForward(n_embd)\n        \n    def forward(self, x):\n        x = x + self.sa(x)    # the \"x + ... is the skip connection here\"\n        x = x + self.ffwd(x)  # the \"x + ... is the skip connection here\"\n        return x\n\n\nclass BigramLanguageModel8(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     # give it index in vocab and it looks up the embedding vector in n_embd dims\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # each position will also get its own embedding vector\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4)\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        B,T = idx.shape\n        \n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the token embeddings from idx, using our embedding table\n        #                                                                             #                             idx dimension: B x T\n        tok_embd = self.token_embedding_table(idx)                                    # token embeddings with result dimension:    B x T x C\n        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # position embeddings with result dimension:     T x C\n        x = tok_embd + pos_embd    # B x T x C\n        x = self.blocks(x)\n        logits = self.lm_head(x)   # (B x T x C) @ (C, vocab_size) = (B x T x vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens:\n            idx_cond = idx[:,-block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\n\ndef train_model_and_show_result(model):\n    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    steps=[]\n    losses=[]\n\n    for step in range(max_iters):\n        # get a batch\n        xb, yb = get_batch('train', batch_size=batch_size)\n\n        # calculate the loss\n        logits,loss = model(xb,yb)\n        optim.zero_grad(set_to_none=True)\n        loss.backward()\n        optim.step()\n\n        if step%eval_interval == 0 :\n            steps.append(step)\n            losses.append(loss.item())\n            # print(f'loss for iteration {step} is: {loss}')\n\n    plt.plot(steps, losses)\n    plt.xlabel('iteration')\n    plt.ylabel('loss')\n    plt.ylim(1,5)\n    plt.title('loss over time')\n    plt.show()\n    \n    return losses\n\n\nmodel = BigramLanguageModel8()\nmodel.to(device)\nlosses = train_model_and_show_result(model)\n\n\n\n\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\n\nlast recorded loss was 1.9664623737335205\n\n\n\ngenerate_some_text(model=model, max_new_tokens=100)\n\n\nDUKEn\nMears as now:\nKeve manasy Va cound for your was trake you, many as Bolew'd be sitculfintientle\n\n\nOur loss keeps going down from:\n\n2.553\n2.298 (scaled dot product attention)\n2.150 (multi-head attention)\n2.081 (feed fwd layer)\n2.16 => blocks without optimization - this is worse; but we’re dealing with a deep network here\n1.96 (blocks optimized with residual pathways)\n\nIn addition, and as described in Let’s build GPT: from scratch, in code, spelled out. we’ll now make sure our inner layer of our feedforward module has dimension 4 times bigger than the input our output.\n\nclass FeedForward(nn.Module):\n    \"\"\"Single Layer followed by a non-linearity\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd)  # projection back into residual pathway\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\n\nmodel = BigramLanguageModel8()\nmodel.to(device)\nlosses = train_model_and_show_result(model)\n\n\n\n\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\n\nlast recorded loss was 1.8393092155456543\n\n\n\ngenerate_some_text(model=model, max_new_tokens=100)\n\n\nSom.\n\nKING RICHENRY VI\nholumf:\nDo with sybonay them what\nYours of stnous acarge, fards, Sir;\nMaster.\n\n\nOur loss keeps going down from:\n\n2.553\n2.298 (scaled dot product attention)\n2.150 (multi-head attention)\n2.081 (feed fwd layer)\n2.16 => blocks without optimization - this is worse; but we’re dealing with a deep network here\n1.96 (blocks optimized with residual pathways)\n1.855 (inner layer of feedforward is 4 x size of input)\n\n\n\nLayer Norm\nSee Let’s build GPT: from scratch, in code, spelled out.  This is the paper on Layer Normalization\nLayerNorm is similar to batchnorm. Batch normalization made sure that across the batch dimension, every neuron had zero mean and a std deviation of 1. Batchnorm worked by substracting the mean and dividing by the std dev:\n\na = torch.tensor(np.array([2.0, 4, 1 ,5]))\na_mean = torch.mean(a)\na_stdev = torch.std(a)\n\nprint(f'the mean is: {a_mean}')\nprint(f'the std dev is: {a_stdev}')\n\nthe mean is: 3.0\nthe std dev is: 1.8257418583505538\n\n\n\nb_min_mean = (a-a_mean)\nb_mean = torch.mean(b_min_mean)\nb_stdev = torch.std(b_min_mean)\nb_min_mean_div_std = b_min_mean / a_stdev\nb_min_mean_dev_std_std = torch.std(b_min_mean_div_std)\nprint(f'the mean after substracting the mean of every number in the tensor is: {b_mean}')\nprint(f'the std dev after substracting the mean of every number in the tensor is: {b_stdev} - this is unchanged \\n')\nprint(f'if we now divide all numbers by the std dev, the std dev becomes: {b_min_mean_dev_std_std} - which is 1')\n\nthe mean after substracting the mean of every number in the tensor is: 0.0\nthe std dev after substracting the mean of every number in the tensor is: 1.8257418583505538 - this is unchanged \n\nif we now divide all numbers by the std dev, the std dev becomes: 0.9999999999999998 - which is 1\n\n\nThis is how BatchNorm looked like:\n\nclass BatchNorm1d:\n  \n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.momentum = momentum\n    self.training = True\n    \n    # parameters (trained with backprop)\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n    \n    # buffers (trained with a running 'momentum update')\n    self.running_mean = torch.zeros(dim)\n    self.running_var = torch.ones(dim)\n  \n  def __call__(self, x):\n    # calculate the forward pass\n    if self.training:\n      xmean = x.mean(0, keepdim=True) # batch mean\n      xvar = x.var(0, keepdim=True) # batch variance\n    else:\n      xmean = self.running_mean\n      xvar = self.running_var\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    # update the buffers\n    if self.training:\n      with torch.no_grad():\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n    return self.out\n  \n  def parameters(self):\n    return [self.gamma, self.beta]\n\nSo what is layer norm? It is highly similar to BatchNorm but instead of calculating the mean and stdev across the batch examples (for each column), we will calculate the mean and stdev for each sample in the batch individually (for each row). This also means, because there’s no connection anymore between the batch samples, there’s no more need to keep a running mean.\n\nclass LayerNorm:\n  \n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n  \n  def __call__(self, x):\n    xmean = x.mean(1, keepdim=True) # batch mean\n    xvar = x.var(1, keepdim=True) # batch variance\n\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n   \n    return self.out\n  \n  def parameters(self):\n    return [self.gamma, self.beta]\n\nWe’ll incorporarte this in our block, but slightly deviate from the paper in the sense that LayerNorm will happen before. The LayerNorm acts as a per-token transformation and both of our batch and time dimension acts as batch dimension. It normalizes the feature.\n\nclass Block(nn.Module):\n    \"\"\"Transformer Block: communications followed by computation\"\"\"\n    \n    def __init__(self,n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head,head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)  # add layer norm\n        self.ln2 = nn.LayerNorm(n_embd)  # add another layer norm\n        \n    def forward(self, x):\n        x = x + self.sa( self.ln1(x))    # the \"x + ... is the skip connection here\", apply layer norm\n        x = x + self.ffwd( self.ln2(x))   # the \"x + ... is the skip connection here\", apply layer norm\n        return x\n\n\nmodel = BigramLanguageModel8()\nmodel.to(device)\nlosses = train_model_and_show_result(model)\n\n\n\n\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\n\nlast recorded loss was 1.9586102962493896\n\n\n\ngenerate_some_text(model=model, max_new_tokens=300)\n\n\nThis shame: why ham bast; I sixt ward be took; I'll sincely?\nThe warr lead me, will deepome fores a bucked shame hid. I'll him\nThat I:\nWell riew,\nWhosily stees we and bed\nWith I this of I'll speap it:\nI myscan my larmandring, ere think. I'll time the counden able callow.\nCupper, we dow beatty name b\n\n\nOur loss keeps going down from:\n\n2.553\n2.298 (scaled dot product attention)\n2.150 (multi-head attention)\n2.081 (feed fwd layer)\n2.16 => blocks without optimization - this is worse; but we’re dealing with a deep network here\n1.96 (blocks optimized with residual pathways)\n1.855 (inner layer of feedforward is 4 x size of input)\n1.845 (layer norm)\n\nWe’ll now also add a layernorm at the end of the transformer, after the last block and right before the input in the last linear layer that decodes into our vocabulary.\n\nclass BigramLanguageModel9(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     # give it index in vocab and it looks up the embedding vector in n_embd dims\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # each position will also get its own embedding vector\n        self.blocks = nn.Sequential(\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            Block(n_embd, n_head=4),\n            nn.LayerNorm(n_embd)\n        )\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        B,T = idx.shape\n        \n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the token embeddings from idx, using our embedding table\n        #                                                                             #                             idx dimension: B x T\n        tok_embd = self.token_embedding_table(idx)                                    # token embeddings with result dimension:    B x T x C\n        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # position embeddings with result dimension:     T x C\n        x = tok_embd + pos_embd    # B x T x C\n        x = self.blocks(x)\n        logits = self.lm_head(x)   # (B x T x C) @ (C, vocab_size) = (B x T x vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens:\n            idx_cond = idx[:,-block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\n\nmodel = BigramLanguageModel9()\nmodel.to(device)\nlosses = train_model_and_show_result(model)\n\n\n\n\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\n\nlast recorded loss was 1.7616101503372192\n\n\nThis is actually a little worse than we had before…\n\ngenerate_some_text(model=model, max_new_tokens=100)\n\n\nIf eaching mier, pliffit\nAnd called and her;\nAnd I groof mistly of I can was queal:\ntry of it come o"
  },
  {
    "objectID": "nanogpt-bigram-model.html#scaling-up-our-model",
    "href": "nanogpt-bigram-model.html#scaling-up-our-model",
    "title": "NanoGPT - Bigram Model",
    "section": "Scaling up our model",
    "text": "Scaling up our model\nLet’s see how far we can push this by scaling our model. To do this, we’ll make some changes to make configuration of the model easier. In addition, we’ll add dropout to our head, feedforward layer and multi-head attention.\nDropout is a regularization technique proposed in a paper from 2014:Dropout: A Simple Way to Prevent Neural Networks from Overfitting\nSee: Let’s build GPT: from scratch, in code, spelled out.\n\n# hyper parameters:\nbatch_size = 64\nblock_size = 256     # this was previously only 8\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 3e-4 #\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384             # number of embedding dimensions\nn_head = 6\nn_layer = 6\ndropout = 0.2        # 20 percent is dropped to zero\n\nclass Head(nn.Module):\n    \"\"\"One head of self-attention\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)    # n_embd x head_size\n        self.query = nn.Linear(n_embd, head_size, bias=False) \n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        B,T,C = x.shape\n        \n        k = self.key(x)   # (B x T x C) @ () => B x T x C\n        q = self.query(x) # B x T x C\n        \n        # compute attention scores:\n        weights = q @ k.transpose(-2,-1) * (C**-0.5) # BxTxC @ BxCxT       => BxTxT\n        weights = weights.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # BxTxT\n        weights = F.softmax(weights, dim=-1) # BxTxT\n        weights = self.dropout(weights)\n        \n        # do weighted aggregation:\n        v = self.value(x) # BxTxC\n        out = weights @ v # BxTxT @ BxTxC => BxTxC\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of attention in parallel\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd) # projection\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim = -1)\n        out = self.dropout(self.proj(out))  # projection\n        return out\n\nclass FeedForward(nn.Module):\n    \"\"\"Single Layer followed by a non-linearity\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),  # projection back into residual pathway\n            nn.Dropout(dropout)\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):  # Note: this is unchanged from before\n    \"\"\"Transformer Block: communications followed by computation\"\"\"\n    \n    def __init__(self,n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head,head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)  # add layer norm\n        self.ln2 = nn.LayerNorm(n_embd)  # add another layer norm\n        \n    def forward(self, x):\n        x = x + self.sa( self.ln1(x))    # the \"x + ... is the skip connection here\", apply layer norm\n        x = x + self.ffwd( self.ln2(x))   # the \"x + ... is the skip connection here\", apply layer norm\n        return x\n    \nclass BigramLanguageModel10(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     # give it index in vocab and it looks up the embedding vector in n_embd dims\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # each position will also get its own embedding vector\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for n in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        B,T = idx.shape\n        \n        # idx here is our input indexes, like the vector xb we saw earlier\n        # we will get the token embeddings from idx, using our embedding table\n        #                                                                             #                             idx dimension: B x T\n        tok_embd = self.token_embedding_table(idx)                                    # token embeddings with result dimension:    B x T x C\n        pos_embd = self.position_embedding_table(torch.arange(T, device=device))  # position embeddings with result dimension:     T x C\n        x = tok_embd + pos_embd    # B x T x C\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)   # (B x T x C) @ (C, vocab_size) = (B x T x vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        \n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens:\n            idx_cond = idx[:,-block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]         # pluck out last time step: B x T x C becomes B x C\n            probs = F.softmax(logits, dim=-1) # B x C\n            idx_next = torch.multinomial(probs, num_samples=1)  # B x 1\n            idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n        return idx\n\n\nmodel = BigramLanguageModel10()\nmodel.to(device)\nlosses = train_model_and_show_result(model)\n\n\n\n\nCPU times: user 10min 6s, sys: 26.6 s, total: 10min 32s\nWall time: 10min 29s\n\n\n\nprint(f'last recorded loss was {losses[len(losses)-1]}')\n\nlast recorded loss was 0.9283599257469177\n\n\n\ngenerate_some_text(model=model, max_new_tokens=500)\n\n\nMark thee, when my poor worthy hob, by banish'd,\nThee light his father, with the garish of looks,\nAnd never crack'd on other ground, which of all.\nThat time we replied to the time allowing\nWhat cannot pit in?\n\nSICINIUS:\nI would tell thee, thou carry what will do,\nBut ever here in her puts in this such a case.\n\nBoth CARLIUS:\nWho hath calf but hath believed their house.\n\nCitizen:\nI will not be but spark to: cries and give wars this\nfacting enemy, and I have.\n\nCORIOLANUS:\nO this last knowledge for \n\n\n\ndevice\n\n'cuda'"
  },
  {
    "objectID": "nanogpt-pytorch-embeddings.html",
    "href": "nanogpt-pytorch-embeddings.html",
    "title": "NanoGPT - Embeddings",
    "section": "",
    "text": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntorch.set_printoptions(precision=2)\ngenerator = torch.manual_seed(42)\n\nvocab_size = 8 # 8 characters or language tokens possible\ntoken_embedding_table = nn.Embedding(vocab_size, vocab_size)\nLet’s look at the contents of our embedding table now:\nWe can index into this embedding table:\nSo, what we have here is that for each of our independent batch items (our first dimension batch_size, 4 in total) we get back a 6 by 8 matrix. Let’s have a look at our first batch item: [6, 1, 3, 0, 3, 5]. Here 6, 1, 3 etc are the integer indexes representing each a token (character or subword in LLM-world).\nThose numbers index into the embedding table:\nSo what we get back is for each token-index of our batch item, a list (size vocab_size, here 8) with probabilities for each next token. That’s why we get an additional dimension returned: every token we input into our embedding, returns a list of probabilities for the next token. So while our:"
  },
  {
    "objectID": "nanogpt-pytorch-embeddings.html#reshaping-tensors-using-view",
    "href": "nanogpt-pytorch-embeddings.html#reshaping-tensors-using-view",
    "title": "NanoGPT - Embeddings",
    "section": "Reshaping tensors using view()",
    "text": "Reshaping tensors using view()\n\nt = torch.randint(0, 10, (2,4,6))\nt\n\ntensor([[[5, 3, 7, 7, 5, 9],\n         [1, 5, 1, 9, 1, 4],\n         [0, 3, 7, 5, 7, 1],\n         [5, 7, 5, 8, 5, 4]],\n\n        [[1, 1, 0, 9, 0, 9],\n         [1, 8, 9, 6, 7, 6],\n         [0, 9, 5, 2, 9, 1],\n         [7, 8, 6, 0, 6, 8]]])\n\n\n\nt.view(2,24)  # combine the second and third dimension into one (4x6=24)\n\ntensor([[5, 3, 7, 7, 5, 9, 1, 5, 1, 9, 1, 4, 0, 3, 7, 5, 7, 1, 5, 7, 5, 8, 5, 4],\n        [1, 1, 0, 9, 0, 9, 1, 8, 9, 6, 7, 6, 0, 9, 5, 2, 9, 1, 7, 8, 6, 0, 6, 8]])\n\n\n\n# we can do the same by having pytorch figure out the size of the remaining dimension, using `-1`\nt.view(2, -1)\n\ntensor([[5, 3, 7, 7, 5, 9, 1, 5, 1, 9, 1, 4, 0, 3, 7, 5, 7, 1, 5, 7, 5, 8, 5, 4],\n        [1, 1, 0, 9, 0, 9, 1, 8, 9, 6, 7, 6, 0, 9, 5, 2, 9, 1, 7, 8, 6, 0, 6, 8]])\n\n\n\nt.view(2,2,-1)\n\ntensor([[[5, 3, 7, 7, 5, 9, 1, 5, 1, 9, 1, 4],\n         [0, 3, 7, 5, 7, 1, 5, 7, 5, 8, 5, 4]],\n\n        [[1, 1, 0, 9, 0, 9, 1, 8, 9, 6, 7, 6],\n         [0, 9, 5, 2, 9, 1, 7, 8, 6, 0, 6, 8]]])"
  },
  {
    "objectID": "nanogpt-pytorch-embeddings.html#preparing-logits-tensor-bxtxc-for-cross-entropy-loss",
    "href": "nanogpt-pytorch-embeddings.html#preparing-logits-tensor-bxtxc-for-cross-entropy-loss",
    "title": "NanoGPT - Embeddings",
    "section": "Preparing logits tensor (BxTxC) for Cross Entropy Loss",
    "text": "Preparing logits tensor (BxTxC) for Cross Entropy Loss\n\nB, T, C = logits.shape\nB, T, C\n\n(4, 6, 8)\n\n\nWhat the cross entropy loss function expects is for a multidimensional input, for the channels (C) to be the second dimension.\n\nlogits\n\ntensor([[[-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n         [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n         [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49]],\n\n        [[-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n         [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n         [-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76]],\n\n        [[ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n         [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n         [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86]],\n\n        [[-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n         [-1.40,  0.04, -0.06,  0.68, -0.10,  1.84, -1.18,  1.38],\n         [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n         [ 1.64, -0.16, -0.50,  0.44, -0.76,  1.08,  0.80,  1.68],\n         [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49],\n         [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60]]],\n       grad_fn=<EmbeddingBackward0>)\n\n\n\nlogits = logits.view(B*T, C) # moves channels (probs for next token for each item in vocabulary) into second dim\nlogits\n\ntensor([[-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n        [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n        [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49],\n        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n        [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n        [-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n        [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n        [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n        [-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n        [-1.40,  0.04, -0.06,  0.68, -0.10,  1.84, -1.18,  1.38],\n        [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n        [ 1.64, -0.16, -0.50,  0.44, -0.76,  1.08,  0.80,  1.68],\n        [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49],\n        [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60]],\n       grad_fn=<ViewBackward0>)"
  },
  {
    "objectID": "nanogpt-pytorch-embeddings.html#calculate-loss-versus-targets",
    "href": "nanogpt-pytorch-embeddings.html#calculate-loss-versus-targets",
    "title": "NanoGPT - Embeddings",
    "section": "Calculate loss versus targets",
    "text": "Calculate loss versus targets\n\n# let's create some made-up targets to play with, shaped: B x T\nidy = torch.randint(low=0, high=vocab_size, size=(batch_size, context_length))\nidy\n\ntensor([[0, 0, 6, 0, 7, 0],\n        [3, 7, 7, 6, 2, 2],\n        [0, 7, 2, 2, 0, 2],\n        [4, 1, 6, 1, 0, 3]])\n\n\nFor the Cross Entropy Loss, pytorch expects a one-dimensional tensor for our targets:\n\ntargets = idy.view(batch_size * context_length)\ntargets\n\ntensor([0, 0, 6, 0, 7, 0, 3, 7, 7, 6, 2, 2, 0, 7, 2, 2, 0, 2, 4, 1, 6, 1, 0, 3])\n\n\n\nloss = F.cross_entropy(logits, targets)\nloss\n\ntensor(2.83, grad_fn=<NllLossBackward0>)"
  },
  {
    "objectID": "llm.html",
    "href": "llm.html",
    "title": "Christof's AI-related notes",
    "section": "",
    "text": "Andrej Karpathy’s YouTube channel on the making of MakeMore and GPT - see notebooks here\nPrimer on Transformers\nSelf Attention from Scratch\nStudy guide on Transformers\nStanford CS224N - NLP with Deep Learning\nStanford CS25 - Tranformers United\nPapers with Code - Language Models\n\n\n\n\n\n\n\nThis is regular prompting. Downside being hallucination and confubulation. The model can only rely on its training data and not react to current events.\n\n\n\nIn-Context Learning is the remarkable capability for a LLM to construct new predictors from sequences of labeled examples in its prompt without the need for fine-tuning.\n\nPaper - What learning algorithm is in-context learning? Investigations with linear models and a summary on 42 papers talks about how similar In-Context Learning is to a trained model.\n\n\n\n\nIntroduced in the Paper - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\nIn your prompt you include explicit reasoning and this gets the model to do the same. This increases accuracy. It is however not grounded to external environments to obtain and update knowledge, and has to rely on limited internal knowledge.\nThe simplest form of this is to add “let’s think step by step” to your prompt. (“Zero shot CoT”)\n\n\n\nIntroduced in the Paper - Precise Zero-Shot Dense Retrieval without Relevance Labels.\nThe idea here is that instead of encoding the user’s query to retrieve relevant documents, you can generate a “hypothetical” (hallucinated) answer and encode that. Documents with right answers are more similar to wrong answers than to questions.\n\n\n\nSee Paper - ReAct: Synergizing Reasoning and Acting in Language Models and the associated ReAct website\n\n\n\nPrompt Engineering Guide Effectiveness of ChatGPT versus state-of-the-art: ChatGPT: Jack of all trades, master of none - tested ChatGPT on 25 tasks and compared performance with the state-of-the-art.\n\n\n\n\n\n\n\nBlog post by Alistair Pullen explains that latency significantly goes up as you use larger models like Da Vinci. He explains an approacht to reduce price by a factor 40 while reducing latency 4 to 5 times at the trade-off of getting 90% similar results to Da Vinci. The approach generates a corpus of completions using Da Vinci. Then use that corpus to train Babbage.\n\n\n\n\n\n\nMost comprehensive study of GPT4 abilities in March 2023"
  },
  {
    "objectID": "cross-entropy-loss.html",
    "href": "cross-entropy-loss.html",
    "title": "Cross-Entropy Loss",
    "section": "",
    "text": "The Cross-Entropy Loss function is typically used as a loss function in multi-class classification problems.\nThe output for a neural network doing classification is a set of probabilities (a so-called probability distribution where every class is associated with a probability). We try to adapt weights to optimize the resulting probabilities to match as close as possible the ground truth. To iteratively adapt the weights and improve the prediction, a loss function is needed. For multi-class classification, Cross-Entropy Loss is used."
  },
  {
    "objectID": "cross-entropy-loss.html#shannon-information",
    "href": "cross-entropy-loss.html#shannon-information",
    "title": "Cross-Entropy Loss",
    "section": "Shannon Information",
    "text": "Shannon Information\nAn occurrence of an unlikely event gives more information than the occurrence of a very likely event. Shannon came up with a way to quantify how unpredictable a series of events is; measuring the “disorder” of system and quantifying uncertainty of a probability distribution.\nLet’s do a thought experiment with two people: person A and person B. They can agree upfront on the meaning of a series of bits thrown back and forth over a wall (like some mapping function saying “0101” means event “abc” happened). Beyond the bits used for communicating they cannot exchange other information. Let’s now assume a number of different scenarios.\n\nA fair coin flip\nAssume person A does a fair coin flip where the probability distribution of having heads is the same as having tails, each being 50 percent: \\(P(H)=0.5\\) and \\(P(T)=0.5\\)\nBoth A and B can agree to exchange the outcome of the coin flip using a single bit of information where 0 means heads and 1 means tails. When B receives the bit, he will know exactly what the outcome of the coin flip was. We can say the entropy of this probability distribution is 1 bit.\n\n\nWinning team out of 8\nAssume on the one side of the wall person A observes one team out of 8 winning a tournament. Each team has a probability of 1/8 or 0.125 of winning, so this is again an equal probability distribution: \\(P(A)=0.125\\), \\(P(B)=0.125\\), \\(P(C)=0.125\\), …, \\(P(H)=0.125\\)\nA and B can agree to communicate the winning team using 3 bits of information. 3 bits give them \\(2^3=8\\) classes, one for each team. Let’s say “000” means team A; “001” team B; “010” team C and so on. We can say the entropy for this probability distribution is 3 bits.\nIf we generalize this: for a uniform distribution of M equally possible outcomes, the entropy is: \\(log_2\\,M\\)\nThis also holds for distributions where the number of outcomes is not exactly a power of 2, like was the case in the examples before. Let’s have a look at this in the next example.\n\n\n10 outcomes\nIf A observes an outcome out of equal distributation of 10 possible outcomes (each with a probability of 0.1), then these can all be encoded using 4 bits. 4 bits allows for representing \\(2^4=16\\) states which is more than needed for our 10 possible outcomes. There are 6 “unused” states.\nWe can group outcomes in groups of 3. There are 1000 such unique triplets possible. If we encode our data per 3 observations, then every such triplet can be encoded using 10 bits, giving us a total of \\(2^10=1024\\) states. That’s still too much but we’re already much more efficient in encoding out information as we can represent on average 1 outcome = 1/3 triplet using \\(\\frac{10}{3}=3.333...\\) bits. This is better but not perfect yet.\nWe grouped our information by 3 outcomes at a time; which gave us \\(10^3\\) outcomes. Let’s call the number of items by which we group is G instead of 3. The number of states we can present with B bits is \\(2^B\\). The most efficient encoding is one where \\(2^B = 10^G\\) where G is the number of grouped observations and B is the number of bits.\n\\[2^B = 10^G\\]\nLet’s take the \\(log_2\\) of both sides:\n\\[B = log_2 (10^G)\\] \\[B = G\\, log_2 10\\] \\[\\frac{B}{G} = log_2 10\\]\n\\(\\frac{B}{G}\\) is our entropy and \\(log_2 10\\) is approximately 3.322…\nSo for a uniform distribution of M possible outcomes (\\(U(M)\\)) in which every probability of an outcome is \\(p_{1..M} = \\frac{1}{M}\\), the entropy is: \\[H(U(M))=log_2\\,M\\]\n\n\nNon-uniform distributions\nAs we’ve seen before, each outcome with probability \\(p\\) needs \\(log_2\\,M\\) bits to encode or \\(log_2\\,\\frac{1}{p}\\) which is \\(-log_2\\,p\\). Summing this over an entire distribution and multiplying each possible outcome with its probability gives us the entropy for a non-uniform distribution: \\(-\\sum_{i=1}^{M}\\,p_{i}\\,log_2\\,p_{i}\\)\nThis describes how much information, on average, is needed to describe the outcome for a distribution.\n\n\nShannon Entropy Formula\nShannon Entropy is defined as: \\[H=-\\sum_{i=1}^{M}P(x_{i}) \\, log_2 \\, P(x_{i})\\]"
  },
  {
    "objectID": "cross-entropy-loss.html#kl-divergence",
    "href": "cross-entropy-loss.html#kl-divergence",
    "title": "Cross-Entropy Loss",
    "section": "KL Divergence",
    "text": "KL Divergence\nThe Kullback Leibler divergence is a natural measure of distance between distributions, or how much one distribution differs from another one.\n\nComparing distributions\nEntropy is the theoretical lower bound on the number of bits we need to to encode information, given a probability distribution.\nAssume two different probability distributions \\(P\\) and \\(Q\\). The most optimal encoding for \\(P\\) is given by its entropy \\(H(P)\\). Any other encoding will need more bits to convey the same events or observations. How many more bits would we need to encode information using the most optimal encoding for distribution Q, given information coming from distribution P?\nIn P, a single event or observation \\(x_{i}\\) with probability \\(p(x_{i})\\) needs \\(-log \\, p(x_{i})\\) bits to be encoded. That same event has a different probability in distribution Q and needs \\(-log \\, q(x_{i})\\) bits to be encoded in the optimal encoding for Q. The difference between the encoding for the same event in both distributions is: \\(-log \\, q(x_{i})-(-log \\, p(x_{i}))\\) or: \\[log \\, p(x_{i})-log \\, q(x_{i})\\]\nThe above equation tells us the “excess bits” for a single event \\(x_{i}\\) when encoded in Q instead of in P. To look at the average difference in number of bits across the entire distribution \\(P\\), we can multiply each event by its probably of occuring in P. This is what we call the KL divergence:\n\\[D_{KL}(p||q)=\\sum_{i=1}^N p(x_{i})\\,(log \\, p(x_{i})-log \\, q(x_{i}))\\]\nor given that \\(log \\, a - log \\, b = log \\,\\frac{a}{b}\\):\n\\[D_{KL}(p||q)=\\sum_{i=1}^N p(x_{i})\\,(log \\, \\frac{p(x_{i})}{q(x_{i})})\\]\n\n\nKL Divergence Formula\nThe KL Divergence is defined as: \\[D_{KL}(P||Q)=\\sum_{i}P_{i}\\,log \\frac{P_{i}}{Q_{i}}\\]\n\n\nFurther intuition\nImagine we have 2 coins: one fair coin with an equal distribution of 0.5 for each outcome (heads or tails) and a biased coin with probability \\(p\\) for heads and \\(q\\) for tails. We’re trying to measure how different those are. If \\(p\\) is close to 0.5 then it would be easy to confuse both distributions by looking at the outcomes they generate. On the opposite side, if \\(p\\) is close to 0.95 for example, then it would be fairly obvious to dinstinguish between both distributions by observing a series of outcomes.\nTo quantitatively measure the difference between two distributions we can look if a given sequence would be equally probably in both. If they assign similar probabilities to similar sequences that implies both are very similar.\nWe can generate observations using coin 1 then calculating the probability of coin 2 generating the observations. Then we can compare that probability to the probability of coin 1. If those are similar, then the distribution is similar or vice versa. We could then take the following ratio: \\[\\frac{P_{\\text{observations from coin 1}}}{P_{\\text{observations from coin 2}}}\\]\nLet’s say our fair coin has probabilities: \\[\\begin{cases}p_{1} & \\text{for heads} \\\\ p_{2} & \\text{for tails} \\end{cases}\\]\nWe flip this coin \\(n\\) times: H T T H H\nThen we work out the probability of coin 1 generating this sequence. To do this we multiply by \\(p_{1}\\) when we see heads and by \\(p_{2}\\) when we see tails: \\(p_{1} \\cdot p_{2} \\cdot p_{2} \\cdot p_{1} \\cdot p_{1}\\)\nOur biased coin has probabilities: \\[\\begin{cases}q_{1} & \\text{for heads} \\\\ q_{2} & \\text{for tails} \\end{cases}\\]\nLet’s work out the probably of coin 2 generating this sequence. In a similar way this is: \\(q_{1} \\cdot q_{2} \\cdot q_{2} \\cdot q_{1} \\cdot q_{1}\\)\nIf \\(N_{H}\\) is the number of times we got heads and \\(N_{T}\\) is the number of times we got tails, we can simplify both as: \\[P(\\text{observations from coin 1})=p_{1}^{N_{H}}\\cdot p_{2}^{N_{T}}\\] and in the biased distribution: \\[P(\\text{observations from coin 2})=q_{1}^{N_{H}}\\cdot q_{2}^{N_{T}}\\]\nCalculating the ratio of both:\n\\[\\frac{P_{\\text{observations from coin 1}}}{P_{\\text{observations from coin 2}}}=\\frac{p_{1}^{N_{H}}p_{2}^{N_{T}}}{q_{1}^{N_{H}}q_{2}^{N_{T}}}\\]\nLet’s normalize this for sample size by raising this to the power of \\(\\frac{1}{N}\\): \\[\\left(\\frac{p_{1}^{N_{H}}p_{2}^{N_{T}}}{q_{1}^{N_{H}}q_{2}^{N_{T}}}\\right)^{\\frac{1}{N}}\\]\nNow let’s take the log of this expression:\n\\[log \\left(\\frac{p_{1}^{N_{H}}p_{2}^{N_{T}}}{q_{1}^{N_{H}}q_{2}^{N_{T}}}\\right)^{\\frac{1}{N}}\\] \\[=\\frac{1}{N} log \\left(\\frac{p_{1}^{N_{H}}p_{2}^{N_{T}}}{q_{1}^{N_{H}}q_{2}^{N_{T}}}\\right)\\] \\[=\\frac{1}{N} \\left( log\\,p_{1}^{N_{H}} + log\\,p_{2}^{N_{T}} - log\\,q_{1}^{N_{H}} -  log\\,q_{2}^{N_{T}} \\right)\\] \\[=\\frac{1}{N}log\\,p_{1}^{N_{H}} + \\frac{1}{N}log\\,p_{2}^{N_{T}} - \\frac{1}{N}log\\,q_{1}^{N_{H}} - \\frac{1}{N}log\\,q_{2}^{N_{T}}\\] \\[=\\frac{N_{H}}{N}log\\,p_{1} + \\frac{N_{T}}{N}log\\,p_{2} - \\frac{N_{H}}{N}log\\,q_{1} - \\frac{N_{T}}{N}log\\,q_{2}\\] \\[=p_{1}log\\,p_{1} + p_{2}log\\,p_{2} - p_{1}log\\,q_{1} - p_{2}log\\,q_{2}\\] \\[=p_{1}log\\,p_{1} - p_{1}log\\,q_{1} + p_{2}log\\,p_{2} - p_{2}log\\,q_{2}\\] \\[=p_{1}log\\frac{p_{1}}{q_{1}} + p_{2}log\\frac{p_{2}}{q_{2}}\\]\nNote how this formula is the formula for KL divergence."
  },
  {
    "objectID": "cross-entropy-loss.html#cross-entropy-loss",
    "href": "cross-entropy-loss.html#cross-entropy-loss",
    "title": "Cross-Entropy Loss",
    "section": "Cross-Entropy Loss",
    "text": "Cross-Entropy Loss\nThe output for a neural network doing a classification task is an entire probability distribution \\(P\\). Also the ground truth can be considered to be a probability distribution \\(P^{*}\\): one where the true class has a probability of 1 and the other categories have a probability of 0. The goal is for the network to learn and bring \\(P\\) as close as possible to \\(P^{*}\\).\nAs discussed before, one measure of distance between two probability distributions is the KL Divergence: \\(D_{KL}(P||Q)=\\sum_{i}P_{i}\\,log \\frac{P_{i}}{Q_{i}}\\). An intuitive loss function would try to minimize the distance between the predicted distribution and the true distribution:\n\\[D_{KL}(P^{*}||P)=\\sum_{y}P^{*}(y)\\,log \\frac{P^{*}(y)}{P(y)}\\] \\[=\\sum_{y}P^{*}(y)\\,\\left (log\\,P^{*}(y) - log\\,P(y) \\right )\\] \\[=\\sum_{y}P^{*}(y)\\,log\\,P^{*}(y) - \\sum_{y}P^{*}(y)\\,log\\,P(y)\\]\nNote how the first part (summation) of this equation is not dependent on our weights or parameters of the model. So if we want our loss function to minimize this expression, it should minimize the second part, which is dependent on our weight values. That second part is our Cross-Entropy Loss formula. Minimizing the Cross-Entropy Loss is the same as minimizing the KL divergence.\nIt is important that our probabilities add up to 1. To do this, a softmax function is typically used to normalize our output values of the network, turning them into probabilities on which we can use Cross-Entropy Loss:\nTODO"
  },
  {
    "objectID": "cross-entropy-loss.html#formula",
    "href": "cross-entropy-loss.html#formula",
    "title": "Cross-Entropy Loss",
    "section": "Formula",
    "text": "Formula\nHere’s the general formula for Cross-Entropy Loss:\n\\[H(P^{*}|P)=-\\sum_{y}\\,P^{*}(y)\\,log\\,P(y)\\]\n\\(P\\) is the predicted class distribution and \\(P^{*}\\) is the true class distribution."
  },
  {
    "objectID": "cross-entropy-loss.html#single-label-classification",
    "href": "cross-entropy-loss.html#single-label-classification",
    "title": "Cross-Entropy Loss",
    "section": "Single-label Classification",
    "text": "Single-label Classification\nThe above formula for Cross-Entropy Loss can be further simplified, given that often we have a classification problem which predicts just a single label out of a range of labels. The true label in the true class distribution has a probability of 1 while all other labels have a probability of 0. If we look at our loss in this particular case then:\n\\[\\text{loss } L = -\\sum_{y}\\,P^{*}(y)\\,log\\,P(y)\\] \\[=-[P^{*}(y_{1}) log P(y_{1}) + P^{*}(y_{2}) log P(y_{2}) + P^{*}(y_{true}) log P(y_{true}) + P^{*}(y_{n}) log P(y_{n})]\\]\nIn this, y iterates over the probability for every class. Out of all these for only one, the ground thruth will be 1 while for all others y is 0: \\(P^{*}(y_{1}) = 0\\) and \\(P^{*}(y_{true}) = 1\\) Because of this, we can simplify the formula to:\n\\[L = - log \\, P(y_{true})\\]\nIf the prediction for the true class is 1, the loss is 0. If the prediction for the true class is closer to 0, the negative log raises up to infinity. Note how small errors towards the 1-side are punished less then errors on the 0-side.\n\n\n\n\n\n\nNote\n\n\n\nNote how the derived formulas are applicable to calculate the loss for a single image input. In reality we’ll have many more inputs and we’ll take the average loss over a batch of inputs. The loss for each item in the batch is \\(-log\\,P(y_{true})\\) - so: the negative log of the predicted probability of the true class.\n\n\nThis observation is interesting, as the Cross-Entropy loss in this situation onliy looks at the loss on the predicted score for the true-label; ignoring all the rest. Intuitively this makes sense however: the more weight and the closer the predicted probability for the true class is to 1, the less probability there’s left for all other classes."
  },
  {
    "objectID": "cross-entropy-loss.html#multi-label-classification",
    "href": "cross-entropy-loss.html#multi-label-classification",
    "title": "Cross-Entropy Loss",
    "section": "Multi-label Classification",
    "text": "Multi-label Classification\nMulti-label classification is where we want to recognize more than a single label in a picture. (“There’s both a panda and a giraffe to be seen.”) Target and prediction vectors in this case are not probabilities and the target is represented as a “one-hot” vector. This vector has 1 for every class that appears in the picture and 0 for all other classes.\nCross-Entropy Loss cannot be used for these one-hot vectors as these do not represent probabilities. (The sum for all items in such vector can be more than one.) We can look at this problem however as a multiple single-label classification subtasks. For every label we calculate the cross-entropy loss: for “cat” that is 0.34, for “dog” that is 0.56, for “panda” that is 1.2. The total loss is now just the sum of all the losses for every label: 0.34 + 0.56 + 1.2 = 2.1"
  },
  {
    "objectID": "cross-entropy-loss.html#references",
    "href": "cross-entropy-loss.html#references",
    "title": "Cross-Entropy Loss",
    "section": "References",
    "text": "References\n\nArticles\n\nMachineLearningMastery - Cross Entropy for Machine Learning\nTowardsDataScience - Cross Entropy for Dummies\nTowardsDataScience - Cross Entropy for Classification\nKL divergence explained\n\n\n\nVideos\n\nIntuitively understanding Shannon Entropy\nIntuitively Understanding the KL Divergence\nIntuitively Understanding the Cross Entropy Loss"
  },
  {
    "objectID": "install-env.html",
    "href": "install-env.html",
    "title": "Christof's AI-related notes",
    "section": "",
    "text": "Make sure that the following commands cause nothing to happen: - ipython - python - jupyter\nelse delete those first\n\n\n\nSee Jeremy’s “Live Coding 1” video here: https://www.youtube.com/watch?v=56sIyFjihEc&list=PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM&index=1 This avoids using the “system python” and allows to just delete the mambaforge directory and start over with a reinstall Don’t install python though apt-get or anything similar. This is just for the system python.\n\nLocate installer for Mambaforge here: https://github.com/conda-forge/miniforge#mambaforge\nDownload the installer: wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\nRun the script: bash Mambaforge-Linux-x86_64.sh\nClose and re-open the terminal\nit should prefix the path with (base)\n\nThis caused also the .bashrc in the homedirectory to be changed automatically so it starts with every shell. We can see where python was installed by running which python\nYou have 3 options: pip, conda and mamba. Conda and Mamba are fully compatible. Mamba is newer. Whenever you see “conda” you can replace that command with “mamba”.\n\n\n\nipython is a Python REPL and can be installed through: mamba install ipython\n\n\n\nGo to: https://pytorch.org/get-started/locally/ Choose the right options (pick “CUDA” if there’s an NVIDEA GPU in the machine). Run the command: mamba install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\nAlso install the package “ipywidgets”: mamba install ipywidgets\n\n\n\nWe’ll use Jupyter Lab. See here: https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html mamba install -c conda-forge jupyterlab\nrun Jupyter with: jupyter lab\n\n\n\nmamba install -c fastchan fastai\n\n\n\nThese are the dependencies needed for the FastAI notebooks.\nmamba install -c fastchan fastbook (Note that this includes FastAI)\n\n\n\nNeeded in the NLP chapter for the FastAI course:\nmamba install -c fastchan sentencepiece\n\n\n\ngit clone git@github.com:fastai/fastbook.git\n\n\n\nmamba install graphviz (Note that is could be this is already installed)"
  },
  {
    "objectID": "jupyter.html",
    "href": "jupyter.html",
    "title": "Christof's AI-related notes",
    "section": "",
    "text": "Use SHIFT + TAB to get help on the current function."
  },
  {
    "objectID": "self-attention.html",
    "href": "self-attention.html",
    "title": "Self Attention",
    "section": "",
    "text": "The below personal learning notes made use of Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch"
  },
  {
    "objectID": "self-attention.html#what-is-self-attention",
    "href": "self-attention.html#what-is-self-attention",
    "title": "Self Attention",
    "section": "What is self-attention?",
    "text": "What is self-attention?\nSelf-Attention started out from research work in translation and was introduced to give access to all elements in a sequence at each time step. In language tasks, the meaning of a word can depend on the context within a larger text document. Attention enables the model to weigh the importance of different elements in the input sequence and adjust their influence on the output."
  },
  {
    "objectID": "self-attention.html#embedding-an-input-sentence",
    "href": "self-attention.html#embedding-an-input-sentence",
    "title": "Self Attention",
    "section": "Embedding an Input Sentence",
    "text": "Embedding an Input Sentence\nOur input is: “Playing music makes me very happy”. We’ll create an embeding for this entire sentence first.\n\nsentence = \"Playing music makes me very happy\"\n\nsentence_words = sentence.split()\nsentence_words\n\n['Playing', 'music', 'makes', 'me', 'very', 'happy']\n\n\n\nsentence_words_sorted = sorted(sentence_words)\nsentence_words_sorted\n\n['Playing', 'happy', 'makes', 'me', 'music', 'very']\n\n\n\ndict = {word_str:word_idx for word_idx, word_str in enumerate(sentence_words_sorted)}\ndict\n\n{'Playing': 0, 'happy': 1, 'makes': 2, 'me': 3, 'music': 4, 'very': 5}\n\n\ndict is our dictionary, conveniently restricted to just the words we’re using here. Every word we’re using has a number associated (the index in our dictionary.\nWe can now translate our sentence in an array of integers:\n\nsentence_int = torch.tensor([dict[word] for word in sentence_words])\nsentence_int\n\ntensor([0, 4, 2, 3, 5, 1])\n\n\nNow that our sentence is translated into a list of integers, we can use those with an embedding layer to encode the inputs into a real vector embedding. Let’s use 16 dimensions, so that each word is translated/mapped onto an embedding of 16 floats.\nIf our sentence is 6 words (or whatever is the context length we end up choosing), the resulting vector after our embedding layer will be: \\(6 \\times 16\\). We’ll create a pytorch embedding layer with 6 possible indices and a 16-dimensional embedding vector for each index.\n\nembed = torch.nn.Embedding(6,16)\nsentence_embedded = embed(sentence_int).detach()\nprint(sentence_embedded)\nprint(sentence_embedded.shape)\n\ntensor([[ 0.3, -0.2, -0.3, -0.6,  0.3,  0.7, -0.2, -0.4,  0.8, -1.2,  0.7, -1.4,\n          0.2,  1.9,  0.5,  0.3],\n        [ 0.5,  1.0, -0.3, -1.1, -0.0,  1.6, -2.3,  1.1,  0.7,  0.7, -0.9, -0.1,\n         -0.2,  0.1,  0.4, -1.4],\n        [-1.3,  0.2, -2.1,  1.1, -0.4, -0.9, -0.5, -1.1,  0.9,  1.6,  0.6, -0.2,\n          0.1, -0.1,  0.3, -0.6],\n        [ 0.9,  1.6, -1.5,  1.1, -1.2,  1.3,  1.1,  0.1,  2.2, -0.8, -0.3,  0.8,\n         -0.7, -0.8,  0.2,  0.2],\n        [ 0.3, -0.5,  1.0,  0.8, -0.4,  0.5, -0.2, -1.7, -1.6, -1.1,  0.9, -0.7,\n         -0.6, -0.7,  0.6, -1.4],\n        [-0.1, -1.0, -0.2,  0.9,  1.6,  1.3,  1.3, -0.2,  0.5, -1.6,  1.0, -1.1,\n         -1.2,  0.3, -0.6, -2.8]])\ntorch.Size([6, 16])\n\n\nSo, we gave the embedding a tensor of 6 integers, which got translated in \\(6 \\times 16\\) tensors, meaning: each index, representing a word, it translated into an array of 16 floats. We can look into the weights of our embedding layer here as well:\n\nembed.weight\n\nParameter containing:\ntensor([[ 0.3, -0.2, -0.3, -0.6,  0.3,  0.7, -0.2, -0.4,  0.8, -1.2,  0.7, -1.4,\n          0.2,  1.9,  0.5,  0.3],\n        [-0.1, -1.0, -0.2,  0.9,  1.6,  1.3,  1.3, -0.2,  0.5, -1.6,  1.0, -1.1,\n         -1.2,  0.3, -0.6, -2.8],\n        [-1.3,  0.2, -2.1,  1.1, -0.4, -0.9, -0.5, -1.1,  0.9,  1.6,  0.6, -0.2,\n          0.1, -0.1,  0.3, -0.6],\n        [ 0.9,  1.6, -1.5,  1.1, -1.2,  1.3,  1.1,  0.1,  2.2, -0.8, -0.3,  0.8,\n         -0.7, -0.8,  0.2,  0.2],\n        [ 0.5,  1.0, -0.3, -1.1, -0.0,  1.6, -2.3,  1.1,  0.7,  0.7, -0.9, -0.1,\n         -0.2,  0.1,  0.4, -1.4],\n        [ 0.3, -0.5,  1.0,  0.8, -0.4,  0.5, -0.2, -1.7, -1.6, -1.1,  0.9, -0.7,\n         -0.6, -0.7,  0.6, -1.4]], requires_grad=True)\n\n\nThis is basically a kind of “lookup” matrix, where we can lookup the embedding vector corresponding to every token in our dictionary. As such: dictionary token with index:\n\n0 will return: [0.3, -0.2, -0.3, -0.6,  0.3,  0.7, -0.2, -0.4,  0.8, -1.2,  0.7, -1.4, 0.2,  1.9,  0.5,  0.3]\n1 will return: [-0.1, -1.0, -0.2,  0.9,  1.6,  1.3,  1.3, -0.2,  0.5, -1.6,  1.0, -1.1, -1.2,  0.3, -0.6, -2.8]\n2 will return: [-1.3,  0.2, -2.1,  1.1, -0.4, -0.9, -0.5, -1.1,  0.9,  1.6,  0.6, -0.2, 0.1, -0.1,  0.3, -0.6]\n\nand so on. Given our sentence had tokens with indexes: \\(\\begin{bmatrix} 0 & 4 & 2 & 3 & 5 & 1 \\end{bmatrix}\\) we expect first the first row, then the 5th, then 3rd, … and so on, which gives the same end result:\n\\[\\begin{bmatrix}\n0.3 & -0.2 & -0.3 & -0.6 & 0.3 & 0.7 & -0.2 & -0.4 & 0.8 & -1.2 & 0.7 & -1.4 & 0.2 & 1.9 & 0.5 & 0.3 \\\\\n0.5 & 1.0 & -0.3 & -1.1 & -0.0 & 1.6 & -2.3 & 1.1 & 0.7 & 0.7 & -0.9 & -0.1 & -0.2 & 0.1 & 0.4 & -1.4 \\\\\n-1.3 & 0.2 & -2.1 & 1.1 & -0.4 & -0.9 & -0.5 & -1.1 & 0.9 & 1.6 & 0.6 & -0.2 & 0.1 & -0.1 & 0.3 & -0.6 \\\\\n0.9 & 1.6 & -1.5 & 1.1 & -1.2 & 1.3 & 1.1 & 0.1 & 2.2 & -0.8 & -0.3 & 0.8 & -0.7 & -0.8 & 0.2 & 0.2 \\\\\n0.3 & -0.5 & 1.0 & 0.8 & -0.4 & 0.5 & -0.2 & -1.7 & -1.6 & -1.1 & 0.9 & -0.7 & -0.6 & -0.7 & 0.6 & -1.4 \\\\\n-0.1 & -1.0 & -0.2 & 0.9 & 1.6 & 1.3 & 1.3 & -0.2 & 0.5 & -1.6 & 1.0 & -1.1 & -1.2 & 0.3 & -0.6 & -2.8\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "self-attention.html#defining-weight-matrices",
    "href": "self-attention.html#defining-weight-matrices",
    "title": "Self Attention",
    "section": "Defining weight matrices",
    "text": "Defining weight matrices\nSelf-attention has 3 weight matrices which are each adjusted, like other model parameters, during training.\n\n\\(W_{q}\\): projects our input to the query\n\\(W_{k}\\): projects our input to the key\n\\(W_{v}\\): projects our input to the value\n\neach of query \\(q\\), key \\(k\\) and value \\(v\\) are vectors of an input element. We can calculate those through matrix multiplication between those \\(W\\) matrices and the embedded inputs \\(x\\). Our sequence has length \\(T\\).\n\n\\(q^{i} = W_{q} x^{(i)}\\) for the element on index i, i between \\(0\\) and \\(T-1\\)\n\\(k^{i} = W_{k} x^{(i)}\\) for the element on index i, i between \\(0\\) and \\(T-1\\)\n\\(v^{i} = W_{v} x^{(i)}\\) for the element on index i, i between \\(0\\) and \\(T-1\\)\n\nThis will give us three vectors for each input element (token) in our sequence.\nLet’s assume that \\(d\\) is the size (number of dimensions) of each (embedded) word vector x (here 16). Our vector \\(q^{i}\\) is the query vector for word at index \\(i\\) and has a dimension we can choose. We’ll call this \\(d_q\\). In the same way we’ll call \\(d_k\\) as the dimension for \\(k^{i}\\).\nWe’ll calculate the dot product between the query and key vectors, this means that each of them needs to have the same dimensions: \\(d_q = d_k\\). Let’s choose \\(d_q = d_k = 24\\) in this case. If \\[q^{i} = W_{q} x^{(i)}\\] then:\n\nthe dimension for \\(q^{i}\\) is \\(d_q\\) which is the same as \\(d_k\\), here 24, something we chose\nthe dimension for \\(W_{q}\\) is \\(d_q \\times d\\), here 24 by 16, because every word is represented by 16 floats\nthe dimension for \\(x^{(i)}\\) is \\(d\\), here 16 (16 floats for every word)\n\nOur dimension for the value vector can be chosen arbitrarily, let’s say: 28 in our example. That’s the size of the resulting context vector.\nLet’s set up some arbitrary weight matrices:\n\nd = 16\nd_q, d_k, d_v = 24, 24, 28\nW_query = torch.rand(d_q,d)\nW_key = torch.rand(d_k,d)\nW_value = torch.rand(d_v,d)\n\nW_query.shape\n\ntorch.Size([24, 16])"
  },
  {
    "objectID": "self-attention.html#calculate-the-query-key-and-value-for-one-word",
    "href": "self-attention.html#calculate-the-query-key-and-value-for-one-word",
    "title": "Self Attention",
    "section": "Calculate the query, key and value for one word",
    "text": "Calculate the query, key and value for one word\n\nx_2 = sentence_embedded[2]\nquery_2 = W_query.matmul(x_2)\nprint(f'W_query has shape {W_query.shape}, x_2 has shape {x_2.shape} and resulting query_2 has shape: {query_2.shape}\\n')\nprint('the resulting tensor is our query tensor for word at index 2:')\nprint(query_2)\n\nW_query has shape torch.Size([24, 16]), x_2 has shape torch.Size([16]) and resulting query_2 has shape: torch.Size([24])\n\nthe resulting tensor is our query tensor for word at index 2:\ntensor([-2.4, -1.3,  0.0,  0.4, -0.2, -1.8, -1.0, -0.7, -1.9, -0.0, -1.6, -0.7,\n        -2.0, -1.3, -1.6, -1.5, -1.1, -2.8, -0.4,  0.7, -1.7,  1.0, -1.1, -3.2])\n\n\nwe can do the same to get the key and value vector for the word at index 2:\n\nquery_2 = W_query @ x_2 # same as matmul\nkey_2 = W_key @ x_2\nvalue_2 = W_value @ x_2\n\nprint(query_2)\nprint(key_2)\nprint(value_2)\n\ntensor([-2.4, -1.3,  0.0,  0.4, -0.2, -1.8, -1.0, -0.7, -1.9, -0.0, -1.6, -0.7,\n        -2.0, -1.3, -1.6, -1.5, -1.1, -2.8, -0.4,  0.7, -1.7,  1.0, -1.1, -3.2])\ntensor([ 0.6, -2.3, -1.8, -1.3, -1.9, -0.6, -1.5, -3.0,  0.4, -1.9, -0.7, -2.1,\n        -2.0, -0.9, -1.6, -2.1, -0.4, -0.2,  0.5, -1.1, -2.5, -0.4,  0.4, -3.0])\ntensor([-1.1, -0.9, -3.0, -0.7, -2.2,  0.1,  0.0, -2.8, -2.1,  0.7, -0.7, -1.6,\n        -2.6, -1.3, -0.9, -0.5, -1.8, -3.0, -0.7, -1.3,  0.5, -1.1, -1.8, -2.2,\n         0.6, -0.0, -1.8, -1.3])"
  },
  {
    "objectID": "self-attention.html#generalizing-the-calculation-to-all-inputs-in-the-sequence",
    "href": "self-attention.html#generalizing-the-calculation-to-all-inputs-in-the-sequence",
    "title": "Self Attention",
    "section": "Generalizing the calculation to all inputs in the sequence",
    "text": "Generalizing the calculation to all inputs in the sequence\nWe can generalize what we did for a single token or word to all of our inputs in our sequence now.\n\nkeys = (W_key @ sentence_embedded.T).T\nvalues = (W_value @ sentence_embedded.T).T\n\nprint(f'all keys (shape {keys.shape}): \\n{keys}')\nprint(f'all values (shape {values.shape}): \\n {values}')\n\nall keys (shape torch.Size([6, 24])): \ntensor([[ 0.9,  1.2,  2.2,  1.3,  0.8, -1.6, -0.2,  2.0, -0.4,  1.7,  0.1,  2.2,\n         -0.2, -0.7, -0.2, -0.2,  0.8,  1.0,  0.7,  1.7,  2.8,  1.5, -0.9,  1.1],\n        [ 0.9,  0.1,  0.7, -1.1,  1.3, -0.2,  1.0, -0.7,  1.5,  0.3, -0.3,  0.3,\n          1.5, -1.1,  1.4,  0.4, -2.5,  0.4,  0.0, -0.1,  1.2,  1.3,  1.7,  0.5],\n        [ 0.6, -2.3, -1.8, -1.3, -1.9, -0.6, -1.5, -3.0,  0.4, -1.9, -0.7, -2.1,\n         -2.0, -0.9, -1.6, -2.1, -0.4, -0.2,  0.5, -1.1, -2.5, -0.4,  0.4, -3.0],\n        [ 2.5,  2.0,  1.3,  2.5,  2.3,  3.6,  2.9,  1.0,  3.3,  2.8,  3.6,  1.1,\n          3.1,  2.8,  1.8,  1.9,  0.4,  1.4,  2.4,  1.3,  2.2,  2.2,  2.4,  1.8],\n        [-3.1, -2.5, -1.1, -3.5, -4.7, -6.2, -0.9, -3.2, -1.4, -3.5, -2.8, -2.3,\n         -1.3, -3.1, -2.3,  0.4, -2.5, -3.9, -4.2, -1.6, -2.0, -1.7, -1.0, -5.0],\n        [-1.1, -1.4,  0.9, -2.3, -2.7, -3.2, -1.4, -1.0, -0.8,  1.0, -2.0, -0.7,\n         -0.7, -2.5, -2.9, -1.0, -1.0, -1.2, -3.1, -0.6,  1.4, -0.7, -0.9, -1.8]])\nall values (shape torch.Size([6, 28])): \n tensor([[-0.8,  0.3,  1.7,  1.6,  2.2,  1.1,  1.7,  1.6,  1.8,  1.0,  1.3,  0.3,\n          0.3,  0.3,  2.4,  2.0, -1.1,  0.7, -0.2,  0.8,  0.5,  1.0,  1.3,  0.2,\n         -0.3,  0.9,  1.7, -0.3],\n        [ 0.5,  0.3,  0.2,  0.1, -0.2, -1.3, -0.9, -1.3, -0.4, -0.1,  1.1,  0.4,\n         -0.7,  0.1, -1.1,  0.3, -0.3,  0.8, -1.1,  3.0, -0.3,  1.6,  2.7,  0.5,\n         -2.5, -1.5, -0.4,  0.2],\n        [-1.1, -0.9, -3.0, -0.7, -2.2,  0.1,  0.0, -2.8, -2.1,  0.7, -0.7, -1.6,\n         -2.6, -1.3, -0.9, -0.5, -1.8, -3.0, -0.7, -1.3,  0.5, -1.1, -1.8, -2.2,\n          0.6, -0.0, -1.8, -1.3],\n        [ 2.2,  3.5, -2.0,  3.1,  0.7,  3.2,  2.8,  0.8,  2.7,  2.6,  0.1,  1.0,\n          1.1,  3.6,  3.5,  2.5,  2.8,  2.3,  2.6,  4.4,  3.1,  5.2,  2.9,  2.7,\n          4.3,  1.3,  1.4,  3.8],\n        [-1.4, -3.1, -1.4, -1.0, -0.9, -2.5, -2.1, -1.9, -2.0, -3.8, -3.9, -3.1,\n         -2.2, -3.1, -3.7, -1.9, -1.9, -1.7, -1.4, -4.2, -3.5, -1.8, -3.1, -1.7,\n         -2.1, -1.9, -2.1, -3.5],\n        [-0.3, -2.7,  0.8,  1.5,  0.0, -1.9, -0.7,  1.2, -1.0, -4.3,  0.8, -3.7,\n         -1.2, -2.9,  0.7, -1.6, -1.8, -0.4, -1.7, -1.0,  0.2,  2.9, -1.4,  0.9,\n         -1.4, -2.6, -0.4, -0.7]])\n\n\nThis is a matrix with one row per word in our input sequence, each such row representing the key or value vector for the correponding word.\nIf we want to get to the attention-vector for the second input element, that element will act as the query. We will matrix-multiply that query with the keys for each of the other input elements so we will do from 1 to \\(T\\) (our sequence length):\n\n\\(q^{(2)} \\cdot k^{(1)} = \\omega_{2,1}\\)\n\\(q^{(2)} \\cdot k^{(2)} = \\omega_{2,2}\\)\n…\n\\(q^{(2)} \\cdot k^{(T)} = \\omega_{2,T}\\)\n\nFor example, for \\(\\omega_{2,4}\\):\n\n\\(q^{(2)}\\) is: [-2.4, -1.3,  0.0,  0.4, -0.2, ... , -1.7,  1.0, -1.1, -3.2]\n\\(k^{(4)}\\) is: [-3.1, -2.5, -1.1, -3.5, -4.7, ..., -2.0, -1.7, -1.0, -5.0]\n\nso \\(\\omega_{2,4} = q^{(2)} \\cdot k^{(4)}\\) (dimensions: \\([24] \\times [24]\\)):\n\nomega_24 = query_2.dot(keys[4]) # note: this is the same as: query_2 @ keys[4]\nomega_24\n\ntensor(76.0)\n\n\nThis tensor is the unnormalized attention weight for the query at the 5th input element (index 4). Also here we can generalize this from one input element to another (element with idx 2 to element with idx 4), towards an element and all input elements in our sequence.\n\\(\\omega_2 = q^{(2)} \\cdot k\\) (dimensions: \\([24] \\times [24, 6]\\))\n\nomega_2 = query_2 @ keys.T\nomega_2\n\ntensor([ -9.9, -13.0,  32.0, -60.8,  76.0,  36.6])\n\n\nThis tensor contains the attention weights for each of the 6 words/tokens in our sequence with respect to token on index 4."
  },
  {
    "objectID": "self-attention.html#computing-attention-scores",
    "href": "self-attention.html#computing-attention-scores",
    "title": "Self Attention",
    "section": "Computing Attention Scores",
    "text": "Computing Attention Scores\nWe now need to move from our unnormalized attention weights \\(\\omega\\) towards normalized weights: \\(\\alpha\\). We’ll do this through a softmax function, scaled by dividing by \\(\\sqrt{d_k}\\)\n\nscaled = omega_2 / (d_k**0.5)\nattention_weights_2 = F.softmax(scaled, dim=0)\ntorch.set_printoptions(precision=9, sci_mode=False, profile='short')\nprint(scaled)\nprint('')\nprint(f'attention weights: \\n{attention_weights_2}')\nprint(f'sum: \\n{torch.sum(attention_weights_2)}') # sums up to 1\n\ntensor([ -2.017098665,  -2.646891594,   6.530897617, -12.409937859,\n         15.506885529,   7.466487408])\n\nattention weights: \ntensor([    0.000000025,     0.000000013,     0.000126352,     0.000000000,\n            0.999551475,     0.000322036])\nsum: \n0.9999998807907104\n\n\nNow we compute the context vector \\(z^{(2)}\\), which is an attention-weighted version of our input element \\(x^{(2)}\\) (the embedded tensor for the token on index 2 of our sequence). Dimensions:\n\n\\(\\alpha_2\\) is 6\n\\(values\\) is 6, 28\n\\(z^{(2)}\\) is 28 = \\(1 \\times 6\\) @ \\(6 \\times 28\\)\n\n\ncontext_vector_2 = attention_weights_2 @ values\ncontext_vector_2.shape, context_vector_2\n\n(torch.Size([28]),\n tensor([-1.434536815, -3.057869434, -1.372985959, -1.015810966, -0.939488649,\n         -2.540240049, -2.134411335, -1.869234562, -1.999078512, -3.760509729,\n         -3.873581171, -3.136465788, -2.163633823, -3.094663382, -3.710036516,\n         -1.867943287, -1.886817217, -1.702059269, -1.404274344, -4.158794403,\n         -3.530863047, -1.818631649, -3.132727146, -1.715395093, -2.099170208,\n         -1.885426879, -2.099651337, -3.486021757]))\n\n\nSo what did we see here? We have all the normalized attention weights for every element in the sequence with respect to our input element on index 2 in the sequence. That’s attention_weights_2 or \\(\\alpha_2\\). We multiply that vector with values, which is the collection of value vectors for each of our input elements: 6 elements, with each a vector of 28 floats. The resulting tensor, is a weighted combination of all 6 vectors with 28 values into a single vector of 28 values. This is the context vector for input element on index 2."
  },
  {
    "objectID": "self-attention.html#multi-head-attention",
    "href": "self-attention.html#multi-head-attention",
    "title": "Self Attention",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\nTODO"
  },
  {
    "objectID": "self-attention.html#summary",
    "href": "self-attention.html#summary",
    "title": "Self Attention",
    "section": "Summary",
    "text": "Summary\n\nwe have our input sequence which is tokenized per word (in this case)\nusing an embedding matrix, words are translated into a tensor of floats with \\(d\\) dimensions\nwe will want to calculate, for every element in the sequence 3 vectors:\n\n\\(q^i\\): the query vector for element on input index i, with dimension we choose: \\(d_q\\) (\\(d_q = d_k\\))\n\\(k^i\\): the key vector for element on input index i, with dimension we choose: \\(d_k\\) (\\(d_q = d_k\\))\n\nto calculate these two vectors for every input element, we define 3 weight matrices:\n\n\\(W_{q}\\)\n\\(W_{k}\\)\n\\(W_{v}\\)\n\n\nTODO"
  },
  {
    "objectID": "self-attention.html#resources",
    "href": "self-attention.html#resources",
    "title": "Self Attention",
    "section": "Resources",
    "text": "Resources\n\nAttention is all you need\nThinking Like Transformers"
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Optimization",
    "section": "",
    "text": "To optimize our weight vector, we’ll use Gradient Descent. This implies finding the gradient in every single dimension of the loss function of our output.\n\n\nIn a single dimension, this is the derivatie of a function:\n\\[\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\\]\nOur loss function (of which we’ll want to calculate the derivate) has many more dimensions. For multiple dimensions/variables, the gradient of a function is vector of partial derivates along each dimension. The slope in any direction is the dot product of the direction with the gradient.\nThe gradient points in the direction of greatest increase of the function. The negative gradient gives us the direction of greatest decrease.\n\n\n\nImagine we have a weight vector \\(W = [0.34, -1.1, 0.78, 3]\\) which leads to a loss of \\(1.254\\). We’ll want to find the gradient \\(dW\\) which is a vector of the same shape as \\(W\\). Each slot in \\(dW\\) will tell us how much the loss will change if we move a tiny amount in that coordinate direction."
  },
  {
    "objectID": "optimization.html#references",
    "href": "optimization.html#references",
    "title": "Optimization",
    "section": "References",
    "text": "References\nStanford - Lecture 3 | Loss Functions and Optimization"
  },
  {
    "objectID": "nanogpt-math-trick.html",
    "href": "nanogpt-math-trick.html",
    "title": "NanoGPT - Math Trick",
    "section": "",
    "text": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntorch.set_printoptions(precision=1, sci_mode=False, profile='short')\ngenerator = torch.manual_seed(42)\n\nbatch_size = 4\ncontext_length = 6\nvocab_size = 8 # 8 characters or language tokens possible\n\nB,T,C = (batch_size, context_length, vocab_size)\nB,T,C\n\n(4, 6, 8)\n\n\n\n# x is our training data\nx = torch.randn(batch_size,context_length, vocab_size)\nx\n\ntensor([[[ 0.48,  1.35, -0.16, -0.42,  0.94, -0.18,  1.06,  0.21],\n         [ 1.31,  0.46,  0.26, -0.76, -2.05, -1.53,  0.40,  0.63],\n         [-0.15, -2.32,  1.30,  0.49,  1.13, -0.36,  0.36,  2.00],\n         [ 1.04,  1.69,  0.02, -0.83, -1.08, -0.78,  0.51,  0.08],\n         [ 0.40,  1.99, -0.46, -0.06, -1.37,  0.33, -0.98,  0.30],\n         [ 0.19,  0.41, -1.58,  2.25,  1.00,  1.36,  0.63,  0.41]],\n\n        [[-0.35,  1.46,  0.17,  1.05,  0.01, -0.08,  0.64,  0.57],\n         [ 0.51,  0.22, -0.91,  1.48, -0.91, -0.53, -0.81,  0.52],\n         [-0.13,  0.78,  0.56,  1.86,  1.04, -0.86,  0.84, -0.32],\n         [-1.98,  0.02, -1.41, -1.88, -0.18,  0.79,  0.52, -0.27],\n         [ 1.71,  0.06,  0.86, -0.59, -1.03, -0.22,  0.80,  0.91],\n         [ 0.27, -0.04, -0.48,  0.32,  0.39,  0.73,  0.25,  0.08]],\n\n        [[-0.71, -0.05,  0.52,  0.97, -0.28, -0.61, -0.56, -0.97],\n         [ 1.34,  0.71,  0.35, -0.54,  0.86, -0.67,  1.07, -0.25],\n         [-2.31, -1.29,  0.21, -1.24,  1.86,  0.06,  0.77,  2.56],\n         [ 1.20, -0.98,  0.30,  0.93, -1.97, -1.41,  1.74,  1.84],\n         [-0.00,  0.08, -0.46, -0.06, -0.22, -1.25, -0.49, -0.34],\n         [-0.59,  0.08,  0.19, -0.97,  1.89,  0.44,  0.14,  0.31]],\n\n        [[-0.49,  0.05,  0.33,  0.13,  2.85, -0.74,  0.20, -1.34],\n         [-0.57, -0.33, -0.31, -0.72,  0.08, -0.21, -0.57,  0.40],\n         [-0.55,  1.99,  0.85, -0.70,  0.31,  0.29,  0.41, -1.26],\n         [-0.39,  1.89,  0.18, -0.04, -0.09, -1.18,  1.55,  0.54],\n         [-0.20,  0.69, -1.34,  1.65,  1.98, -0.10,  0.49, -0.44],\n         [-0.49, -0.36, -0.06, -0.48,  0.99,  0.27, -1.83,  0.36]]])\n\n\n\n# for the 2nd item in our batch (index 1), the features of token 5 (index 4) are:\nx[1,4]\n\ntensor([ 0.0, -0.3, -1.3, -0.6,  0.5,  0.5,  1.1,  0.1])\n\n\n\n# if instead we want all the features of all the tokens before token 5 (index 4):\nx[1, :4]\n\ntensor([[-0.9, -0.7,  0.1,  0.5, -0.5,  1.2, -0.8, -0.7],\n        [-1.4,  0.0, -0.1,  0.7, -0.1,  1.8, -1.2,  1.4],\n        [ 1.4,  0.9,  2.2,  0.5,  0.3, -0.2, -1.1,  1.3],\n        [-0.2,  0.5,  0.1,  0.4,  0.6, -0.6, -2.2, -0.8]])\n\n\n\n# and if we want to include the features for token 5 itself:\nx[1,:4+1]\n\ntensor([[-0.9, -0.7,  0.1,  0.5, -0.5,  1.2, -0.8, -0.7],\n        [-1.4,  0.0, -0.1,  0.7, -0.1,  1.8, -1.2,  1.4],\n        [ 1.4,  0.9,  2.2,  0.5,  0.3, -0.2, -1.1,  1.3],\n        [-0.2,  0.5,  0.1,  0.4,  0.6, -0.6, -2.2, -0.8],\n        [ 0.0, -0.3, -1.3, -0.6,  0.5,  0.5,  1.1,  0.1]])\n\n\n\n# if we want to calculate the mean:\ntorch.mean(x[1, :4+1],0) # 0 here means: take means across the columns\n\ntensor([-0.2,  0.1,  0.2,  0.3,  0.2,  0.5, -0.8,  0.2])\n\n\nNote how the mean of -0.9, 1.3, -0.7, -1.6, -1.4 is -0.7\n\n(-0.9+1.3-0.7-1.6-1.4)/5\n\n-0.6599999999999999\n\n\n\nx_bag_of_words = torch.zeros((B,T,C))\nx_bag_of_words\n\ntensor([[[0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0.]]])\n\n\n\nfor b in range(0,batch_size):\n    for t in range(0,context_length):\n        xprev_and_self = x[b,:t+1] # (t+1,vocab_size)\n        x_bag_of_words[b,t] = torch.mean(xprev_and_self, 0) # (1, vocab_size)\nx_bag_of_words\n\ntensor([[[ 1.9,  1.5,  0.9, -2.1,  0.7, -1.2, -0.0, -1.6],\n         [ 0.6,  1.6,  0.3, -1.8, -0.0, -0.9, -0.4, -0.4],\n         [ 0.9,  1.0,  0.0, -1.0, -0.3, -0.2, -0.0,  0.3],\n         [ 1.0,  1.1,  0.2, -0.4, -0.3, -0.2, -0.1,  0.4],\n         [ 0.5,  0.7,  0.1, -0.0, -0.1, -0.2,  0.0,  0.2],\n         [ 0.2,  0.7, -0.1, -0.1, -0.3,  0.2, -0.2,  0.1]],\n\n        [[-0.9, -0.7,  0.1,  0.5, -0.5,  1.2, -0.8, -0.7],\n         [-1.2, -0.3,  0.0,  0.6, -0.3,  1.5, -1.0,  0.3],\n         [-0.3,  0.1,  0.7,  0.6, -0.1,  0.9, -1.0,  0.6],\n         [-0.3,  0.2,  0.6,  0.5,  0.1,  0.5, -1.3,  0.3],\n         [-0.2,  0.1,  0.2,  0.3,  0.2,  0.5, -0.8,  0.2],\n         [-0.0, -0.0, -0.0,  0.4, -0.1,  0.3, -0.5,  0.3]],\n\n        [[-2.5,  0.5,  0.8,  0.0,  0.6,  0.6,  1.1, -0.5],\n         [-1.3,  0.6,  0.6,  0.1,  0.5,  0.9,  0.5, -0.4],\n         [-1.4,  0.4,  0.2,  0.2,  0.5,  0.6,  0.2, -0.1],\n         [-1.0,  0.3,  0.2,  0.4,  0.7,  0.6,  0.3,  0.0],\n         [-0.4,  0.5, -0.1,  0.1,  0.5,  0.8,  0.4,  0.2],\n         [-0.2,  0.4, -0.4,  0.2,  0.4,  0.8,  0.2,  0.2]],\n\n        [[-1.0,  1.0,  1.6,  1.5,  0.3, -0.2, -0.7,  0.1],\n         [-0.3,  1.0,  0.6,  1.5, -1.1, -0.3, -1.0,  0.5],\n         [-0.8,  0.7,  0.4,  0.9, -0.6, -0.5, -0.6, -0.1],\n         [-1.0,  0.9,  0.6,  0.7, -0.9, -0.2, -0.2,  0.5],\n         [-0.8,  0.7,  0.3,  0.5, -0.9, -0.4, -0.4,  0.3],\n         [-0.8,  0.4, -0.0,  0.4, -0.6, -0.5, -0.2,  0.3]]])\n\n\nNote how we calculated before that for:\n\nbatch item 2 (index 1)\ntoken 5 (index 4)\n\nThe bag of words was: [-0.7,  0.1, -0.6, -0.1, -0.1,  0.1, -0.0,  0.5]\nEach of these represent the mean of token 5 and all tokens that came before it, for each channel/features."
  },
  {
    "objectID": "nanogpt-math-trick.html#making-this-efficient",
    "href": "nanogpt-math-trick.html#making-this-efficient",
    "title": "NanoGPT - Math Trick",
    "section": "Making this efficient",
    "text": "Making this efficient\nWhat we just did was incredibly inefficient and can be optimized through matrix multiplication. Let’s start by creating some test matrices to play with:\n\na = torch.ones(3,3)\nb = torch.randint(0,4,(3,2)).float() # need to convert to float for matrix multiplication\nc = a @ b # matrix multiplication\n\na,b,c\n\n(tensor([[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]),\n tensor([[0., 2.],\n         [2., 1.],\n         [1., 3.]]),\n tensor([[3., 6.],\n         [3., 6.],\n         [3., 6.]]))\n\n\nNote how in the first row of matrix c, the 7 8 is the sum of columns 2 3 2 and 2 3 3 because of how matrix multiplying works. The other rows are exactly the same values (7 8) because matrix a has 1 in every row.\nLet’s introduce the torch.tril() function now, which gives us back the lower triangle of a matrix, zero-ing out the upper part:\n\na = torch.tril(a)\na\n\ntensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])\n\n\nLet’s multiply this by our matrix b:\n\nc = a @ b\na,b,c\n\n(tensor([[1., 0., 0.],\n         [1., 1., 0.],\n         [1., 1., 1.]]),\n tensor([[0., 2.],\n         [2., 1.],\n         [1., 3.]]),\n tensor([[0., 2.],\n         [2., 3.],\n         [3., 6.]]))\n\n\nThe first row of a is multiplied by both the columns of b to get to the first row of b: (1*2 + 0*3 + 0*2)=2   (1*2 + 0*3 + 0*3)=2  The second row of a is multiplied by both the columns of b to get to the second row of b: (1*2 + 1*3 + 0*2)=5   (1*2 + 1*3 + 0*3)=5  The third row of a is multiplied by both the columns of b to get to the third row of b: (1*2 + 1*3 + 1*2)=7   (1*2 + 1*3 + 1*3)=8\nNote how:\n\nthe first row in the resulting matrix c has the original values of row 1 of b,\nthe second row has the sum of the values of row 1 and 2 of b and\nthe third row has the sum of the values of row 1, 2 and 3 of b\n\nWe don’t want to sum however. We want to try and get to a mean of a value and all values that came before it. To get the mean, we need to divide the sum by the number of items. For that reason, we won’t use all 1s to multiply with:\n\na1 = torch.ones(3,3)\na2 = torch.tril(a1)\na2_sum = torch.sum(a2, 1, keepdim=True) # keepdim is required given that we need the broadcasting to work\na3 = a2 / a2_sum\na1, a2, a2_sum, a3, b\n\n(tensor([[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]),\n tensor([[1., 0., 0.],\n         [1., 1., 0.],\n         [1., 1., 1.]]),\n tensor([[1.],\n         [2.],\n         [3.]]),\n tensor([[1.0, 0.0, 0.0],\n         [0.5, 0.5, 0.0],\n         [0.3, 0.3, 0.3]]),\n tensor([[0., 2.],\n         [2., 1.],\n         [1., 3.]]))\n\n\nIf we now want the averages:\n\nc = a3 @ b\nc\n\ntensor([[0.0, 2.0],\n        [1.0, 1.5],\n        [1.0, 2.0]])\n\n\n(see Let’s build GPT: from scratch, in code, spelled out. )"
  },
  {
    "objectID": "nanogpt-math-trick.html#applying-the-matrix-technique-on-our-training-data",
    "href": "nanogpt-math-trick.html#applying-the-matrix-technique-on-our-training-data",
    "title": "NanoGPT - Math Trick",
    "section": "Applying the matrix technique on our training data",
    "text": "Applying the matrix technique on our training data\nOur training data x looked like:\n\nx # shape: (batch_size x context_length x vocab_size) == (B x T x C)\n\ntensor([[[ 0.48,  1.35, -0.16, -0.42,  0.94, -0.18,  1.06,  0.21],\n         [ 1.31,  0.46,  0.26, -0.76, -2.05, -1.53,  0.40,  0.63],\n         [-0.15, -2.32,  1.30,  0.49,  1.13, -0.36,  0.36,  2.00],\n         [ 1.04,  1.69,  0.02, -0.83, -1.08, -0.78,  0.51,  0.08],\n         [ 0.40,  1.99, -0.46, -0.06, -1.37,  0.33, -0.98,  0.30],\n         [ 0.19,  0.41, -1.58,  2.25,  1.00,  1.36,  0.63,  0.41]],\n\n        [[-0.35,  1.46,  0.17,  1.05,  0.01, -0.08,  0.64,  0.57],\n         [ 0.51,  0.22, -0.91,  1.48, -0.91, -0.53, -0.81,  0.52],\n         [-0.13,  0.78,  0.56,  1.86,  1.04, -0.86,  0.84, -0.32],\n         [-1.98,  0.02, -1.41, -1.88, -0.18,  0.79,  0.52, -0.27],\n         [ 1.71,  0.06,  0.86, -0.59, -1.03, -0.22,  0.80,  0.91],\n         [ 0.27, -0.04, -0.48,  0.32,  0.39,  0.73,  0.25,  0.08]],\n\n        [[-0.71, -0.05,  0.52,  0.97, -0.28, -0.61, -0.56, -0.97],\n         [ 1.34,  0.71,  0.35, -0.54,  0.86, -0.67,  1.07, -0.25],\n         [-2.31, -1.29,  0.21, -1.24,  1.86,  0.06,  0.77,  2.56],\n         [ 1.20, -0.98,  0.30,  0.93, -1.97, -1.41,  1.74,  1.84],\n         [-0.00,  0.08, -0.46, -0.06, -0.22, -1.25, -0.49, -0.34],\n         [-0.59,  0.08,  0.19, -0.97,  1.89,  0.44,  0.14,  0.31]],\n\n        [[-0.49,  0.05,  0.33,  0.13,  2.85, -0.74,  0.20, -1.34],\n         [-0.57, -0.33, -0.31, -0.72,  0.08, -0.21, -0.57,  0.40],\n         [-0.55,  1.99,  0.85, -0.70,  0.31,  0.29,  0.41, -1.26],\n         [-0.39,  1.89,  0.18, -0.04, -0.09, -1.18,  1.55,  0.54],\n         [-0.20,  0.69, -1.34,  1.65,  1.98, -0.10,  0.49, -0.44],\n         [-0.49, -0.36, -0.06, -0.48,  0.99,  0.27, -1.83,  0.36]]])\n\n\nIt has 3 dimensions: batch, time/context, features. Let’s try to treat this matrix so that every value is the mean of all prior values in the same batch item. So for example, the 3rd token (index 2) on the 2nd item in our batch (index 1) has features:  [-0.1,  0.8,  0.6,  1.9,  1.0, -0.9,  0.8, -0.3]\nWe want these features to be the averages of the features of itself and all the features of the prior tokens:\n\n[-0.4,  1.5,  0.2,  1.1,  0.0, -0.1,  0.6,  0.6]\n[ 0.5,  0.2, -0.9,  1.5, -0.9, -0.5, -0.8,  0.5]\n\nLet’s start by applying a triangular vector (shape ) to each batch item:\n\ntril = torch.tril(torch.ones(T,T))\ntril, tril.shape\n\n(tensor([[1., 0., 0., 0., 0., 0.],\n         [1., 1., 0., 0., 0., 0.],\n         [1., 1., 1., 0., 0., 0.],\n         [1., 1., 1., 1., 0., 0.],\n         [1., 1., 1., 1., 1., 0.],\n         [1., 1., 1., 1., 1., 1.]]),\n torch.Size([6, 6]))\n\n\nLet’s apply this to our entire training data set; using broadcasting:\n\ntril @ x # element-wise (T,T) @ (BxTxC) == (BxTxT) @ (BxTxC) == (BxTxC)\n\ntensor([[[ 1.9,  1.5,  0.9, -2.1,  0.7, -1.2, -0.0, -1.6],\n         [ 1.2,  3.1,  0.5, -3.5, -0.0, -1.8, -0.8, -0.8],\n         [ 2.8,  3.0,  0.0, -3.1, -0.8, -0.7, -0.0,  0.8],\n         [ 4.1,  4.3,  0.6, -1.7, -1.0, -0.7, -0.3,  1.7],\n         [ 2.7,  3.4,  0.4, -0.0, -0.7, -1.1,  0.0,  0.9],\n         [ 1.2,  4.4, -0.5, -0.6, -2.0,  1.0, -1.2,  0.4]],\n\n        [[-0.9, -0.7,  0.1,  0.5, -0.5,  1.2, -0.8, -0.7],\n         [-2.3, -0.6,  0.0,  1.2, -0.6,  3.0, -2.0,  0.6],\n         [-0.9,  0.2,  2.2,  1.7, -0.2,  2.8, -3.1,  1.9],\n         [-1.0,  0.8,  2.3,  2.2,  0.3,  2.2, -5.3,  1.2],\n         [-1.0,  0.4,  0.9,  1.6,  0.9,  2.7, -4.1,  1.2],\n         [-0.3, -0.1, -0.1,  2.2, -0.9,  1.9, -2.8,  1.7]],\n\n        [[-2.5,  0.5,  0.8,  0.0,  0.6,  0.6,  1.1, -0.5],\n         [-2.7,  1.2,  1.2,  0.2,  0.9,  1.9,  1.1, -0.8],\n         [-4.2,  1.1,  0.6,  0.7,  1.6,  1.9,  0.7, -0.2],\n         [-4.2,  1.4,  0.7,  1.4,  2.7,  2.3,  1.4,  0.2],\n         [-2.2,  2.4, -0.7,  0.3,  2.6,  3.9,  2.1,  0.8],\n         [-1.1,  2.4, -2.5,  1.2,  2.2,  5.0,  1.4,  1.4]],\n\n        [[-1.0,  1.0,  1.6,  1.5,  0.3, -0.2, -0.7,  0.1],\n         [-0.6,  1.9,  1.2,  3.1, -2.2, -0.6, -1.9,  0.9],\n         [-2.5,  2.2,  1.2,  2.7, -1.9, -1.4, -1.8, -0.2],\n         [-4.1,  3.5,  2.5,  2.8, -3.5, -0.6, -1.0,  1.8],\n         [-4.1,  3.6,  1.7,  2.6, -4.4, -2.2, -2.1,  1.3],\n         [-4.6,  2.1, -0.3,  2.7, -3.4, -2.7, -1.4,  2.0]]])\n\n\nThis is similar to our previous example on how to incrementally add numbers, but now with an additional broadcasted batch dimension. We’ll want not to add those numbers those, but to average them out as before:\n\nweights = tril / torch.sum(tril, 1, keepdim=True) # keepdim for broadcasting\nweights\n\ntensor([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.5, 0.5, 0.0, 0.0, 0.0, 0.0],\n        [0.3, 0.3, 0.3, 0.0, 0.0, 0.0],\n        [0.2, 0.2, 0.2, 0.2, 0.0, 0.0],\n        [0.2, 0.2, 0.2, 0.2, 0.2, 0.0],\n        [0.2, 0.2, 0.2, 0.2, 0.2, 0.2]])\n\n\n\nx_bag_of_words_with_matrix = weights @ x\nx[0], x_bag_of_words_with_matrix[0]\n\n(tensor([[ 1.9,  1.5,  0.9, -2.1,  0.7, -1.2, -0.0, -1.6],\n         [-0.8,  1.6, -0.4, -1.4, -0.7, -0.6, -0.8,  0.8],\n         [ 1.6, -0.2, -0.5,  0.4, -0.8,  1.1,  0.8,  1.7],\n         [ 1.3,  1.3,  0.6,  1.3, -0.2,  0.0, -0.3,  0.9],\n         [-1.4, -0.9, -0.2,  1.7,  0.3, -0.4,  0.3, -0.8],\n         [-1.6,  1.0, -0.9, -0.6, -1.3,  2.1, -1.2, -0.5]]),\n tensor([[ 1.9,  1.5,  0.9, -2.1,  0.7, -1.2, -0.0, -1.6],\n         [ 0.6,  1.6,  0.3, -1.8, -0.0, -0.9, -0.4, -0.4],\n         [ 0.9,  1.0,  0.0, -1.0, -0.3, -0.2, -0.0,  0.3],\n         [ 1.0,  1.1,  0.2, -0.4, -0.3, -0.2, -0.1,  0.4],\n         [ 0.5,  0.7,  0.1, -0.0, -0.1, -0.2,  0.0,  0.2],\n         [ 0.2,  0.7, -0.1, -0.1, -0.3,  0.2, -0.2,  0.1]]))\n\n\n\nx_bag_of_words.allclose(x_bag_of_words_with_matrix)\n\nTrue\n\n\n\ntorch.set_printoptions(precision=2, sci_mode=False, profile='short')\nprint(x[0])\nprint('===')\nprint(x_bag_of_words_with_matrix[0])\n\ntensor([[ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n        [ 1.64, -0.16, -0.50,  0.44, -0.76,  1.08,  0.80,  1.68],\n        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n        [-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n        [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49]])\n===\ntensor([[ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n        [ 0.59,  1.57,  0.25, -1.75, -0.02, -0.90, -0.41, -0.42],\n        [ 0.94,  0.99,  0.00, -1.02, -0.27, -0.24, -0.00,  0.28],\n        [ 1.02,  1.07,  0.16, -0.43, -0.26, -0.17, -0.07,  0.42],\n        [ 0.54,  0.68,  0.08, -0.00, -0.14, -0.22,  0.01,  0.18],\n        [ 0.19,  0.73, -0.08, -0.10, -0.33,  0.17, -0.20,  0.07]])"
  },
  {
    "objectID": "nanogpt-math-trick.html#doing-the-same-with-softmax",
    "href": "nanogpt-math-trick.html#doing-the-same-with-softmax",
    "title": "NanoGPT - Math Trick",
    "section": "Doing the same with SoftMax",
    "text": "Doing the same with SoftMax\nThis was our tril matrix, which we got by tril = torch.tril(torch.ones(T,T))\n\ntril\n\ntensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\n\n\n\nweights = torch.zeros(T,T)\nweights\n\ntensor([[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]])\n\n\nWe can mask the weights and and apply a number (here minus infinity) to the non-masked values:\n\nweights = weights.masked_fill(tril == 0, float('-inf')) # => this is essentially saying: the future cannot communicate with the past\nweights\n\ntensor([[0., -inf, -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf, -inf],\n        [0., 0., 0., 0., -inf, -inf],\n        [0., 0., 0., 0., 0., -inf],\n        [0., 0., 0., 0., 0., 0.]])\n\n\nThe above matrix is essentially making sure that for:\n\nthe first token (which is line 1), it only has itself to refer to: [0., -inf, -inf, -inf, -inf, -inf]\ntoken 2 (which is line 2), has itself and token 1 to refer to: [0., 0., -inf, -inf, -inf, -inf]\ntoken 3 (which is line 3), has token 1, 2 and itself to refer to: [0., 0., 0., -inf, -inf, -inf]\n\nand so on…\n\nweights = weights.softmax(1)\nweights\n\ntensor([[1.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n        [0.50, 0.50, 0.00, 0.00, 0.00, 0.00],\n        [0.33, 0.33, 0.33, 0.00, 0.00, 0.00],\n        [0.25, 0.25, 0.25, 0.25, 0.00, 0.00],\n        [0.20, 0.20, 0.20, 0.20, 0.20, 0.00],\n        [0.17, 0.17, 0.17, 0.17, 0.17, 0.17]])\n\n\n\nx_bag_of_words_using_softmax = weights @ x\ntorch.allclose(x_bag_of_words_using_softmax, x_bag_of_words)\n\nTrue\n\n\n(For more explanation, see Let’s build GPT: from scratch, in code, spelled out.)"
  },
  {
    "objectID": "nanogpt-math-trick.html#summary",
    "href": "nanogpt-math-trick.html#summary",
    "title": "NanoGPT - Math Trick",
    "section": "Summary",
    "text": "Summary\nWhat we’ve done here is to do a weighted average of elements and their past using matrix multiplication. In our case here, all weights were equal, but you can see that this does not have to be the case necesarily. The elements in the lower part of the triangular weights matrix determine how much of each element in the past the weighted average is build of."
  },
  {
    "objectID": "backprop.html",
    "href": "backprop.html",
    "title": "Exploring Backpropagation",
    "section": "",
    "text": "These notes accompany the Backpropagation and MLP lesson from Jeremy Howard. The associated workbook can be found here: https://github.com/fastai/course22p2/blob/master/nbs/03_backprop.ipynb"
  },
  {
    "objectID": "backprop.html#loading-data",
    "href": "backprop.html#loading-data",
    "title": "Exploring Backpropagation",
    "section": "Loading Data",
    "text": "Loading Data\nWe start by loading MNIST.\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\nfrom pathlib import Path\nimport torch\nfrom torch import tensor\nfrom fastcore.test import test_close\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(42)\n\nmpl.rcParams['image.cmap'] = 'gray'\ntorch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\nnp.set_printoptions(precision=2, linewidth=125)\n\n# path_data = Path('data')\n# path_gz = path_data/'mnist.pkl.gz'\n# with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n# x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n\npath = untar_data(URLs.MNIST)\npath.ls()\n\n(#2) [Path('/home/xstof/.fastai/data/mnist_png/training'),Path('/home/xstof/.fastai/data/mnist_png/testing')]\n\n\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nbatch_size = 32\n\n# Define the transformation to apply to the data\ntransform = transforms.Compose(\n    [transforms.Grayscale(),\n     transforms.ToTensor(),  # convert PIL image to PyTorch tensor\n     transforms.Normalize((0.5,), (0.5,))])  # normalize the data to have a mean of 0.5 and std of 0.5\n\n# Load the training dataset\ntrainset = torchvision.datasets.MNIST(root='./data/mnist', train=True, download=True, transform=transform)\n\n# Create a data loader for the training dataset\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\ndataiter = iter(trainloader)\nimages, labels = dataiter.__next__()\n\n\nimages.shape, labels.shape\n\n(torch.Size([32, 1, 28, 28]), torch.Size([32]))\n\n\nAs we can see, every image is stored with one explicit channel. Let’s simplify this by removing this dimension. In addition we’ll concatenate all pixels.\n\nimages = images.view(32,28*28)\nimages.shape\n\ntorch.Size([32, 784])\n\n\nThis means, we have a batch of 32 images, each with 784 pixels (28 * 28). Visuially that looks like this:\n\n# Display the third image in the batch as an image\ndef show_image(img):\n    plt.imshow(img.view(28,28), cmap='gray')\n    plt.show()\nshow_image(images[2])\nprint(f\"label for this image: {labels[2]}\")\n\n\n\n\nlabel for this image: 4"
  },
  {
    "objectID": "backprop.html#doing-a-forward-and-backward-pass",
    "href": "backprop.html#doing-a-forward-and-backward-pass",
    "title": "Exploring Backpropagation",
    "section": "Doing a Forward and Backward Pass",
    "text": "Doing a Forward and Backward Pass\n\nBuilding the model\nLet’s set some parameters first:\n\nnr_hidden: number of hidden layers (50)\nn: number of images (32)\nm: number of pixels in every image (784)\n\n\nnr_hidden = 50\nn, m = images.shape\n\nAnd let’s assume some weights and biases for two layers of our neural net:\n\nw1 = torch.randn(m, nr_hidden) # 784 x 50 (every hidden node has 784 inputs, 50 of them)\nb1 = torch.zeros(nr_hidden)    # 50\nw2 = torch.randn(nr_hidden, 1) # 50 x 1 (every end node has 50 inputs, 1 of them)\nb2 = torch.zeros(1)            # 1\n\n\nw1, w1.shape\n\n(tensor([[ 0.90, -2.11,  0.68,  ...,  0.90,  0.08,  0.53],\n         [-0.49,  1.19, -0.81,  ...,  0.03,  0.64,  0.58],\n         [ 1.07, -0.45, -0.68,  ..., -0.21, -0.73,  0.10],\n         ...,\n         [ 1.08, -0.58, -0.85,  ..., -0.02, -0.74, -0.13],\n         [-1.27, -0.45,  1.57,  ...,  0.27,  0.46, -0.78],\n         [-0.37,  0.00, -0.45,  ..., -1.25,  0.91, -0.74]]),\n torch.Size([784, 50]))\n\n\n\nw2[:5,:], w2.shape\n\n(tensor([[ 0.93],\n         [-0.03],\n         [-1.70],\n         [-1.88],\n         [ 0.12]]),\n torch.Size([50, 1]))\n\n\nLet’s define a simple lineair layer using these weights and biases. The layer takes as input a vector x and calculates the output for each of it’s output nodes (nr_hidden in this case for the first layer)\n\ndef lin(x, w, b): return x @ w + b\n\n\nt = lin(images, w1, b1)\nt.shape, t\n\n(torch.Size([32, 50]),\n tensor([[ 25.58,  -7.93, -26.05,  ..., -18.41,  17.69,   1.37],\n         [ 31.21,   7.71,   4.64,  ...,  -3.45,  26.99,  11.58],\n         [ 29.64, -11.81, -27.58,  ..., -11.57,  12.22, -25.35],\n         ...,\n         [ 30.52, -23.99,  -2.10,  ...,   1.38, -15.33, -17.99],\n         [ 23.25,  -0.25, -21.73,  ...,  -6.70,  26.05, -10.02],\n         [ -1.56, -12.81, -29.23,  ..., -24.95,  -1.28,   5.35]]))\n\n\nSo, when we put our training batch (images) through a linear layer defined by w1 and b1, we get back a tensor 32 rows (our 32 images in the batch) and for each image/row a set of 50 values. One for each of our hidden layer neurons.\nLet’s define a non-linear function now:\n\ndef relu(x): return x.clamp_min(0.)\n\n\nt = relu(t)\nt.shape, t\n\n(torch.Size([32, 50]),\n tensor([[25.58,  0.00,  0.00,  ...,  0.00, 17.69,  1.37],\n         [31.21,  7.71,  4.64,  ...,  0.00, 26.99, 11.58],\n         [29.64,  0.00,  0.00,  ...,  0.00, 12.22,  0.00],\n         ...,\n         [30.52,  0.00,  0.00,  ...,  1.38,  0.00,  0.00],\n         [23.25,  0.00,  0.00,  ...,  0.00, 26.05,  0.00],\n         [ 0.00,  0.00,  0.00,  ...,  0.00,  0.00,  5.35]]))\n\n\nThe shape did not change, but any element below 0 is brought back to zero now.\nOur entire two-layer model now can be defined like this:\n\ndef model(x):\n    l1 = lin(x, w1, b1)\n    l2 = relu(l1)\n    return lin(l2, w2, b2)\n\n\nres = model(images)\nres.shape, res[:5,:]\n\n(torch.Size([32, 1]),\n tensor([[73.00],\n         [87.68],\n         [16.46],\n         [48.06],\n         [95.10]]))\n\n\nThis is the resulting output of our neural net: one result for each of our input images in our batch of 32.\n\n\nDefine a loss function\nWe’ll define a completely wrong, unreasonable loss function here, for our learning purposes. We’ll replace this later by something more relevant. We’ll use a Mean Square Error or MSE function.\nOur loss is always defined between the predicted output of our neural net and the actual truth values:\n\nx_train = images\ny_train = labels\n\nres.shape, y_train.shape\n\n(torch.Size([32, 1]), torch.Size([32]))\n\n\nLet’s substract the truth values from the network’s output:\n\n(res - y_train).shape\n\ntorch.Size([32, 32])\n\n\n\n(res - y_train)\n\ntensor([[68.00, 73.00, 69.00,  ..., 66.00, 70.00, 65.00],\n        [82.68, 87.68, 83.68,  ..., 80.68, 84.68, 79.68],\n        [11.46, 16.46, 12.46,  ...,  9.46, 13.46,  8.46],\n        ...,\n        [33.79, 38.79, 34.79,  ..., 31.79, 35.79, 30.79],\n        [77.07, 82.07, 78.07,  ..., 75.07, 79.07, 74.07],\n        [34.46, 39.46, 35.46,  ..., 32.46, 36.46, 31.46]])\n\n\nWhat does this mean? When we subtract y_train values from res, we’re doing broadcasting. This does not pan out like we would like it to be. res looks like this and has a dimension too much:\n\nres[:5,:]\n\ntensor([[73.00],\n        [87.68],\n        [16.46],\n        [48.06],\n        [95.10]])\n\n\nLet’s remove this dimension:\n\nres.view(-1)\n\ntensor([  73.00,   87.68,   16.46,   48.06,   95.10,    3.29,   42.34,  -19.67,   91.37,   35.68,  -29.29,   33.01,  -27.92,\n           6.47,  122.97,   36.14,  -19.11,   41.91,   70.73,   78.53,  -34.66,   40.94,   89.76,   29.97,  110.14,   -4.39,\n          11.30, -121.07,    9.19,   38.79,   82.07,   39.46])\n\n\nan alternative to this way of doing this is:\n\nres.squeeze()\n\ntensor([  73.00,   87.68,   16.46,   48.06,   95.10,    3.29,   42.34,  -19.67,   91.37,   35.68,  -29.29,   33.01,  -27.92,\n           6.47,  122.97,   36.14,  -19.11,   41.91,   70.73,   78.53,  -34.66,   40.94,   89.76,   29.97,  110.14,   -4.39,\n          11.30, -121.07,    9.19,   38.79,   82.07,   39.46])\n\n\nOr just like this:\n\nres[:,0]\n\ntensor([  73.00,   87.68,   16.46,   48.06,   95.10,    3.29,   42.34,  -19.67,   91.37,   35.68,  -29.29,   33.01,  -27.92,\n           6.47,  122.97,   36.14,  -19.11,   41.91,   70.73,   78.53,  -34.66,   40.94,   89.76,   29.97,  110.14,   -4.39,\n          11.30, -121.07,    9.19,   38.79,   82.07,   39.46])\n\n\nBack to our MSE:\n\n(res[:,0]-y_train).shape\n\ntorch.Size([32])\n\n\n\ndef mse(output, targ): return (output[:,0]-targ).pow(2).mean()\n\n\npreds = model(x_train)\n\n\nmse(preds, y_train)\n\ntensor(3401.72)\n\n\n\n\nGradients and backwards pass\nOur forward and pass looks like:\n\ninp, targ = x_train, y_train\nl1 = lin(inp, w1, b1)\nl2 = relu(l1)\nout = lin(l2, w2, b2) # let's call this layer 3\ndiff = out[:,0]-targ\nloss = diff.pow(2).mean()\n\nout[:,0],loss\n\n(tensor([  73.00,   87.68,   16.46,   48.06,   95.10,    3.29,   42.34,  -19.67,   91.37,   35.68,  -29.29,   33.01,  -27.92,\n            6.47,  122.97,   36.14,  -19.11,   41.91,   70.73,   78.53,  -34.66,   40.94,   89.76,   29.97,  110.14,   -4.39,\n           11.30, -121.07,    9.19,   38.79,   82.07,   39.46]),\n tensor(3401.72))\n\n\nFor the backward pass we now need to work backwards using the gradients.\nWe’ll store the gradients on the variables themselves. Let’s start with our loss. This is defined like \\[\\frac{\\sum_{i=0}^{n}(out_i-targ_i)^2}{n}\\] or \\[\\frac{\\sum_{i=0}^{n}(x_i-y_i)^2}{n}\\]\nThe derivative of this with respect to for example \\(x_0\\) is:\n\\[\\frac{2(x_0-y_0)}{n}\\]\nIn this equation, n is equal to the number of elements in our input (like in: inp.shape[0]).\nSo we have: \\[\\frac{\\delta loss}{\\delta out_i} = \\frac{2(out_i-y_i)}{n}\\]\n\nout.g = 2.*diff / inp.shape[0]\nout.g\n\ntensor([ -5.70,  -0.52,  -2.79,  -5.61,  -9.25,  -1.48,  -5.61,  -3.56,  -7.16,  -3.42,  -6.27,  -3.59,  -9.84,  -3.85,\n         -6.12,  -4.37,  -4.44, -11.04, -10.75,  -5.65,  -6.34,   1.36,  -6.78,  -5.28,  -7.37,  -9.56,  -4.15,  -6.23,\n         -4.51,  -3.03,  -8.17,  -9.61])\n\n\nWhat do these mean? Each of those numbers is an indication to: “if I change this out param (the result of the last linear layer), then how much does that influence the loss?”. At this point however we only still have the influence on the loss with respec to this “out” values, not yet with respect to our weights. To get there we need to remember that:\n\\[ \\frac{\\delta loss}{\\delta w2_i} = \\frac{\\delta loss}{\\delta out_i} \\frac{\\delta out_i}{\\delta w2_i}  \\]\nL3 only had one neuron with 50 incoming connections. How did our weights for w2 look like again?\n\nw2.shape # 50 x 1 (every end node has 50 inputs, 1 of them)\n\ntorch.Size([50, 1])\n\n\nThis means we now need to calculate the gradient of our linear layer. This in turn will determine how much the change of each param of our lineair layer has an influence on the final loss. Let’s consider the single output neuron \\(o\\) from layer l3 (as it only has one output neuron). Its output is determined by the formula:\n\\[o = (l2_0 . w2_0 + l2_1 . w2_1 + ... + l2_{50}) + b2\\]\nwhat is now the derivate of this function with regards to, let’s say \\(w2_3\\)?\n\ndef lin_grad(inp, out, w, b):\n    inp.g"
  },
  {
    "objectID": "tmux.html",
    "href": "tmux.html",
    "title": "Christof's AI-related notes",
    "section": "",
    "text": "Also see Jeremy’s “Live Coding 2” video here: https://www.youtube.com/watch?v=0pWjZByJ3Lk&t=2341s\n\n\nsudo apt install tmux\n\n\n\nUse CTRL + B as the prime shortcut for tmux commands.\n\n\n\nCommand\nAction\n\n\n\n\n%\nsplit vertical\n\n\n”\nsplit horizontal\n\n\narrow\nmove between panes\n\n\nCTRL + D\nclose pane\n\n\nz\nmake pane full screen (press again to revert)\n\n\nd\ndetach from session\n\n\n\nRun tmux a to attach to last session again."
  },
  {
    "objectID": "architectures.html",
    "href": "architectures.html",
    "title": "Architectures",
    "section": "",
    "text": "import gc\ndef report_gpu():\n    print(torch.cuda.list_gpu_processes())\n    gc.collect()\n    torch.cuda.empty_cache()"
  },
  {
    "objectID": "architectures.html#references",
    "href": "architectures.html#references",
    "title": "Architectures",
    "section": "References",
    "text": "References\n\nPractical Deep Learning for Coders - Lesson 6 (evaluating different architectures\nWhich image models are best (Notebook from Jeremy on Kaggle\nThe best vision models for fine tuning (Notebook from Jeremy on Kaggle"
  },
  {
    "objectID": "traffic-sign-recognition.html",
    "href": "traffic-sign-recognition.html",
    "title": "Traffic Sign Recognition",
    "section": "",
    "text": "from fastai.vision.all import *\nfrom pathlib import Path\nimport torch, numpy as np, pandas as pd"
  },
  {
    "objectID": "traffic-sign-recognition.html#data-discovery",
    "href": "traffic-sign-recognition.html#data-discovery",
    "title": "Traffic Sign Recognition",
    "section": "Data Discovery",
    "text": "Data Discovery\nLet’s have a look at one single image from the dataset first, so we know what we’re dealing with.\n\ndata_path = Path('./traffic-sign')\ntraining_path = Path(data_path/'BelgiumTSC_Training')\ntesting_path = Path(data_path/'BelgiumTSC_Testing')\nimg = PILImage.create(training_path/'00006/00147_00000.ppm')\nimg.to_thumb(400)\n\n\n\n\n\ntraining_path.absolute()\n\nPath('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training')"
  },
  {
    "objectID": "traffic-sign-recognition.html#build-a-quick-first-model",
    "href": "traffic-sign-recognition.html#build-a-quick-first-model",
    "title": "Traffic Sign Recognition",
    "section": "Build a quick first model",
    "text": "Build a quick first model\nWe’ll use FastAI to build a quick first model and see how far we get with that:\n\nfnames = get_image_files(training_path) # gives us something like: [Path('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training/00006/00221_00000.ppm'),Path('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training/00006/00327_00001.ppm'),Path('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training/00006/00326_00001.ppm')]\ndef label_func(x): return x.parent.name\ndls = ImageDataLoaders.from_path_func(training_path.absolute(), fnames, label_func, valid_ptc=0.2, seed=42, item_tfms=Resize(224))\ndls.valid_ds.items[:3]\n\n[Path('traffic-sign/BelgiumTSC_Training/00041/01401_00002.ppm'),\n Path('traffic-sign/BelgiumTSC_Training/00041/01149_00000.ppm'),\n Path('traffic-sign/BelgiumTSC_Training/00056/00151_00000.ppm')]\n\n\n\ndls.show_batch(nrows=2, ncols=4)\n\n\n\n\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n/home/xstof/mambaforge/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  warnings.warn(\n/home/xstof/mambaforge/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      2.765614\n      0.682876\n      0.177243\n      00:14\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.525985\n      0.168609\n      0.049234\n      00:17\n    \n    \n      1\n      0.250160\n      0.116856\n      0.032823\n      00:17\n    \n  \n\n\n\nLet’s experiment with using a different learning rate; inspired by the learning rate finder:\n\nlr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00<?]\n    \n    \n\n\n    \n      \n      24.56% [14/57 00:04<00:13 0.1065]\n    \n    \n\n\nKeyboardInterrupt: \n\n\n\n#learn = vision_learner(dls, resnet50, metrics=error_rate)\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n#learn.fine_tune(9, base_lr=1e-4, freeze_epochs=3)\nlearn.fine_tune(60, base_lr=1e-4)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      5.272032\n      3.511821\n      0.810722\n      00:12\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      4.322254\n      3.168630\n      0.722101\n      00:16\n    \n    \n      1\n      4.073076\n      2.831528\n      0.649891\n      00:16\n    \n    \n      2\n      3.727599\n      2.497461\n      0.554705\n      00:16\n    \n    \n      3\n      3.388522\n      2.206383\n      0.492341\n      00:17\n    \n    \n      4\n      3.040637\n      1.924404\n      0.439825\n      00:17\n    \n    \n      5\n      2.693980\n      1.672572\n      0.386214\n      00:17\n    \n    \n      6\n      2.407907\n      1.453600\n      0.324945\n      00:17\n    \n    \n      7\n      2.051530\n      1.267668\n      0.291028\n      00:17\n    \n    \n      8\n      1.805684\n      1.090083\n      0.249453\n      00:17\n    \n    \n      9\n      1.539369\n      0.948155\n      0.214442\n      00:17\n    \n    \n      10\n      1.306851\n      0.811161\n      0.172867\n      00:17\n    \n    \n      11\n      1.106547\n      0.700792\n      0.148796\n      00:17\n    \n    \n      12\n      0.940430\n      0.611577\n      0.134573\n      00:17\n    \n    \n      13\n      0.800753\n      0.532640\n      0.122538\n      00:17\n    \n    \n      14\n      0.702655\n      0.451409\n      0.111597\n      00:17\n    \n    \n      15\n      0.593532\n      0.394403\n      0.092998\n      00:17\n    \n    \n      16\n      0.500641\n      0.342239\n      0.080963\n      00:17\n    \n    \n      17\n      0.422322\n      0.299860\n      0.068928\n      00:17\n    \n    \n      18\n      0.366605\n      0.262785\n      0.065646\n      00:17\n    \n    \n      19\n      0.297391\n      0.230043\n      0.061269\n      00:17\n    \n    \n      20\n      0.261126\n      0.208636\n      0.057987\n      00:17\n    \n    \n      21\n      0.234320\n      0.183331\n      0.051422\n      00:17\n    \n    \n      22\n      0.197685\n      0.165864\n      0.041575\n      00:17\n    \n    \n      23\n      0.170155\n      0.149604\n      0.033917\n      00:17\n    \n    \n      24\n      0.142079\n      0.138600\n      0.032823\n      00:17\n    \n    \n      25\n      0.135151\n      0.128846\n      0.029540\n      00:17\n    \n    \n      26\n      0.117065\n      0.115731\n      0.025164\n      00:17\n    \n    \n      27\n      0.107656\n      0.105887\n      0.020788\n      00:17\n    \n    \n      28\n      0.099796\n      0.098209\n      0.022976\n      00:17\n    \n    \n      29\n      0.086735\n      0.095067\n      0.022976\n      00:17\n    \n    \n      30\n      0.074561\n      0.092893\n      0.020788\n      00:17\n    \n    \n      31\n      0.068633\n      0.087713\n      0.021882\n      00:17\n    \n    \n      32\n      0.060317\n      0.084592\n      0.018600\n      00:17\n    \n    \n      33\n      0.057718\n      0.081010\n      0.019694\n      00:17\n    \n    \n      34\n      0.054945\n      0.077566\n      0.018600\n      00:17\n    \n    \n      35\n      0.051517\n      0.072584\n      0.016411\n      00:17\n    \n    \n      36\n      0.045129\n      0.074221\n      0.017505\n      00:17\n    \n    \n      37\n      0.040007\n      0.072452\n      0.016411\n      00:17\n    \n    \n      38\n      0.037295\n      0.069297\n      0.016411\n      00:17\n    \n    \n      39\n      0.035892\n      0.068540\n      0.016411\n      00:17\n    \n    \n      40\n      0.031437\n      0.066725\n      0.017505\n      00:17\n    \n    \n      41\n      0.035449\n      0.066943\n      0.013129\n      00:17\n    \n    \n      42\n      0.033334\n      0.066297\n      0.015317\n      00:17\n    \n    \n      43\n      0.028813\n      0.063224\n      0.012035\n      00:17\n    \n    \n      44\n      0.030409\n      0.063654\n      0.014223\n      00:17\n    \n    \n      45\n      0.030178\n      0.060446\n      0.012035\n      00:17\n    \n    \n      46\n      0.028744\n      0.062024\n      0.014223\n      00:17\n    \n    \n      47\n      0.030050\n      0.060515\n      0.012035\n      00:17\n    \n    \n      48\n      0.022793\n      0.062337\n      0.013129\n      00:17\n    \n    \n      49\n      0.026851\n      0.061434\n      0.013129\n      00:17\n    \n    \n      50\n      0.031460\n      0.060007\n      0.010941\n      00:17\n    \n    \n      51\n      0.024205\n      0.059654\n      0.013129\n      00:17\n    \n    \n      52\n      0.019963\n      0.060336\n      0.013129\n      00:17\n    \n    \n      53\n      0.021899\n      0.060539\n      0.013129\n      00:17\n    \n    \n      54\n      0.020242\n      0.061180\n      0.014223\n      00:17\n    \n    \n      55\n      0.028077\n      0.060424\n      0.013129\n      00:17\n    \n    \n      56\n      0.029257\n      0.060081\n      0.013129\n      00:17\n    \n    \n      57\n      0.027021\n      0.058778\n      0.012035\n      00:17\n    \n    \n      58\n      0.028170\n      0.060581\n      0.013129\n      00:17\n    \n    \n      59\n      0.023330\n      0.059495\n      0.013129\n      00:17\n    \n  \n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nWhat we learn here is that with a slow learning rate and 60 epochs, we’re getting down to an error rate of 0.0185 or approximately 2%. Let’s have a look at where those errors are in our data set by looking at a couple of examples.\n\ninterp = ClassificationInterpretation.from_learner(learn)\n# interp.plot_confusion_matrix(figsize=(12,12), dpi=60)\n# interp.most_confused()\ninterp.plot_top_losses(12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.lr\n\n0.001\n\n\n\nimg = PILImage.create(training_path/'00047/00010_00000.ppm')\nimg.to_thumb(400)"
  },
  {
    "objectID": "traffic-sign-recognition.html#data-augmentation",
    "href": "traffic-sign-recognition.html#data-augmentation",
    "title": "Traffic Sign Recognition",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nWe’ll bring more variation in our dataset by augmenting our data now: resizing, skewing etc..\n\nfnames = get_image_files(training_path) # gives us something like: [Path('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training/00006/00221_00000.ppm'),Path('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training/00006/00327_00001.ppm'),Path('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training/00006/00326_00001.ppm')]\ndef label_func(x): return x.parent.name\nprint(f'taking training images from path: {training_path.absolute()}')\n# dls = ImageDataLoaders.from_path_func(training_path.absolute(), fnames, label_func, valid_ptc=0.2, seed=42, \n#                                     item_tfms=RandomResizedCropGPU(224, min_scale=.5),\n#                                     batch_tfms=aug_transforms())\ndls = ImageDataLoaders.from_path_func(training_path.absolute(), fnames, label_func, valid_ptc=0.2, seed=42, \n                                      item_tfms=Resize(224),\n                                      batch_tfms=aug_transforms())\n\ntaking training images from path: /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training\n\n\n\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\nNow let’s train another model using these augmented versions of our images:\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n\nlr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(30, base_lr=1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      3.608980\n      1.084114\n      0.268053\n      00:13\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.430907\n      0.698026\n      0.169584\n      00:17\n    \n    \n      1\n      1.056159\n      0.454872\n      0.114880\n      00:17\n    \n    \n      2\n      0.749442\n      0.302111\n      0.082057\n      00:17\n    \n    \n      3\n      0.528836\n      0.194878\n      0.048140\n      00:17\n    \n    \n      4\n      0.355295\n      0.115626\n      0.030635\n      00:17\n    \n    \n      5\n      0.253510\n      0.082918\n      0.021882\n      00:17\n    \n    \n      6\n      0.168605\n      0.061095\n      0.014223\n      00:17\n    \n    \n      7\n      0.124957\n      0.046257\n      0.010941\n      00:17\n    \n    \n      8\n      0.096753\n      0.034474\n      0.008753\n      00:17\n    \n    \n      9\n      0.074080\n      0.032612\n      0.006565\n      00:18\n    \n    \n      10\n      0.053752\n      0.034777\n      0.009847\n      00:17\n    \n    \n      11\n      0.052620\n      0.021781\n      0.007659\n      00:17\n    \n    \n      12\n      0.049211\n      0.022058\n      0.004376\n      00:17\n    \n    \n      13\n      0.042934\n      0.025418\n      0.006565\n      00:18\n    \n    \n      14\n      0.027902\n      0.031443\n      0.007659\n      00:17\n    \n    \n      15\n      0.029918\n      0.029799\n      0.007659\n      00:17\n    \n    \n      16\n      0.033211\n      0.019079\n      0.004376\n      00:18\n    \n    \n      17\n      0.020829\n      0.017851\n      0.004376\n      00:17\n    \n    \n      18\n      0.016527\n      0.020309\n      0.006565\n      00:17\n    \n    \n      19\n      0.015539\n      0.024692\n      0.008753\n      00:17\n    \n    \n      20\n      0.017411\n      0.017495\n      0.003282\n      00:17\n    \n    \n      21\n      0.011809\n      0.016099\n      0.004376\n      00:17\n    \n    \n      22\n      0.006842\n      0.015237\n      0.004376\n      00:18\n    \n    \n      23\n      0.006921\n      0.017040\n      0.005470\n      00:18\n    \n    \n      24\n      0.009414\n      0.017562\n      0.005470\n      00:18\n    \n    \n      25\n      0.006611\n      0.015416\n      0.004376\n      00:18\n    \n    \n      26\n      0.006826\n      0.014822\n      0.004376\n      00:18\n    \n    \n      27\n      0.007830\n      0.014976\n      0.004376\n      00:18\n    \n    \n      28\n      0.006141\n      0.014720\n      0.004376\n      00:18\n    \n    \n      29\n      0.004856\n      0.015269\n      0.004376\n      00:18\n    \n  \n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(12)"
  },
  {
    "objectID": "traffic-sign-recognition.html#data-cleaning",
    "href": "traffic-sign-recognition.html#data-cleaning",
    "title": "Traffic Sign Recognition",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nSomething tells us that there might still be images wrongly classified in our training data. We can explore this using the ImageClassifierCleaner widget. When we do so, it turns out that indeed there’s images that are wrongly classified in our dataset. Let’s clean those up.\n\nimport ipywidgets\nfrom fastai.vision.widgets import ImageClassifierCleaner\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndeleted = cleaner.delete()\nfor d in deleted: print(f'deleted: {d}')\nfor idx,cat in cleaner.change(): print(f'index: {idx} in cat: {cat}')\n\ndeleted: 3\nindex: 2 in cat: 00038\nindex: 6 in cat: 00038\n\n\n\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n\n\n#for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\nfor idx,cat in cleaner.change(): print(f'move {cleaner.fns[idx]} to {Path(cleaner.fns[idx]).parent.parent / cat}')\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), Path(cleaner.fns[idx]).parent.parent / cat)\n\nmove traffic-sign/BelgiumTSC_Training/00039/01911_00001.ppm to traffic-sign/BelgiumTSC_Training/00038\nmove traffic-sign/BelgiumTSC_Training/00039/01911_00002.ppm to traffic-sign/BelgiumTSC_Training/00038\n\n\nAfter our cleaning operation, our error rate went down even a bit further, to less than 0,5%."
  },
  {
    "objectID": "traffic-sign-recognition.html#testing-the-model",
    "href": "traffic-sign-recognition.html#testing-the-model",
    "title": "Traffic Sign Recognition",
    "section": "Testing the Model",
    "text": "Testing the Model\nWe now have a model that we can test against a testing data set. Let’s download that first:\n\ntest_files = get_image_files(testing_path)\ntest_files_string = test_files.map(lambda f: str(f.absolute()))\nprint(f'file {test_files[0]} has correct label: {label_func(test_files[0])}')\ntype(test_files_string)\n\nfile traffic-sign/BelgiumTSC_Testing/00006/02039_00002.ppm has correct label: 00006\n\n\nfastcore.foundation.L\n\n\n\ntestdata = pd.DataFrame(np.array(test_files_string) , columns = ['filename'])\ntestdata\n\n\n\n\n\n  \n    \n      \n      filename\n    \n  \n  \n    \n      0\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00002.ppm\n    \n    \n      1\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00001.ppm\n    \n    \n      2\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00000.ppm\n    \n    \n      3\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02083_00000.ppm\n    \n    \n      4\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02083_00002.ppm\n    \n    \n      ...\n      ...\n    \n    \n      2515\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02030_00000.ppm\n    \n    \n      2516\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/01984_00001.ppm\n    \n    \n      2517\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02193_00002.ppm\n    \n    \n      2518\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02194_00000.ppm\n    \n    \n      2519\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02171_00002.ppm\n    \n  \n\n2520 rows × 1 columns\n\n\n\nLet’s extend our testdata with a column that contains the ground truth:\n\ntestdata['label'] = testdata.apply(lambda row: label_func(Path(row['filename'])), axis = 1)\ntestdata\n\n\n\n\n\n  \n    \n      \n      filename\n      label\n    \n  \n  \n    \n      0\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00002.ppm\n      00006\n    \n    \n      1\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00001.ppm\n      00006\n    \n    \n      2\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00000.ppm\n      00006\n    \n    \n      3\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02083_00000.ppm\n      00006\n    \n    \n      4\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02083_00002.ppm\n      00006\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      2515\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02030_00000.ppm\n      00018\n    \n    \n      2516\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/01984_00001.ppm\n      00018\n    \n    \n      2517\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02193_00002.ppm\n      00018\n    \n    \n      2518\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02194_00000.ppm\n      00018\n    \n    \n      2519\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02171_00002.ppm\n      00018\n    \n  \n\n2520 rows × 2 columns\n\n\n\nNow we need the predictions from our model for each row, so we can compare those against the truth label. For a single item this goes like this:\n\npredicted_label, predicted_idx, predicted_probs = learn.predict(testdata['filename'][0])\npredicted_label2, predicted_idx, predicted_probs = learn.predict(testdata['filename'][2515])\npredicted_label3, predicted_idx, predicted_probs = learn.predict(testdata['filename'][1000])\nprint(f'for file {testdata.filename[0]} we predicted a label of {predicted_label}')\nprint(f'for file {testdata.filename[2515]} we predicted a label of {predicted_label2}')\nprint(f'for file {testdata.filename[1000]} we predicted a label of {predicted_label3}')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor file /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00002.ppm we predicted a label of 00005\nfor file /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02030_00000.ppm we predicted a label of 00018\nfor file /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00019/02124_00000.ppm we predicted a label of 00019\n\n\nFor all our labels:\n\ndef predict(filename): \n    predicted_label, _, _ = learn.predict(filename)\n    return predicted_label\n# predict(testdata['filename'][0])\n\n\n# testdata['predicted'] = testdata.apply(lambda row: predict(row['filename']), axis=1)\n# testdata\n\n\n# get a dataloaders that has the exact same transformations applied as during training:\ntst_dl = learn.dls.test_dl(testdata.filename)\npreds, _ = learn.get_preds(dl=tst_dl)\npreds\n\n\n\n\n\n\n\n\nTensorBase([[1.1896e-03, 3.2670e-05, 9.1291e-06,  ..., 2.1299e-06,\n             1.9771e-07, 6.4971e-07],\n            [7.4278e-05, 2.8059e-06, 5.6598e-06,  ..., 7.9164e-07,\n             8.2710e-07, 4.7385e-07],\n            [1.3759e-04, 1.2551e-04, 1.1870e-04,  ..., 1.3204e-05,\n             1.2395e-05, 3.2555e-05],\n            ...,\n            [2.2055e-07, 1.0322e-06, 1.1944e-05,  ..., 1.5188e-07,\n             1.5343e-06, 6.6106e-07],\n            [1.4748e-07, 2.0833e-07, 1.8539e-07,  ..., 2.2542e-08,\n             9.2468e-07, 1.0912e-07],\n            [7.6813e-08, 4.3783e-09, 1.5067e-08,  ..., 2.6169e-09,\n             1.4087e-08, 2.1832e-09]])\n\n\n\npreds[0]\n\nTensorBase([1.1896e-03, 3.2670e-05, 9.1291e-06, 3.8356e-05, 2.5520e-04,\n            9.5691e-01, 4.1199e-02, 2.4053e-06, 2.2224e-05, 6.6508e-06,\n            2.9380e-06, 9.0039e-07, 5.7958e-07, 2.8193e-07, 1.2864e-06,\n            8.2779e-08, 5.7793e-06, 7.6418e-07, 4.1857e-06, 6.6967e-08,\n            6.4850e-07, 5.7714e-06, 3.5365e-08, 1.5344e-06, 1.2382e-04,\n            1.7104e-06, 3.4046e-06, 6.3849e-06, 4.5234e-07, 4.1998e-05,\n            1.7630e-07, 2.4071e-07, 2.4519e-07, 9.8147e-07, 1.1657e-06,\n            2.7170e-07, 9.9690e-07, 3.8178e-07, 6.5608e-08, 1.0690e-07,\n            9.0703e-08, 3.6442e-06, 2.3846e-07, 7.8937e-08, 6.5177e-07,\n            2.4670e-07, 1.4663e-05, 6.6853e-06, 1.0686e-06, 1.9420e-06,\n            3.4722e-06, 2.9240e-08, 5.8613e-06, 5.9746e-09, 2.1060e-07,\n            1.2808e-06, 1.7598e-06, 4.8697e-06, 7.5268e-05, 2.1299e-06,\n            1.9771e-07, 6.4971e-07])\n\n\n\npreds[0].sum()\n\nTensorBase(1.)\n\n\n\nprint(f'index of max value in first row is: {preds[0].argmax()}') # this gives us the index of where the max value in this row is:\npreds_indexes = preds.argmax(dim=1)\npreds_indexes\n\nindex of max value in first row is: 5\n\n\nTensorBase([ 5,  6,  6,  ..., 18, 18, 18])\n\n\nNow we have the indexes for the max values for each row in our predictions. Let’s translate that to the actual label:\n\npreds_indexes_list = L(preds_indexes.tolist())\npred_labels = preds_indexes_list.map(lambda idx: learn.dls.vocab[idx])\npred_labels[:5]\n\n(#5) ['00005','00006','00006','00005','00006']\n\n\nLets turn this information into another column on our dataframe:\n\ntestdata['predicted'] = pd.DataFrame(np.array(pred_labels))\ntestdata['correct'] = testdata.label == testdata.predicted\ntestdata\n\n\n\n\n\n  \n    \n      \n      filename\n      label\n      predicted\n      correct\n    \n  \n  \n    \n      0\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00002.ppm\n      00006\n      00005\n      False\n    \n    \n      1\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00001.ppm\n      00006\n      00006\n      True\n    \n    \n      2\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02039_00000.ppm\n      00006\n      00006\n      True\n    \n    \n      3\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02083_00000.ppm\n      00006\n      00005\n      False\n    \n    \n      4\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00006/02083_00002.ppm\n      00006\n      00006\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2515\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02030_00000.ppm\n      00018\n      00018\n      True\n    \n    \n      2516\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/01984_00001.ppm\n      00018\n      00018\n      True\n    \n    \n      2517\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02193_00002.ppm\n      00018\n      00018\n      True\n    \n    \n      2518\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02194_00000.ppm\n      00018\n      00018\n      True\n    \n    \n      2519\n      /home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Testing/00018/02171_00002.ppm\n      00018\n      00018\n      True\n    \n  \n\n2520 rows × 4 columns\n\n\n\n\ncorrectCount = testdata.correct.sum()\ntotalCount = testdata.correct.count()\nprint(f'Out of {totalCount} items {correctCount} were predicted correctly.  This is {(correctCount/totalCount)*100}%')\n\nOut of 2520 items 2467 were predicted correctly.  This is 97.89682539682539%"
  },
  {
    "objectID": "traffic-sign-recognition.html#trying-convnext_large_in22ft1k",
    "href": "traffic-sign-recognition.html#trying-convnext_large_in22ft1k",
    "title": "Traffic Sign Recognition",
    "section": "Trying convnext_large_in22ft1k",
    "text": "Trying convnext_large_in22ft1k\nOur choice for resnet might not be optimal. Let’s have a look if an architecture like convnext (which should perform better and be better suitable for transfer learning) significantly impacts the results:\n\n!pip install timm\n\nRequirement already satisfied: timm in /home/xstof/mambaforge/lib/python3.9/site-packages (0.6.7)\nRequirement already satisfied: torch>=1.4 in /home/xstof/mambaforge/lib/python3.9/site-packages (from timm) (1.12.1)\nRequirement already satisfied: torchvision in /home/xstof/mambaforge/lib/python3.9/site-packages (from timm) (0.13.1)\nRequirement already satisfied: typing_extensions in /home/xstof/mambaforge/lib/python3.9/site-packages (from torch>=1.4->timm) (4.3.0)\nRequirement already satisfied: numpy in /home/xstof/mambaforge/lib/python3.9/site-packages (from torchvision->timm) (1.23.2)\nRequirement already satisfied: requests in /home/xstof/mambaforge/lib/python3.9/site-packages (from torchvision->timm) (2.28.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/xstof/mambaforge/lib/python3.9/site-packages (from torchvision->timm) (9.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /home/xstof/mambaforge/lib/python3.9/site-packages (from requests->torchvision->timm) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /home/xstof/mambaforge/lib/python3.9/site-packages (from requests->torchvision->timm) (2022.6.15)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/xstof/mambaforge/lib/python3.9/site-packages (from requests->torchvision->timm) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /home/xstof/mambaforge/lib/python3.9/site-packages (from requests->torchvision->timm) (2.1.0)\n\n\n\nimport timm\nfnames = get_image_files(training_path) # gives us something like: [Path('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training/00006/00221_00000.ppm'),Path('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training/00006/00327_00001.ppm'),Path('/home/xstof/code/xstofai/nbs/traffic-sign/BelgiumTSC_Training/00006/00326_00001.ppm')]\ndef label_func(x): return x.parent.name\ndls = ImageDataLoaders.from_path_func(training_path.absolute(), fnames, label_func, valid_ptc=0.2, seed=42, \n                                      bs = 32,\n                                      item_tfms=Resize(224),\n                                      batch_tfms=aug_transforms())\n\n# arch = \"convnext_large_in22ft1k\"\n# arch = \"convnext_base_in22ft1k\"\n# arch = \"convnext_small_in22k\"\narch = \"convnext_tiny\"\nlearn = vision_learner(dls, arch, metrics=error_rate)\nlr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.cuda.empty_cache() \nlearn.fine_tune(40, base_lr=5*0.01)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.954276\n      1.035131\n      0.152079\n      01:08\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.461623\n      0.096517\n      0.024070\n      02:58\n    \n    \n      1\n      0.176597\n      0.045619\n      0.013129\n      03:05\n    \n    \n      2\n      0.174104\n      0.078769\n      0.028446\n      03:04\n    \n    \n      3\n      0.223794\n      0.124076\n      0.029540\n      03:08\n    \n    \n      4\n      0.179282\n      0.093229\n      0.013129\n      03:05\n    \n    \n      5\n      0.240123\n      0.092536\n      0.009847\n      03:02\n    \n    \n      6\n      0.235270\n      0.144539\n      0.024070\n      02:59\n    \n    \n      7\n      0.306131\n      0.137176\n      0.022976\n      02:58\n    \n    \n      8\n      0.331769\n      0.090923\n      0.020788\n      02:58\n    \n    \n      9\n      0.343526\n      0.239809\n      0.024070\n      02:57\n    \n    \n      10\n      0.318985\n      0.075061\n      0.016411\n      02:57\n    \n    \n      11\n      0.462391\n      0.045153\n      0.008753\n      02:58\n    \n    \n      12\n      0.286320\n      0.079018\n      0.017505\n      02:58\n    \n    \n      13\n      0.242126\n      0.105642\n      0.013129\n      02:59\n    \n    \n      14\n      0.379540\n      0.112675\n      0.010941\n      03:00\n    \n    \n      15\n      0.363734\n      0.095806\n      0.008753\n      03:00\n    \n    \n      16\n      0.296819\n      0.076006\n      0.009847\n      03:00\n    \n    \n      17\n      0.279385\n      0.068440\n      0.004376\n      02:59\n    \n    \n      18\n      0.205650\n      0.041700\n      0.006565\n      02:59\n    \n    \n      19\n      0.209169\n      0.089480\n      0.007659\n      02:58\n    \n    \n      20\n      0.167243\n      0.040720\n      0.005470\n      02:58\n    \n    \n      21\n      0.150439\n      0.085281\n      0.005470\n      02:57\n    \n    \n      22\n      0.096399\n      0.019508\n      0.005470\n      02:58\n    \n    \n      23\n      0.135546\n      0.132828\n      0.005470\n      02:58\n    \n    \n      24\n      0.113812\n      0.034404\n      0.005470\n      02:58\n    \n    \n      25\n      0.097064\n      0.019203\n      0.004376\n      02:58\n    \n    \n      26\n      0.128343\n      0.018315\n      0.002188\n      02:57\n    \n    \n      27\n      0.074050\n      0.000174\n      0.000000\n      02:57\n    \n    \n      28\n      0.055352\n      0.001801\n      0.001094\n      02:58\n    \n    \n      29\n      0.025187\n      0.009907\n      0.002188\n      02:58\n    \n    \n      30\n      0.035481\n      0.018227\n      0.003282\n      02:59\n    \n    \n      31\n      0.028558\n      0.049025\n      0.003282\n      02:59\n    \n    \n      32\n      0.024543\n      0.026493\n      0.004376\n      02:58\n    \n    \n      33\n      0.014045\n      0.018288\n      0.001094\n      02:57\n    \n    \n      34\n      0.008337\n      0.011975\n      0.001094\n      02:57\n    \n    \n      35\n      0.021824\n      0.017372\n      0.001094\n      02:58\n    \n    \n      36\n      0.015128\n      0.026644\n      0.001094\n      02:57\n    \n    \n      37\n      0.023150\n      0.029900\n      0.001094\n      02:58\n    \n    \n      38\n      0.010913\n      0.027740\n      0.001094\n      02:57\n    \n    \n      39\n      0.010624\n      0.025277\n      0.001094\n      02:58\n    \n  \n\n\n\n\ntorch.cuda.empty_cache()\n\n\nlearn.dls.bs\n\n64\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_files = get_image_files(testing_path)\ntest_files_string = test_files.map(lambda f: str(f.absolute()))\nprint(f'file {test_files[0]} has correct label: {label_func(test_files[0])}')\ntestdata = pd.DataFrame(np.array(test_files_string) , columns = ['filename'])\ntestdata['label'] = testdata.apply(lambda row: label_func(Path(row['filename'])), axis = 1)\ndef predict(filename): \n    predicted_label, _, _ = learn.predict(filename)\n    return predicted_label\ntst_dl = learn.dls.test_dl(testdata.filename)\npreds, _ = learn.get_preds(dl=tst_dl)\npreds_indexes = preds.argmax(dim=1)\npreds_indexes_list = L(preds_indexes.tolist())\npred_labels = preds_indexes_list.map(lambda idx: learn.dls.vocab[idx])\ntestdata['predicted'] = pd.DataFrame(np.array(pred_labels))\ntestdata['correct'] = testdata.label == testdata.predicted\ncorrectCount = testdata.correct.sum()\ntotalCount = testdata.correct.count()\nprint(f'Out of {totalCount} items {correctCount} were predicted correctly.  This is {(correctCount/totalCount)*100}%')\n\nfile traffic-sign/BelgiumTSC_Testing/00006/02039_00002.ppm has correct label: 00006\n\n\n\n\n\n\n\n\n\nOut of 2520 items 2495 were predicted correctly.  This is 99.0079365079365%"
  }
]