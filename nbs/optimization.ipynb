{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba1052d-b19c-46b4-9751-c8b31b395580",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c38d6-17c0-4f76-ae28-dea41e3227a9",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "To optimize our weight vector, we'll use Gradient Descent.  This implies finding the gradient in every single dimension of the loss function of our output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46026861-2a3f-4b18-9bb3-c06e4e945155",
   "metadata": {},
   "source": [
    "### Partial Derivate\n",
    "\n",
    "In a single dimension, this is the derivatie of a function:\n",
    "\n",
    "$$\\frac{df(x)}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "Our loss function (of which we'll want to calculate the derivate) has many more dimensions.  For multiple dimensions/variables, the gradient of a function is vector of partial derivates along each dimension.  The slope in any direction is the dot product of the direction with the gradient.\n",
    "\n",
    "The gradient points in the direction of greatest increase of the function.  The negative gradient gives us the direction of greatest decrease.\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine we have a weight vector $W = [0.34, -1.1, 0.78, 3]$ which leads to a loss of $1.254$.  We'll want to find the gradient $dW$ which is a vector of the same shape as $W$.  Each slot in $dW$ will tell us how much the loss will change if we move a tiny amount in that coordinate direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12eeed6-4154-4b02-a341-3a3adf389705",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Stanford - Lecture 3 | Loss Functions and Optimization](https://www.youtube.com/watch?v=h7iBpEHGVNc&t=3224s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
