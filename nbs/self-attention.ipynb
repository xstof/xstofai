{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341662c6-481d-4f92-8503-1a6bbe143dd3",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7714a-bb40-4ed4-8ee0-bf541a1f1383",
   "metadata": {},
   "source": [
    "The below personal learning notes made use of [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff7092-c379-4a92-936e-51d21c2d7f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(precision=1, sci_mode=False, profile='short')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c85faf-19b7-4fae-be47-8fefd1ce95a4",
   "metadata": {},
   "source": [
    "## What is self-attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127bedb-2259-4371-a142-5a52c55a2ef3",
   "metadata": {},
   "source": [
    "Self-Attention started out from research work in translation and was introduced to give access to all elements in a sequence at each time step.  In language tasks, the meaning of a word can depend on the context within a larger text document.  Attention enables the model to weigh the importance of different elements in the input sequence and adjust their influence on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f5447-e490-481b-9e67-caf7088fb80d",
   "metadata": {},
   "source": [
    "## Embedding an Input Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e6d1d-6a9e-4e59-a02e-1e192545d4be",
   "metadata": {},
   "source": [
    "Our input is: \"Playing music makes me very happy\".  We'll create an embeding for this entire sentence first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf2806c-2d72-4258-95b6-548e20114c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Playing', 'music', 'makes', 'me', 'very', 'happy']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Playing music makes me very happy\"\n",
    "\n",
    "sentence_words = sentence.split()\n",
    "sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5741b7-579a-4af4-9aad-84bae49fab1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Playing', 'happy', 'makes', 'me', 'music', 'very']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_words_sorted = sorted(sentence_words)\n",
    "sentence_words_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302470f-6f89-43ec-a8e1-09515d85c408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Playing': 0, 'happy': 1, 'makes': 2, 'me': 3, 'music': 4, 'very': 5}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {word_str:word_idx for word_idx, word_str in enumerate(sentence_words_sorted)}\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affec714-84fc-4520-ab58-5abf2601280f",
   "metadata": {},
   "source": [
    "`dict` is our dictionary, conveniently restricted to just the words we're using here.  Every word we're using has a number associated (the index in our dictionary.  \n",
    "\n",
    "We can now translate our sentence in an array of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855eb7d-de37-4383-abee-8e448980a4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 2, 3, 5, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_int = torch.tensor([dict[word] for word in sentence_words])\n",
    "sentence_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e66a8-73c3-4651-ad67-82ec844cfb13",
   "metadata": {},
   "source": [
    "Now that our sentence is translated into a list of integers, we can use those with an embedding layer to encode the inputs into a real vector embedding.  Let's use 16 dimensions, so that each word is translated/mapped onto an embedding of 16 floats.\n",
    "\n",
    "If our sentence is 6 words (or whatever is the context length we end up choosing), the resulting vector after our embedding layer will be: $6 \\times 16$.  We'll create a pytorch embedding layer with 6 possible indices and a 16-dimensional embedding vector for each index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581c473-51db-4ece-994f-b500a6da5939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0, -2.1, -0.3, -0.4,  1.1, -0.6, -2.3, -1.4,  1.2, -0.4, -0.4,  0.7,\n",
      "          1.0, -0.0, -0.1, -0.1],\n",
      "        [ 0.2,  0.1, -1.3, -2.9,  0.1, -1.2, -0.3,  0.1, -1.3, -0.5, -2.1,  0.9,\n",
      "         -0.6, -0.1,  0.7, -2.8],\n",
      "        [ 1.4, -0.1, -0.8,  0.2, -0.6, -1.2,  1.0,  0.1,  1.1, -0.1, -0.1,  0.4,\n",
      "         -1.4,  0.1,  0.1, -1.0],\n",
      "        [ 1.3, -0.0,  0.2, -0.0,  1.9,  2.1, -0.5, -0.8, -1.1, -1.0, -0.5,  1.2,\n",
      "         -0.9,  1.3,  0.1, -0.1],\n",
      "        [ 1.1,  0.9,  0.9,  0.0, -0.9,  0.7, -0.4, -0.2, -0.9,  0.3, -0.0,  0.3,\n",
      "          0.2, -0.5, -0.6,  0.2],\n",
      "        [-0.1,  0.2, -0.9, -1.0,  1.3,  0.4, -0.5, -0.6, -1.1,  1.8, -0.1, -0.3,\n",
      "          0.9,  0.5, -1.3,  0.8]])\n",
      "torch.Size([6, 16])\n"
     ]
    }
   ],
   "source": [
    "embed = torch.nn.Embedding(6,16)\n",
    "sentence_embedded = embed(sentence_int).detach()\n",
    "print(sentence_embedded)\n",
    "print(sentence_embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b207e-5de7-40c1-8533-3debd497a9d5",
   "metadata": {},
   "source": [
    "So, we gave the embedding a tensor of 6 integers, which got translated in $6 \\times 16$ tensors, meaning: each index, representing a word, it translated into an array of 16 floats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f691eb-c88b-4ec0-869c-1f8d5aa0089a",
   "metadata": {},
   "source": [
    "## Defining weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc8583-3fd6-4810-bad2-928c316c24f0",
   "metadata": {},
   "source": [
    "Self-attention has 3 weight matrices which are each adjusted, like other model parameters, during training.\n",
    "\n",
    "- $W_{q}$: projects our input to the *query*\n",
    "- $W_{k}$: projects our input to the *key*\n",
    "- $W_{v}$: projects our input to the *value*\n",
    "\n",
    "each of *query* $q$, *key* $k$ and *value* $v$ are vectors of an input element.  We can calculate those through matrix multiplication between those $W$ matrices and the embedded inputs $x$.  Our sequence has length $T$.\n",
    "\n",
    "- $q^{i} = W_{q} x^{(i)}$ for every element on index i, ranging from $1$ to $T$\n",
    "- $k^{i} = W_{k} x^{(i)}$ for every element on index i, ranging from $1$ to $T$\n",
    "- $v^{i} = W_{v} x^{(i)}$ for every element on index i, ranging from $1$ to $T$\n",
    "\n",
    "This will give us three vectors for each input element (token) in our sequence.\n",
    "\n",
    "Let's assume that $d$ is the size (number of dimensions) of each (embedded) word vector x.  Our vector $q^{i}$ is the query vector for word at index $i$ and has a dimension we can choose.  We'll call this $d_q$.  In the same way we'll call $d_k$ as the dimension for $k^{i}$.\n",
    "\n",
    "We'll calculate the dot product between the query and key vectors, this means that each of them needs to have the same dimensions: $d_q = d_k$. Let's choose $d_q = d_k = 24$ in this case.<br/>\n",
    "If $q^{i} = W_{q} x^{(i)}$ then the dimension for $q^{i}$ is $d_q \\times d$; dimension for $W_{q}$ is $d_q \\times d$; and dim for $x^{(i)}$ is $d \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f99ed-2cdd-4d1e-8896-c577fa7f0516",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4441d-ae46-456c-bd66-463299f85559",
   "metadata": {},
   "source": [
    "- [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "- [Thinking Like Transformers](https://arxiv.org/abs/2106.06981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119134fe-eea8-4d9d-a0f1-f875272541ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
