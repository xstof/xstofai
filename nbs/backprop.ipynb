{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1a9805-cf40-4c88-9abf-0d7949e54a51",
   "metadata": {},
   "source": [
    "# Exploring Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dec261-450b-4d54-b6f5-73c4d1551375",
   "metadata": {},
   "source": [
    "These notes accompany the [Backpropagation and MLP lesson](https://course.fast.ai/Lessons/lesson13.html#lesson-resources) from Jeremy Howard.  The associated workbook can be found here: [https://github.com/fastai/course22p2/blob/master/nbs/03_backprop.ipynb](https://github.com/fastai/course22p2/blob/master/nbs/03_backprop.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885da2da-3d43-4b36-b44c-09a2551d12b3",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4b0898-81e9-4be5-bfd0-fb66821dcb36",
   "metadata": {},
   "source": [
    "We start by loading MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b90e31-f875-405a-93fc-c24d140e86c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/home/xstof/.fastai/data/mnist_png/training'),Path('/home/xstof/.fastai/data/mnist_png/testing')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "# path_data = Path('data')\n",
    "# path_gz = path_data/'mnist.pkl.gz'\n",
    "# with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "# x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n",
    "\n",
    "path = untar_data(URLs.MNIST)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02dfe306-c574-4faa-8632-ef8586394203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Define the transformation to apply to the data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),\n",
    "     transforms.ToTensor(),  # convert PIL image to PyTorch tensor\n",
    "     transforms.Normalize((0.5,), (0.5,))])  # normalize the data to have a mean of 0.5 and std of 0.5\n",
    "\n",
    "# Load the training dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data/mnist', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create a data loader for the training dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0d0fc4-e2c8-4f55-a585-8f4ee433d58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800efc5-ec7c-4257-9f04-bef2de36dcfb",
   "metadata": {},
   "source": [
    "As we can see, every image is stored with one explicit channel.  Let's simplify this by removing this dimension. In addition we'll concatenate all pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9c26b2-3c5c-45c2-898e-b774d4323ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 784])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = images.view(32,28*28)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0728d-9624-46db-9b10-224864291ac5",
   "metadata": {},
   "source": [
    "This means, we have a batch of 32 images, each with 784 pixels (28 * 28).  Visuially that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9959cabd-5ea5-4881-a770-568c78f57c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for this image: 4\n"
     ]
    }
   ],
   "source": [
    "# Display the third image in the batch as an image\n",
    "def show_image(img):\n",
    "    plt.imshow(img.view(28,28), cmap='gray')\n",
    "    plt.show()\n",
    "show_image(images[2])\n",
    "print(f\"label for this image: {labels[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b8987-30c5-47d1-a7b2-ea703790a244",
   "metadata": {},
   "source": [
    "## Doing a Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b438e19-8152-42e7-8601-575c1f32b490",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25717a24-e803-4c4d-b537-1c001555ba74",
   "metadata": {},
   "source": [
    "Let's set some parameters first:\n",
    "\n",
    "- `nr_hidden`: number of hidden layers (50)\n",
    "- `n`: number of images (32)\n",
    "- `m`: number of pixels in every image (784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b5d773-af04-41c6-b976-0c8504aa3390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nr_hidden = 50\n",
    "n, m = images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28320621-0dd8-453d-bbd2-9c7907619782",
   "metadata": {},
   "source": [
    "And let's assume some weights and biases for two layers of our neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1664389-59f6-4e0b-aba7-286cd02d448c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w1 = torch.randn(m, nr_hidden) # 784 x 50 (every hidden node has 784 inputs, 50 of them)\n",
    "b1 = torch.zeros(nr_hidden)    # 50\n",
    "w2 = torch.randn(nr_hidden, 1) # 50 x 1 (every end node has 50 inputs, 1 of them)\n",
    "b2 = torch.zeros(1)            # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75490283-1820-459c-9df0-12ff96814429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.25,  0.58,  0.80,  ..., -1.15,  0.85, -0.78],\n",
       "         [-1.12, -0.24,  0.28,  ..., -0.76,  0.92,  0.53],\n",
       "         [ 1.11,  0.82, -1.63,  ..., -0.63, -0.77,  1.05],\n",
       "         ...,\n",
       "         [ 0.51,  0.76, -0.02,  ...,  0.60, -0.22, -1.02],\n",
       "         [ 1.35, -0.80, -1.00,  ..., -1.15, -1.02,  0.19],\n",
       "         [ 0.35, -0.66,  2.23,  ..., -0.79,  0.29,  1.98]]),\n",
       " torch.Size([784, 50]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00c4cefd-3d6e-4f3f-9f55-d0a22e3a9637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.15],\n",
       "         [-0.10],\n",
       "         [-0.63],\n",
       "         [ 0.84],\n",
       "         [ 1.20]]),\n",
       " torch.Size([50, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2[:5,:], w2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a5bb2-b76d-4f30-9bf7-3609ced63061",
   "metadata": {},
   "source": [
    "Let's define a simple lineair layer using these weights and biases.  The layer takes as input a vector x and calculates the output for each of it's output nodes (`nr_hidden` in this case for the first layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52eb2bb4-496b-454f-ab00-ece9ab144041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5674881b-eaf0-4078-8db2-0838e014b97f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 50]),\n",
       " tensor([[ 10.32, -39.56, -13.43,  ...,  -1.25,  52.43, -27.01],\n",
       "         [  6.32, -19.67,  10.22,  ...,  35.99,  74.31,  16.61],\n",
       "         [ 33.00, -13.33,  13.57,  ...,  24.38,  27.36,   1.30],\n",
       "         ...,\n",
       "         [ 15.04, -54.34,  33.99,  ..., -13.76,  59.42,   7.30],\n",
       "         [ 39.27, -66.01,   4.90,  ...,  -0.77,  49.98,  11.18],\n",
       "         [-32.05, -47.28,  10.01,  ...,  19.10,  69.69,   7.22]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = lin(images, w1, b1)\n",
    "t.shape, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b64f73-3ea1-4b82-acb9-8bbee1bbfd75",
   "metadata": {},
   "source": [
    "So, when we put our training batch (`images`) through a linear layer defined by `w1` and `b1`, we get back a tensor 32 rows (our 32 images in the batch) and for each image/row a set of 50 values.  One for each of our hidden layer neurons.\n",
    "\n",
    "Let's define a non-linear function now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51180a98-ab2e-49a6-af10-4b32fd162f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2ada696-a288-423a-879b-cb24b683c35e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 50]),\n",
       " tensor([[10.32,  0.00,  0.00,  ...,  0.00, 52.43,  0.00],\n",
       "         [ 6.32,  0.00, 10.22,  ..., 35.99, 74.31, 16.61],\n",
       "         [33.00,  0.00, 13.57,  ..., 24.38, 27.36,  1.30],\n",
       "         ...,\n",
       "         [15.04,  0.00, 33.99,  ...,  0.00, 59.42,  7.30],\n",
       "         [39.27,  0.00,  4.90,  ...,  0.00, 49.98, 11.18],\n",
       "         [ 0.00,  0.00, 10.01,  ..., 19.10, 69.69,  7.22]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(t)\n",
    "t.shape, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc1b49-b403-46c7-8f96-0efdb915f800",
   "metadata": {},
   "source": [
    "The shape did not change, but any element below 0 is brought back to zero now.\n",
    "\n",
    "Our entire two-layer model now can be defined like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef05af36-0584-4942-8bca-b78951ce60d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    l1 = lin(x, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b19accc3-36b2-4982-b426-75827f7c0517",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]),\n",
       " tensor([[ -86.24],\n",
       "         [  -8.37],\n",
       "         [ -40.68],\n",
       "         [ -88.72],\n",
       "         [-139.06]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(images)\n",
    "res.shape, res[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b7570-cc10-469c-baf7-76d9dd727db3",
   "metadata": {},
   "source": [
    "This is the resulting output of our neural net: one result for each of our input images in our batch of 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0933dc7-f73f-4131-9eb7-a9978ea0f59b",
   "metadata": {},
   "source": [
    "### Define a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9098dd-e90b-4ef9-82ff-382cd4cdaaba",
   "metadata": {},
   "source": [
    "We'll define a completely wrong, unreasonable loss function here, for our learning purposes.  We'll replace this later by something more relevant.  We'll use a Mean Square Error or MSE function.\n",
    "\n",
    "Our loss is always defined between the predicted output of our neural net and the actual truth values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98a791de-fbe7-4341-8e7e-1cd282f22287",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = images\n",
    "y_train = labels\n",
    "\n",
    "res.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bfa1c-68c9-4f0c-ac35-c43c388ff292",
   "metadata": {},
   "source": [
    "Let's substract the truth values from the network's output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c75ca7b2-924e-4fb1-81b9-0469e81f6010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res - y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e9671a8-42bd-493f-9ac4-623e6fa2cfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -91.24,  -86.24,  -90.24,  ...,  -93.24,  -89.24,  -94.24],\n",
       "        [ -13.37,   -8.37,  -12.37,  ...,  -15.37,  -11.37,  -16.37],\n",
       "        [ -45.68,  -40.68,  -44.68,  ...,  -47.68,  -43.68,  -48.68],\n",
       "        ...,\n",
       "        [ -46.43,  -41.43,  -45.43,  ...,  -48.43,  -44.43,  -49.43],\n",
       "        [-132.75, -127.75, -131.75,  ..., -134.75, -130.75, -135.75],\n",
       "        [-150.83, -145.83, -149.83,  ..., -152.83, -148.83, -153.83]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res - y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d31e3-99bc-42ba-acfc-f7ab51c56d0b",
   "metadata": {},
   "source": [
    "What does this mean?  When we subtract y_train values from res, we're doing broadcasting.  This does not pan out like we would like it to be.  `res` looks like this and has a dimension too much:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf44e18c-ea49-4e94-99b9-6da736dbd8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -86.24],\n",
       "        [  -8.37],\n",
       "        [ -40.68],\n",
       "        [ -88.72],\n",
       "        [-139.06]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:5,:] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3eb277-46e6-41eb-ac5d-489fe61defb0",
   "metadata": {},
   "source": [
    "Let's remove this dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39cac487-a894-468c-9ad7-81c1dd871a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -86.24,   -8.37,  -40.68,  -88.72, -139.06,  -21.66,  -88.79,  -54.04, -113.56,  -50.79,  -97.39,  -52.44, -154.46,\n",
       "         -55.64,  -96.86,  -62.93,  -68.99, -168.57, -166.04,  -81.36,  -97.52,   21.72,  -99.52,  -83.41, -116.91, -150.95,\n",
       "         -62.47,  -96.60,  -70.09,  -41.43, -127.75, -145.83])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5af1b3-01b4-42da-9ea2-08d471bb98e7",
   "metadata": {},
   "source": [
    "an alternative to this way of doing this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84938bcc-9baf-4711-ad4b-f693b8d8c061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -86.24,   -8.37,  -40.68,  -88.72, -139.06,  -21.66,  -88.79,  -54.04, -113.56,  -50.79,  -97.39,  -52.44, -154.46,\n",
       "         -55.64,  -96.86,  -62.93,  -68.99, -168.57, -166.04,  -81.36,  -97.52,   21.72,  -99.52,  -83.41, -116.91, -150.95,\n",
       "         -62.47,  -96.60,  -70.09,  -41.43, -127.75, -145.83])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e67d27-66c7-44b8-9cc5-02e5b7a98f92",
   "metadata": {},
   "source": [
    "Or just like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56c9594b-289d-42d4-9279-73d65795a704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -86.24,   -8.37,  -40.68,  -88.72, -139.06,  -21.66,  -88.79,  -54.04, -113.56,  -50.79,  -97.39,  -52.44, -154.46,\n",
       "         -55.64,  -96.86,  -62.93,  -68.99, -168.57, -166.04,  -81.36,  -97.52,   21.72,  -99.52,  -83.41, -116.91, -150.95,\n",
       "         -62.47,  -96.60,  -70.09,  -41.43, -127.75, -145.83])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e35cfb-f605-4456-976b-2f94d7a9ba6d",
   "metadata": {},
   "source": [
    "Back to our MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "451e7ee1-0261-42a8-8ccc-cda9449b751f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res[:,0]-y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "706b02b3-8074-45f2-b227-6cee6cebb856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse(output, targ): return (output[:,0]-targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f69a205a-5c1c-469b-8f49-23573a18bf88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5dd13fe-3976-48c5-b73d-12132241eb65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10262.35)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547e6a7-60c0-4224-a96c-a057082d0727",
   "metadata": {},
   "source": [
    "### Gradients and backwards pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c1282-6d24-414e-a4c2-069e3a6aa1ef",
   "metadata": {},
   "source": [
    "Our forward and pass looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0bf5d31-56da-4794-b597-56505faf0341",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10262.35)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp, targ = x_train, y_train\n",
    "l1 = lin(inp, w1, b1)\n",
    "l2 = relu(l1)\n",
    "out = lin(l2, w2, b2)\n",
    "diff = out[:,0]-targ\n",
    "loss = diff.pow(2).mean()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a6806-c299-481d-9d76-77a2a797af01",
   "metadata": {},
   "source": [
    "For the backward pass we now need to work backwards using the gradients.  \n",
    "\n",
    "We'll store the gradients on the variables themselves.  Let's start with our loss.  This is defined like $$\\frac{\\sum_{i=0}^{n}(out_i-targ_i)^2}{n}$$ or $$\\frac{\\sum_{i=0}^{n}(x_i-y_i)^2}{n}$$\n",
    "\n",
    "The [derivative of this](https://www.wolframalpha.com/input?i2d=true&i=Partial%5BDivide%5BPower%5Bx_0-y_0%2C2%5D%2Cn%5D%2Cx_0%5D) with respect to for example $x_0$ is:\n",
    "\n",
    "$$\\frac{2(x_0-y_0)}{n}$$\n",
    "\n",
    "In this equation, n is equal to the number of elements in our input (like in: `inp.shape[0]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e341a55-8ee3-48ae-8f8f-ca3fddfb4b47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -5.70,  -0.52,  -2.79,  -5.61,  -9.25,  -1.48,  -5.61,  -3.56,  -7.16,  -3.42,  -6.27,  -3.59,  -9.84,  -3.85,\n",
       "         -6.12,  -4.37,  -4.44, -11.04, -10.75,  -5.65,  -6.34,   1.36,  -6.78,  -5.28,  -7.37,  -9.56,  -4.15,  -6.23,\n",
       "         -4.51,  -3.03,  -8.17,  -9.61])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.g = 2.*diff / inp.shape[0]\n",
    "out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b021f59-0083-47eb-8e76-e850d70903bf",
   "metadata": {},
   "source": [
    "What do these mean?  Each of those numbers is an indication to: \"if I change this out param (the result of the last linear layer), then how much does that influence the loss?\"\n",
    "\n",
    "We now need to calculate the gradient of our linear layer.  We will determine how much the change of each param out of which our lineair layer consist has an influence on its output.  Let's consider the single output neuron $o$ from layer l3 (as it only has one output neuron). Its output is determined by the formula:\n",
    "\n",
    "$$o = (l2_0 . w2_0 + l2_1 . w2_1 + ... + l2_{50}) + b2$$\n",
    "\n",
    "what is now the derivate of this function with regards to, let's say $w2_3$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea83427-8f8e-44d7-8e41-56427289e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    inp.g "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
