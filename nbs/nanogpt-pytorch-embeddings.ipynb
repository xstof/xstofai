{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoGPT - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.set_printoptions(precision=2)\n",
    "generator = torch.manual_seed(42)\n",
    "\n",
    "vocab_size = 8 # 8 characters or language tokens possible\n",
    "token_embedding_table = nn.Embedding(vocab_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the contents of our embedding table now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n",
      "        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
      "        [ 1.64, -0.16, -0.50,  0.44, -0.76,  1.08,  0.80,  1.68],\n",
      "        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
      "        [-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n",
      "        [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49],\n",
      "        [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
      "        [-1.40,  0.04, -0.06,  0.68, -0.10,  1.84, -1.18,  1.38]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(token_embedding_table.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index into this embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 1, 3, 0, 3, 5],\n",
       "        [1, 1, 0, 1, 4, 1],\n",
       "        [3, 3, 6, 3, 6, 3],\n",
       "        [4, 7, 6, 2, 5, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's assume a batch of 4 independent rows (B-dimension) which each have 6 characters/tokens (T-dimension):\n",
    "batch_size = 4\n",
    "context_length = 6\n",
    "idx = torch.randint(low=0, high=vocab_size, size=(batch_size, context_length))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49]],\n",
       "\n",
       "        [[-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "         [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n",
       "         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "         [-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n",
       "         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76]],\n",
       "\n",
       "        [[ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86]],\n",
       "\n",
       "        [[-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n",
       "         [-1.40,  0.04, -0.06,  0.68, -0.10,  1.84, -1.18,  1.38],\n",
       "         [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "         [ 1.64, -0.16, -0.50,  0.44, -0.76,  1.08,  0.80,  1.68],\n",
       "         [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49],\n",
       "         [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's us that to index into our embedding table:\n",
    "logits = token_embedding_table(idx)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we have here is that for each of our independent batch items (our first dimension `batch_size`, 4 in total) we get back a 6 by 8 matrix.  Let's have a look at our first batch item: `[6, 1, 3, 0, 3, 5]`.  Here `6`, `1`, `3` etc are the integer indexes representing each a token (character or subword in LLM-world). \n",
    "\n",
    "Those numbers index into the embedding table:\n",
    "\n",
    "- `6` looks up in the 7th row of our embedding and gives: `[-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74]`\n",
    "- `1` looks up in the 2nd row of our embedding and gives: `[-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76]`\n",
    "- `3` looks up in the 4th row of our embedding and gives: `[ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86]`\n",
    "\n",
    "So what we get back is for each token-index  of our batch item, a list (size `vocab_size`, here 8) with probabilities for each next token.  That's why we get an additional dimension returned: every token we input into our embedding, returns a list of probabilities for the next token.  So while our:\n",
    "\n",
    "- input dimension is `batch_size` x `context_length`, here 4 x 6, the \n",
    "- output dimension is `batch_size` x `context_length` x `vocab_size`, here 4 x 6 x 8\n",
    "\n",
    "> This last tensor is a B x T x C tensor. (Batch, Time, Channel).\n",
    ">\n",
    "> When calculating the Cross Entropy Loss, it wants to have a B x C x T tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Cross Entropy Loss\n",
    "\n",
    "Now that we have embeddings, for a simple bigram model, these represent the chances for each character in the vocabulary will be the next character/token.  These are logits: the unprocessed outcome of our network, before they're turned into probabilities.  We can compare that to what we see in reality in our training data to calculate our loss using Cross Entropy.\n",
    "\n",
    "First let's see how we can shape tensors using their `view()` method.\n",
    "\n",
    "## Reshaping tensors using `view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5, 3, 7, 7, 5, 9],\n",
       "         [1, 5, 1, 9, 1, 4],\n",
       "         [0, 3, 7, 5, 7, 1],\n",
       "         [5, 7, 5, 8, 5, 4]],\n",
       "\n",
       "        [[1, 1, 0, 9, 0, 9],\n",
       "         [1, 8, 9, 6, 7, 6],\n",
       "         [0, 9, 5, 2, 9, 1],\n",
       "         [7, 8, 6, 0, 6, 8]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randint(0, 10, (2,4,6))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 7, 7, 5, 9, 1, 5, 1, 9, 1, 4, 0, 3, 7, 5, 7, 1, 5, 7, 5, 8, 5, 4],\n",
       "        [1, 1, 0, 9, 0, 9, 1, 8, 9, 6, 7, 6, 0, 9, 5, 2, 9, 1, 7, 8, 6, 0, 6, 8]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(2,24)  # combine the second and third dimension into one (4x6=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 7, 7, 5, 9, 1, 5, 1, 9, 1, 4, 0, 3, 7, 5, 7, 1, 5, 7, 5, 8, 5, 4],\n",
       "        [1, 1, 0, 9, 0, 9, 1, 8, 9, 6, 7, 6, 0, 9, 5, 2, 9, 1, 7, 8, 6, 0, 6, 8]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can do the same by having pytorch figure out the size of the remaining dimension, using `-1`\n",
    "t.view(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5, 3, 7, 7, 5, 9, 1, 5, 1, 9, 1, 4],\n",
       "         [0, 3, 7, 5, 7, 1, 5, 7, 5, 8, 5, 4]],\n",
       "\n",
       "        [[1, 1, 0, 9, 0, 9, 1, 8, 9, 6, 7, 6],\n",
       "         [0, 9, 5, 2, 9, 1, 7, 8, 6, 0, 6, 8]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(2,2,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing logits tensor (BxTxC) for Cross Entropy Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6, 8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, T, C = logits.shape\n",
    "B, T, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the cross entropy loss function expects is for a multidimensional input, for the channels (C) to be the second dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49]],\n",
       "\n",
       "        [[-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "         [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n",
       "         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "         [-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n",
       "         [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76]],\n",
       "\n",
       "        [[ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "         [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "         [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86]],\n",
       "\n",
       "        [[-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n",
       "         [-1.40,  0.04, -0.06,  0.68, -0.10,  1.84, -1.18,  1.38],\n",
       "         [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "         [ 1.64, -0.16, -0.50,  0.44, -0.76,  1.08,  0.80,  1.68],\n",
       "         [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49],\n",
       "         [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "        [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n",
       "        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "        [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49],\n",
       "        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "        [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60],\n",
       "        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "        [-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n",
       "        [-0.75,  1.65, -0.39, -1.40, -0.73, -0.56, -0.77,  0.76],\n",
       "        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "        [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "        [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "        [ 1.28,  1.30,  0.61,  1.33, -0.23,  0.04, -0.25,  0.86],\n",
       "        [-1.38, -0.87, -0.22,  1.72,  0.32, -0.42,  0.31, -0.77],\n",
       "        [-1.40,  0.04, -0.06,  0.68, -0.10,  1.84, -1.18,  1.38],\n",
       "        [-0.91, -0.66,  0.08,  0.53, -0.49,  1.19, -0.81, -0.74],\n",
       "        [ 1.64, -0.16, -0.50,  0.44, -0.76,  1.08,  0.80,  1.68],\n",
       "        [-1.56,  1.00, -0.88, -0.60, -1.27,  2.12, -1.23, -0.49],\n",
       "        [ 1.93,  1.49,  0.90, -2.11,  0.68, -1.23, -0.04, -1.60]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = logits.view(B*T, C) # moves channels (probs for next token for each item in vocabulary) into second dim\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate loss versus targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 6, 0, 7, 0],\n",
       "        [3, 7, 7, 6, 2, 2],\n",
       "        [0, 7, 2, 2, 0, 2],\n",
       "        [4, 1, 6, 1, 0, 3]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create some made-up targets to play with, shaped: B x T\n",
    "idy = torch.randint(low=0, high=vocab_size, size=(batch_size, context_length))\n",
    "idy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Cross Entropy Loss, pytorch expects a one-dimensional tensor for our targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 6, 0, 7, 0, 3, 7, 7, 6, 2, 2, 0, 7, 2, 2, 0, 2, 4, 1, 6, 1, 0, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = idy.view(batch_size * context_length)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.83, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
